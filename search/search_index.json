{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Introduction/","title":"Introduction","text":""},{"location":"Introduction/#introduction-chapter","title":"Introduction Chapter","text":"<p>This repository aims at giving any reader the ability to become of Full Stack ML engineer. I mean to be a full stack ML Engineer when considering the wild ecosystem of Data and Computer Science across all fields. Some parts of the repository focuses on Natural Language Processing that became LLM driven in the recent years. A specific focus that came to my attention is the MLOps and LLMOps field.</p> <p>The repo is not an extensive deep dive into each concept but rather an overview of what is needed to become full stack. There are links shared towards other blogs and notebooks when a topic needs to be deep dived or when I found the resources to be amazing for someone who wants to learn.</p> <p>I've created this repo so people (including myself) can have a central place to cover these many topics, keep learning and keep improving through time. I've tried to make topics accessible but most of them require already a great level of what data and computer science is. I've tried as much as possible to relay ML/LLM Ops practices, give real life (i.e. enterprise) experiences and feedbacks to go beyond the theory.</p> <p>It may be incomplete, work in progress as I keep updating it. Sometimes AI assistants helped me writing or reviewing it.</p> <p>Ping me if you want to add any contribution to it or if you have any comments.</p> <p>Have fun reading it !</p>"},{"location":"Resources/","title":"Resources","text":"<p>Over the year, we all accumulated a great amount of resources. This handbook is built upon a great deal of experiences but also based on the resources I'm sharing below.</p> <p>Feel free to explore, follow and recommend new ones. I'm doing my best to keep the list up to date.</p> <ul> <li>Blogs:</li> <li>Sebastian Raschka blog</li> <li>Hugging Face blog posts</li> <li>Chip Huyen\u2019s blog</li> <li>Lilian Weng\u2019s blog     </li> <li>Andrej Karpathy's blog</li> <li>The Deep Learning textbook</li> <li> <p>The GenAI Handbook from Brown, William</p> </li> <li> <p>GitHub courses:</p> </li> <li>ML Engineering repository</li> <li>LLM Engineering Handbook from MLabonne</li> <li>LLM Engineering weeks</li> <li>The Hundred-Page Language Models Book from A Burkov NOT TESTED</li> <li>LLM Engineering guide</li> <li>LLM Course from Maxime Labonne</li> <li> <p>DAIR repositories: https://github.com/orgs/dair-ai/repositories</p> </li> <li> <p>Courses:</p> </li> <li>The d2l.ai interactive textbook with notebooks and github examples</li> <li>The Weaviate Academy is very interesting about indexinv, vectorization, quantization, tokenization, compression strategies</li> <li> <p>The Standford CME 295 is quite dense but definitely the best in class for those who wants to dive technical in the topic</p> </li> <li> <p>(Free) Certifications</p> </li> <li> <p>Lighting AI certification</p> </li> <li> <p>YouTube:</p> </li> <li>Andrej Karpathy\u2019s \u201cZero to Hero\u201d videos</li> <li>3Blue1Brown videos</li> </ul> <p>Interview sandboxes:</p> <ul> <li>https://github.com/Devinterview-io?tab=repositories</li> <li>https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/tree/main</li> </ul> <p>https://docs.zenml.io/user-guides/llmops-guide https://weaviate.io/blog/cross-encoders-as-reranker unsloth: https://github.com/unslothai/notebooks/ https://github.com/philschmid/gemini-2.5-ai-engineering-workshop?tab=readme-ov-file https://github.com/ritchieng/the-incredible-pytorch https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/Deep%20Learning%20Questions%20&amp;%20Answers%20for%20Data%20Scientists.md https://github.com/khangich/machine-learning-interview https://tactlabs.gitbook.io/featurepreneur/pytorch-interview-questions https://paperswithcode.com/paper/deep-learning-interviews-hundreds-of-fully https://multimodalai.substack.com/p/understanding-llm-optimization-techniques</p> <p>https://www.linkedin.com/posts/aishwarya-srinivasan_if-youre-an-ai-engineer-trying-to-optimize-activity-7334975771561680900-BXr-?utm_source=share&amp;utm_medium=member_android&amp;rcm=ACoAAAVV2dEBAuuJCv1jGmfAXdBgR9YAUI0StlMhttps://encord.com/blog/embeddings-machine-learning/?utm_source=linkedin Postraining labonne MIT: https://www.youtube.com/watch?v=_HfdncCbMOE&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=10 Merge models: only merge same architecture and same size models ; merge FP/quantized not recommended ; test-time compute scaling (process reward models) Fine tuning data preparation: https://docs.unsloth.ai/basics/datasets-guidehttps://www.anthropic.com/news/contextual-retrievalhttps://github.com/KalyanKS-NLP/rag-zero-to-hero-guide?tab=readme-ov-file Opti: https://neuralbits.substack.com/p/understanding-llm-optimization-techniqueshttps://github.com/stas00/ml-engineering?tab=readme-ov-filehttps://github.com/huggingface/evaluation-guidebook?tab=readme-ov-file (useful to have beginner and advanced (tips and tricks) https://github.com/youssefHosni/Awesome-AI-Data-Guided-Projects?tab=readme-ov-filehttps://levelup.gitconnected.com/14-free-large-language-models-fine-tuning-notebooks-532055717cb7https://pub.towardsai.net/mastering-large-language-model-llm-fine-tuning-top-learning-resources-dcef012256fdhttps://levelup.gitconnected.com/best-resources-on-building-datasets-to-trian-llms-f6c6e02fc375https://levelup.gitconnected.com/14-free-large-language-models-fine-tuning-notebooks-532055717cb7https://pub.towardsai.net/best-resources-to-learn-understand-evaluating-llms-4610ee5dc5c1https://inkd.in/dW8wBJjXhttps://inkd.in/dSU922JQhttps://inkd.in/dz2rJ2PC Explication of quantiazation and lora: https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/ Search and sampling techniques next K: https://huggingface.co/blog/mlabonne/decoding-strategieshttps://poloclub.github.io/transformer-explainer/https://huggingface.co/docs/safetensors/en/index MoE: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/README.md https://gist.github.com/philschmid/d188034c759811a7183e7949e1fa0aa4 https://cameronrwolfe.substack.com/p/nano-moe https://machinelearningmastery.com/mixture-of-experts-architecture-in-transformer-models/</p> <p>https://genai-handbook.github.io/# Free clouds and GPU: https://www.ori.co/alphasignal-1000?utm_source=referral&amp;utm_medium=alphasignal&amp;utm_campaign=alphasignal_newsletter_1 bentoML: https://github.com/bentoml/OpenLLM Nvidia: https://github.com/NVIDIA/GenerativeAIExamples https://www.linkedin.com/pulse/build-rag-app-nvidia-nim-milvus-running-locally-janakiram-msv-lch4c/?utm_source=share&amp;utm_medium=member_android&amp;utm_campaign=share_via</p> <p>Cookbook/Courses: https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x https://github.com/PrunaAI/ai-efficiency-courses/blob/main/slides/01-language_model_architectures.pdf Mistral Cookbook: https://github.com/mistralai/cookbook HF cookbook: https://huggingface.co/learn/cookbook/index Anthropic cookbook: https://github.com/anthropics/anthropic-cookbook</p> <p>Gui thomas: https://github.com/guillaume-thomas/kto-mlflowhttps://benjaminwarner.dev/2023/07/01/attention-mechanism Mlops course: https://github.com/iusztinpaul/energy-forecastinghttps://archive.docs.dagger.io/0.9/sdk/python/628797/get-started/https://mlflow.org/docs/latest/recipes.html MIT courses: https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB</p> <p>Tutorial for RAG: https://www.sakunaharinda.xyz/ragatouille-book/intro.htmlhttps://github.com/decodingml/llm-twin-course?tab=readme-ov-file Microsoft engineering playbook: https://microsoft.github.io/code-with-engineering-playbook/https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ For beginner: https://www.youtube.com/playlist?list=PLlrxD0HtieHj2nfK54c62lcs3-YSTx3Je DL LeCun: https://atcold.github.io/NYU-DLSP21/ Git flush: https://www.linkedin.com/posts/rajatgajbhiye_i-will-never-understand-git-it-is-so-confusing-activity-7231890613313396736-XJZA/?utm_source=share&amp;utm_medium=member_android example: https://weaviate.io/developers/academy/py/multitenancy/overview End to end llm twin: https://github.com/decodingml/llm-twin-course \u2014&gt; advanced course for optimization engineering Productionize LLM: https://mrmaheshrajput.medium.com/how-to-productionize-large-language-models-llms-060a4cb1a169 FINOS certifs: https://teams.microsoft.com/l/message/19:da592d9ec47a4b75b5c5be73d06a82c6@thread.skype/1722937676415?tenantId=396b38cc-aa65-492b-bb0e-3d94ed25a97b&amp;groupId=380efd03-e182-4c93-987f-90a86cd096d7&amp;parentMessageId=1722937676415&amp;teamName=AXA Software Engineering (SE4A)&amp;channelName=Open Source and InnerSource&amp;createdTime=1722937676415 Llm prod like: https://github.com/decodingml/llm-twin-course?tab=readme-ov-file https://apps.courses.comet.com/learning/course/course-v1:Comet+101+2024/home https://www.zenml.io/blog/automating-lightning-studio-ml-pipelines-for-fine-tuning-llm https://parlance-labs.com/education/#fine-tuning https://learning.oreilly.com/videos/what-is-llmops/0642572035242/0642572035242-video361334/ Books list pdf: https://github.com/anishLearnsToCode/books/tree/master Anriy burkov: https://github.com/aburkov/theLMbook https://github.com/marvelousmlops/marvel-characters pruning and prunai: https://github.com/PrunaAI/awesome-ai-efficiency?tab=readme-ov-file https://github.com/PrunaAI/ai-efficiency-courses</p> <p>Dependencies: Comparison: https://www.playfulpython.com/environment-tools-pdm-poetry-rye/ rye: https://github.com/astral-sh/ryehttps://github.com/pdm-project/pdm https://github.com/pdm-project/pdm</p> <p>Generic: Repo of repo: https://github.com/eugeneyan/applied-ml https://ml-ops.org/content/mlops-principles Yellowbrick: https://www.scikit-yb.org/en/latest/ ludwig: https://ludwig.ai/latest/ pyreft: https://github.com/stanfordnlp/pyrefthttps://github.com/stanfordnlp/pyreft?tab=readme-ov-filehttps://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing#scrollTo=NL0yGhbe3EFk Git: https://dev.to/glasskube/the-guide-to-git-i-never-had-1450 Link list: https://towardsai.net/book Nice LLM overview: https://areganti.notion.site/Week-7-Building-Your-Own-LLM-Application-fbac298e688a42148c18a1ddc7594362 Uihub: https://uithub.com/openapi.html#/operations/getRepositoryContents</p> <p>Other</p> <ul> <li>wavelet: https://github.com/ShawhinT/YouTube-Blog/tree/main</li> </ul>"},{"location":"AI%20Agents%20Engineering/Resources/","title":"Resources","text":"<p>Agents/md example: https://agents.md/#examples // https://github.blog/ai-and-ml/github-copilot/how-to-write-a-great-agents-md-lessons-from-over-2500-repositories/ https://github.com/Agent-on-the-Fly/Memento https://github.com/swirl-ai/ai-angineers-handbook/tree/main/building_agents_from_scratch Clear agents patterns: https://www.agentrecipes.com https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents To test with access to llama or secureGPT: https://docs.phidata.com/introduction https://medium.com/@sokratis.kartakis/demystifying-generative-ai-agents-cf5ad36322bd \u00a0AWS lab: https://github.com/orgs/awslabs/repositories?q=agent llamacloud: LlamaCloud Windows agents evaluation: https://github.com/microsoft/WindowsAgentArena Anthropic course: https://dair-ai.thinkific.com/courses/introduction-ai-agents https://www.youtube.com/watch?v=Pz9YeBs_afo https://github.com/AgentOps-AI/agentops Bee stack: https://github.com/i-am-bee/bee-stack https://medium.com/@sokratis.kartakis/genaiops-operationalize-generative-ai-a-practical-guide-d5bedaa59d78 To test: https://github.com/awslabs/amazon-bedrock-agent-samples https://github.com/togethercomputer/moa https://www.kaggle.com/whitepaper-agent-companion https://github.com/google/A2A https://mlflow.org/docs/latest/tracing/integrations/openai-agent https://www.anthropic.com/engineering/claude-code-best-practices?utm_source=alphasignal Agentic memory: https://github.com/agiresearch/A-mem https://strandsagents.com/0.1.x/user-guide/safety-security/prompt-engineering/ https://github.com/BrainBlend-AI/atomic-agents https://github.com/anthropic-experimental/agentic-misalignment https://github.com/NirDiamant/agents-towards-production https://aws.amazon.com/fr/blogs/machine-learning/build-an-intelligent-multi-agent-business-expert-using-amazon-bedrock/ Agents patterns: https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/edit?tab=t.0 https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/edit?tab=t.0 https://github.com/HKUDS/AutoAgent?tab=readme-ov-file#-quick-start https://venturebeat.com/ai/this-new-framework-lets-llm-agents-learn-from-experience-no-fine-tuning Agent training: https://github.com/OpenPipe/ART?tab=readme-ov-file Agent Spec: https://arxiv.org/pdf/2510.04173 Agents kubernetes: https://github.com/kagenti/kagenti?tab=readme-ov-file#agent-and-tool-authorization-pattern kubernetesnativeagents: https://github.com/kagent-dev/kagent/tree/main Agent-user integration protocol: https://docs.ag-ui.com/introduction</p> <p>Compute agents: https://github.com/simular-ai/Agent-S</p>"},{"location":"AI%20Agents%20Engineering/introduction/","title":"Introduction","text":""},{"location":"AI%20Agents%20Engineering/introduction/#introduction-to-ai-agents-and-agentic-ai","title":"Introduction to AI Agents and Agentic AI","text":"<p>Artificial Intelligence (AI) is rapidly evolving. Beyond its well-known capabilities in pattern recognition, prediction, and generation, AI is increasingly being developed to act more autonomously to achieve specific goals. This shift is leading to the rise of systems often described as \"AI agents\" or exhibiting \"agentic AI.\" These terms refer to AI that can perceive its environment, make decisions, and take actions in a more independent and goal-directed manner.</p> <p>This chapter aims to demystify AI agents and the broader concept of agentic AI. We will explore their core definitions, distinguish between them, and delve into how they are designed and built, particularly with the advent of powerful Large Language Models (LLMs) that can serve as their reasoning engines. Furthermore, we will examine their diverse applications, current limitations, and the important ethical considerations and risks associated with their increasing capabilities and autonomy. Understanding these concepts is crucial for anyone looking to leverage or navigate the next wave of AI innovation.</p>"},{"location":"AI%20Agents%20Engineering/introduction/#defining-ai-agents","title":"Defining AI Agents","text":"<p>An AI agent is an autonomous entity designed to operate within an environment to achieve specific goals. It does this by perceiving its surroundings, making decisions, and then taking actions. Think of an agent as a system that can sense, think, and act.</p>"},{"location":"AI%20Agents%20Engineering/introduction/#core-components-of-an-ai-agent","title":"Core Components of an AI Agent","text":"<p>The behavior and capabilities of an AI agent are typically defined by several key components:</p> <ul> <li>Perception: This is how the agent gathers information about its current state and the environment it operates in. For a software agent, this could be new data inputs, API responses, or user messages. For a robot, it would be data from its sensors (cameras, microphones, lidar, etc.).</li> <li>Reasoning/Decision-Making: This is the \"brain\" of the agent. It's the process by which the agent processes its perceived information, potentially considers its internal state or knowledge, and chooses what action to take next to best achieve its goals. This logic can range from simple if-then rules in basic agents to complex machine learning models or even Large Language Models (LLMs) in more advanced agents.</li> <li>Action: These are the operations the agent performs on its environment. An action could be generating a piece of text, calling a software API, sending a command to a robotic actuator, making a recommendation, or filtering data.</li> <li>Environment: This is the world or context in which the agent exists and operates. It can be physical (like a room for a robot), virtual (like a software application, a game world, or the internet), or a hybrid. Agents are designed to interact with and respond to changes in their environment.</li> <li>Goals: These are the objectives or desired outcomes the agent is programmed or has learned to achieve. Goals drive the agent's decision-making process. They can be simple (e.g., \"keep the room temperature at 22\u00b0C\") or complex (e.g., \"successfully book a flight itinerary that meets multiple user constraints\").</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#types-of-agents-high-level-overview","title":"Types of Agents (High-Level Overview)","text":"<p>AI agents can vary significantly in their complexity and how they make decisions. Based on the classic AI textbook \"Artificial Intelligence: A Modern Approach\" by Russell and Norvig, some general categories include:</p> <ul> <li>Simple Reflex Agents: These agents select actions based only on the current percept, ignoring the rest of the percept history. They follow simple condition-action rules (e.g., \"if car_in_front_brakes, then initiate_braking\").</li> <li>Model-Based Reflex Agents: These agents maintain an internal state to keep track of the part of the world they can't see currently. This internal model helps them handle partially observable environments.</li> <li>Goal-Based Agents: These agents have explicit goal information that describes desirable situations. They combine this with information about the results of possible actions to choose actions that will achieve their goals (often involving search and planning).</li> <li>Utility-Based Agents: When there are conflicting goals or multiple ways to achieve a goal, utility-based agents choose the action that maximizes their expected \"utility\" or \"happiness.\" This allows for more nuanced decision-making in complex scenarios.</li> <li>Learning Agents: These agents can improve their performance over time by learning from their experiences. They can adapt to new environments or become better at achieving their goals.</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#simple-examples-of-ai-agents","title":"Simple Examples of AI Agents","text":"<p>To make the concept more concrete, here are a few examples:</p> <ul> <li>Thermostat: A basic example. It perceives the room temperature, decides if it's above or below the target, and acts by turning the heating or cooling system on or off. Its goal is to maintain a set temperature.</li> <li>Robotic Vacuum (e.g., Roomba): It perceives its environment using sensors to detect dirt, obstacles, and room boundaries. It decides on a cleaning path and acts by moving, rotating brushes, and suctioning dirt. Its goal is to clean the floor area efficiently.</li> <li>Email Spam Filter: It perceives the content and metadata of an incoming email. Based on learned patterns or rules, it decides whether the email is spam or not. It then acts by either allowing the email into the inbox or moving it to the spam folder. Its goal is to filter out unwanted emails.</li> <li>Non-Player Character (NPC) in a Video Game: A simple enemy NPC might perceive the player's location and status. It decides whether to attack, flee, or hide based on predefined rules or simple logic. It then acts by moving, attacking, or playing an animation. Its goal might be to challenge the player or survive.</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#understanding-agentic-ai","title":"Understanding Agentic AI","text":"<p>While \"AI agent\" refers to a somewhat formally defined entity, \"Agentic AI\" describes a broader paradigm or a set of characteristics that AI systems can exhibit. It emphasizes the AI's capacity to act with a significant degree of autonomy, proactivity, and reactivity in pursuing goals, particularly within complex and dynamic environments. Agentic AI is less about a specific system architecture and more about the quality and degree of intelligent, independent, goal-directed behavior a system demonstrates.</p>"},{"location":"AI%20Agents%20Engineering/introduction/#the-role-of-large-language-models-llms-in-agentic-ai","title":"The Role of Large Language Models (LLMs) in Agentic AI","text":"<p>The recent surge in interest and capability in agentic AI is largely fueled by advancements in Large Language Models (LLMs). LLMs can serve as the sophisticated \"brain\" or reasoning and decision-making engine for agentic systems. Their powerful capabilities enable:</p> <ul> <li>Advanced Natural Language Understanding: LLMs can interpret complex user requests, extract intent, and understand nuanced information from text-based environments or tool outputs.</li> <li>Complex Reasoning and Planning: They can break down high-level goals into smaller, manageable steps, create plans, and adapt those plans based on new information.</li> <li>In-Context Learning and Decision-Making: LLMs can leverage their vast pretrained knowledge and the current context to make informed decisions.</li> <li>Tool Use and Action Generation: They can learn to interact with external tools, APIs, or software by generating the necessary commands or code, and then process the results to inform subsequent actions.</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#key-characteristics-of-agentic-ai-systems","title":"Key Characteristics of Agentic AI Systems","text":"<p>Systems exhibiting agentic AI typically showcase several key characteristics:</p> <ul> <li>Autonomy: They can operate and make decisions without constant direct human supervision for extended periods to achieve their goals.</li> <li>Proactivity: They don't just passively wait for commands; they can take initiative, anticipate needs, or explore strategies to achieve their objectives.</li> <li>Reactivity: They can perceive changes in their environment (or new information) and respond appropriately, adjusting their plans or actions as needed.</li> <li>Goal-Orientation: Their actions are consistently driven by and directed towards achieving predefined or dynamically set goals.</li> <li>Learning and Adaptation (Often): Many sophisticated agentic systems possess the ability to learn from past interactions, feedback, or outcomes, improving their performance and strategies over time.</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#examples-of-agentic-ai-in-action","title":"Examples of Agentic AI in Action","text":"<p>Agentic AI goes beyond the simpler agent examples. Here are some illustrations of more complex agentic systems:</p> <ul> <li>Advanced Virtual Assistants: Imagine an assistant that can handle a request like, \"Book a flight to London for next Tuesday, find a pet-friendly hotel near Hyde Park for three nights under $200 per night, and add the details to my calendar.\" This requires planning, interacting with multiple tools (flight booking, hotel search, calendar API), and decision-making based on constraints.</li> <li>Autonomous Research Agents: An AI system tasked with \"Investigate the latest advancements in quantum computing for drug discovery.\" Such an agent might autonomously browse research paper databases, summarize key findings, identify leading researchers, and even propose new research directions based on the information gathered.</li> <li>AI Systems with Tool Use (Tool-Augmented LLMs): An LLM-powered agent that, when asked a complex question, can decide to use a web search for current information, a calculator for precise math, or a code interpreter to run a simulation, then synthesize the results from these tools into a coherent answer.</li> <li>Sophisticated Non-Player Characters (NPCs) in Games: NPCs in advanced simulations or games that have their own long-term goals, memories of past interactions, can form complex relationships with other characters (including the player), and adapt their behavior dynamically based on the evolving game world.</li> </ul> <p>These examples highlight how agentic AI systems leverage their components and characteristics to perform complex, multi-step tasks with a significant level of independence.</p>"},{"location":"AI%20Agents%20Engineering/introduction/#key-differences-ai-agents-vs-agentic-ai","title":"Key Differences: AI Agents vs. Agentic AI","text":"<p>While the terms \"AI Agent\" and \"Agentic AI\" are closely related and often used in overlapping contexts, they have slightly different nuances:</p> <ul> <li> <p>AI Agent: Typically refers to a more formally defined computational entity or system designed with specific components: perception, reasoning/decision-making, action capabilities, operating within an environment to achieve set goals. The definition often emphasizes the agent's architecture and its role as an individual actor.</p> </li> <li> <p>Agentic AI: Describes a quality or characteristic of an AI system. It signifies that the system exhibits a high degree of autonomy, proactivity, reactivity, and goal-orientation in its behavior. It's less about a specific type of system and more about how the system operates and the sophistication of its independent, goal-driven actions.</p> </li> </ul> <p>Here's a breakdown of key differentiators:</p> <ul> <li>Scope and Definition:</li> <li>AI Agent: Often refers to a specific instance or a well-defined class of system (e.g., a reinforcement learning agent, a BDI agent). Its definition is rooted in its structure and components.</li> <li> <p>Agentic AI: A broader behavioral description. A system is \"agentic\" if it acts like an agent, especially with notable autonomy and intelligence.</p> </li> <li> <p>Emphasis:</p> </li> <li>AI Agent: Emphasis is often on the architectural design \u2013 the mechanisms for perception, decision-making, and action.</li> <li> <p>Agentic AI: Emphasis is on the observable traits \u2013 the degree of self-governance, initiative, responsiveness, and purposeful behavior.</p> </li> <li> <p>Complexity and Sophistication:</p> </li> <li>AI Agent: Can range from very simple (like a thermostat) to highly complex.</li> <li> <p>Agentic AI: While a simple agent is technically \"agentic,\" the term \"agentic AI\" is more frequently used today to describe systems with a higher level of sophistication, often involving complex reasoning (like that provided by LLMs), learning, and adaptation in dynamic environments.</p> </li> <li> <p>Formalism vs. Popular Usage:</p> </li> <li>AI Agent: Has a longer, more formal history in AI research (e.g., concepts from textbooks like Russell and Norvig's \"Artificial Intelligence: A Modern Approach\").</li> <li>Agentic AI: A more recent term that has gained prominence with the rise of LLMs capable of driving more complex and autonomous behaviors, sometimes even if the system isn't explicitly designed following a classical agent architecture.</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#analogy-vehicle-vs-advanced-autonomous-driving","title":"Analogy: Vehicle vs. Advanced Autonomous Driving","text":"<p>Consider the difference between a \"vehicle\" and \"advanced autonomous driving capability\":</p> <ul> <li>A bicycle is a simple type of vehicle (an AI Agent). It has components for motion and direction.</li> <li>A self-driving truck navigating complex city traffic and long-haul routes to deliver cargo exhibits a high degree of advanced autonomous driving capability (Agentic AI). It's not just a vehicle; it's operating with significant autonomy, making complex decisions, and proactively pursuing its goal. The truck itself is a sophisticated agent, and \"agentic\" powerfully describes how it operates.</li> </ul>"},{"location":"AI%20Agents%20Engineering/introduction/#relationship","title":"Relationship","text":"<p>Essentially, an advanced AI agent is expected to exhibit strong agentic AI characteristics. The more complex, autonomous, proactive, and adaptive an AI agent is, the more \"agentic\" it is considered. However, a system (like an LLM orchestrating a few tools based on a prompt) might be described as demonstrating \"agentic behavior\" even if it's not formally defined or built as a classical AI agent from the ground up. The key is the observed capacity for intelligent, goal-directed action.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/","title":"CPU vs GPU vs TPU","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/#what-is-a-cpu","title":"What is a CPU ?","text":"<p>Central Processing Units (CPUs) are general-purpose processors used in all computers and servers. CPUs are widely available and suitable for running small models or serving infrequent requests. However, they lack the parallel processing power to run LLMs efficiently. For production-grade LLM inference, especially with larger models or high request volumes, CPUs often fall short in both latency and throughput.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/#what-is-a-gpu","title":"What is a GPU ?","text":"<p>Graphics Processing Units (GPUs) were originally designed for graphics rendering and digital visualization tasks. As they could perform highly parallel operations, they also turned out to be a great fit for ML and AI workloads. Today, GPUs are the default choice for both training and inference of GenAI like LLMs.</p> <p>The architecture of GPUs is optimized for matrix multiplication and tensor operations, which are core components of transformer-based models. Modern inference frameworks and runtimes (e.g., vLLM, SGLang, LMDeploy, TensorRT-LLM, and Hugging Face TGI) are designed to take full advantage of GPU acceleration.</p> <p>You can find here a fascinating (rather dense) free book on GPU and how to make them work collectively and at scale (as this is the main goal): https://jax-ml.github.io/scaling-book/gpus/#what-is-a-gpu</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/#what-is-a-tpu","title":"What is a TPU ?","text":"<p>Tensor Processing Units (TPUs) are custom-built by Google to accelerate AI workloads like training and inference. Compared with GPUs, TPUs are designed from the ground up for tensor operations \u2014 the fundamental math behind neural networks. This specialization makes TPUs faster and more efficient than GPUs for many AI-based compute tasks, like LLM inference.</p> <p>TPUs are behind some of the most advanced AI applications today: agents, recommendation systems and personalization, image, video &amp; audio synthesis, and more. Google uses TPUs in Search, Photos, Maps, and to power Gemini and DeepMind models.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/#so-what","title":"So what ?","text":"<p>First, make sure that your use case requires beyond CPU usage as this is a whole new world. Then, while TPU have been created specifically for matrices operations like ML/LLM models, there is still, to date, a lot of infrastructure running on GPUs.</p> <p>Not all models architecture support TPU infrastructure which shows GPUs are more general purpose compute oriented. TPU (if model allows) is normally more efficient at inference time for large scale recommendation engines or training large LLMs.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Debugging%20Pytorch/","title":"Debugging Pytorch - WIP","text":"<p>Follow this link: See Debug.</p> <p>https://sebastianraschka.com/teaching/pytorch-1h/</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/","title":"Deep Learning: Optimization techniques and limitations","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#first-order-optimization-methods","title":"First-Order Optimization Methods","text":"<p>Overview of first-order optimization methods include Stochastic Gradient Descent, Adagrad, Adadelta, and RMSprop, as well as recent momentum-based and adaptive gradient methods such as Nesterov accelerated gradient, Adam, Nadam, AdaMax, and AMSGrad.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#1-stochastic-gradient-descent-sgd","title":"1. Stochastic Gradient Descent (SGD)","text":"<ul> <li>Basic Principle: Updates weights using gradients from small batches</li> <li>Advantages: Simple, memory efficient</li> <li>Disadvantages: Slow convergence, sensitive to learning rate</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#2-momentum-based-methods","title":"2. Momentum-Based Methods","text":"<ul> <li>SGD with Momentum: Accumulates gradients to overcome local minima</li> <li>Nesterov Accelerated Gradient: Looks ahead before making updates</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#3-adaptive-learning-rate-methods","title":"3. Adaptive Learning Rate Methods","text":"<ul> <li>AdaGrad: Adapts learning rate based on historical gradients</li> <li>RMSprop: Addresses AdaGrad's diminishing learning rate problem</li> <li>Adam: Combines momentum and adaptive learning rates</li> <li>AdaMax: Variant of Adam based on infinity norm</li> <li>AMSGrad: Addresses convergence issues in Adam</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#model-optimization-techniques","title":"Model Optimization Techniques","text":"<p>Optimization techniques like pruning, quantization, and knowledge distillation are vital for improving computational efficiency.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#1-pruning","title":"1. Pruning","text":"<ul> <li>Purpose: Reduces model size by removing less important neurons, involving identification, elimination, and optional fine-tuning</li> <li>Types: Structured vs. unstructured pruning</li> <li>Benefits: Smaller models, faster inference, lower memory usage</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#2-quantization","title":"2. Quantization","text":"<ul> <li>Purpose: Decreases memory usage by reducing precision of weights and activations</li> <li>Types: Post-training quantization, quantization-aware training</li> <li>Benefits: Reduced memory footprint, faster computation</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#3-knowledge-distillation","title":"3. Knowledge Distillation","text":"<ul> <li>Purpose: Transfer knowledge from large \"teacher\" models to smaller \"student\" models</li> <li>Process: Student learns to mimic teacher's outputs</li> <li>Benefits: Maintains performance while reducing model size</li> </ul> <p>Examples:</p> <ul> <li>https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/7_1_knowledge_distillation_Llama.ipynb</li> <li>https://github.com/predibase/llm_distillation_playbook</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#advanced-optimization-strategies","title":"Advanced Optimization Strategies","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#1-learning-rate-scheduling","title":"1. Learning Rate Scheduling","text":"<ul> <li>Techniques: Step decay, exponential decay, cosine annealing</li> <li>Purpose: Adjust learning rate during training for better convergence</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#2-batch-normalization","title":"2. Batch Normalization","text":"<ul> <li>Purpose: Normalizes inputs to each layer</li> <li>Benefits: Faster training, improved stability, regularization effect</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#3-gradient-clipping","title":"3. Gradient Clipping","text":"<ul> <li>Purpose: Prevents exploding gradients by limiting gradient magnitude</li> <li>Implementation: Norm clipping, value clipping</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#4-early-stopping","title":"4. Early Stopping","text":"<ul> <li>Purpose: Prevents overfitting by stopping training when validation performance stops improving</li> <li>Implementation: Monitor validation loss with patience parameter</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#1-dropout","title":"1. Dropout","text":"<ul> <li>Purpose: Randomly deactivates neurons during training</li> <li>Benefits: Prevents overfitting, improves generalization</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#2-l1l2-regularization","title":"2. L1/L2 Regularization","text":"<ul> <li>L1 (Lasso): Adds absolute value of weights to loss function</li> <li>L2 (Ridge): Adds squared weights to loss function</li> <li>Purpose: Prevents overfitting by penalizing large weights</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#3-data-augmentation","title":"3. Data Augmentation","text":"<ul> <li>Purpose: Artificially increases training data through transformations</li> <li>Techniques: Rotation, scaling, cropping, flipping for images</li> <li>Benefits: Improves generalization, reduces overfitting</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#4-ensemble-methods","title":"4. Ensemble Methods","text":"<ul> <li>Purpose: Combines predictions from multiple models</li> <li>Techniques: Bagging, boosting, stacking</li> <li>Benefits: Improved performance, reduced variance</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#major-limitations-and-challenges","title":"Major Limitations and Challenges","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#1-vanishing-gradient-problem","title":"1. Vanishing Gradient Problem","text":"<p>What it is: The vanishing gradient problem is a challenge that emerges during backpropagation when the derivatives or slopes of the activation functions become progressively smaller as we move backward through the layers of a neural network.</p> <p>Consequences: Slow convergence, network getting stuck in low minima, and impaired learning of deep representations.</p> <p>Solutions:</p> <ul> <li>Use ReLU activation functions</li> <li>Implement residual connections (ResNet)</li> <li>Apply batch normalization</li> <li>Use LSTM/GRU for sequential data</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#2-catastrophic-forgetting","title":"2. Catastrophic Forgetting","text":"<p>What it is: Deep learning models can struggle to learn new tasks and update their knowledge without access to previous data, leading to a significant loss of accuracy known as Catastrophic Forgetting.</p> <p>Why it happens: The continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference.</p> <p>Solutions:</p> <ul> <li>EWC (Elastic Weight Consolidation) algorithm allows knowledge of previous tasks to be protected during new learning by selectively decreasing the plasticity of weights</li> <li>Progressive neural networks</li> <li>Memory replay techniques</li> <li>Multi-task learning approaches</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#3-computational-requirements","title":"3. Computational Requirements","text":"<p>High Resource Needs:</p> <ul> <li>Require significant computational power (GPUs/TPUs)</li> <li>Large memory requirements for training</li> <li>Long training times for complex models</li> </ul> <p>Solutions:</p> <ul> <li>Model compression techniques</li> <li>Distributed training</li> <li>Transfer learning</li> <li>Efficient architectures (MobileNets, EfficientNets)</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#sources","title":"Sources","text":"<ul> <li>Future of Deep Learning according to top AI Experts in 2025</li> <li>Optimization Methods in Deep Learning: A Comprehensive Overview - ArXiv Paper</li> <li>Deep Learning Modelling Techniques: Current Progress, Applications, Advantages, and Challenges - Artificial Intelligence Review</li> <li>Deep Learning Model Optimization Methods - Neptune.ai Blog</li> <li>Vanishing and Exploding Gradients Problems in Deep Learning - GeeksforGeeks</li> <li>Overcoming Catastrophic Forgetting in Neural Networks - PNAS</li> <li>Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy - ArXiv</li> <li> </li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/#artificial-neural-variability-for-deep-learning-on-overfitting-noise-memorization-and-catastrophic-forgetting-researchgate","title":"Artificial Neural Variability for Deep Learning: On Overfitting, Noise Memorization, and Catastrophic Forgetting - ResearchGate","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/","title":"Deep Learning: Comprehensive Guide to Techniques","text":"<p>This page aims at giving a very very high overview of Deep Learning. It is not exhaustive. If you're interesting to dig deeper, many great online courses are free. I personally found this one from Sebastien Raschka really well done. You can even get a free certification (from lighting AI) here.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#what-is-deep-learning","title":"What is Deep Learning?","text":"<p>Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to model and understand complex patterns in data. Unlike traditional machine learning approaches that require manual feature engineering, deep learning can automatically learn hierarchical representations of data through multiple levels of abstraction.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Neural Networks: Computational models inspired by biological neural networks</li> <li>Layers: Different levels of data processing, from input to output</li> <li>Weights and Biases: Parameters that the network learns during training</li> <li>Activation Functions: Functions that determine whether a neuron should be activated</li> <li>Backpropagation: Algorithm for calculating gradients and updating weights</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#core-deep-learning-architectures","title":"Core Deep Learning Architectures","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#1-convolutional-neural-networks-cnns","title":"1. Convolutional Neural Networks (CNNs)","text":"<p>What they are: CNNs are specialized neural networks designed for processing grid-like data such as images. They use convolutional layers that apply filters to detect local features.</p> <p>How they work:</p> <ul> <li>Convolution: Applies filters to detect features like edges, textures</li> <li>Pooling: Reduces spatial dimensions while retaining important information</li> <li>Feature Maps: Representations of detected features at different layers</li> </ul> <p>Key Components:</p> <ul> <li>Convolutional layers</li> <li>Pooling layers (max, average)</li> <li>Fully connected layers</li> <li>Activation functions (ReLU, sigmoid, tanh)</li> </ul> <p>Applications: Image classification, object detection, medical imaging, autonomous vehicles</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#2-recurrent-neural-networks-rnns","title":"2. Recurrent Neural Networks (RNNs)","text":"<p>What they are: RNNs are designed to process sequential data by maintaining a hidden state that captures information from previous time steps.</p> <p>How they work:</p> <ul> <li>Process sequences one element at a time</li> <li>Maintain internal memory through hidden states</li> <li>Share parameters across time steps</li> </ul> <p>Variants:</p> <ul> <li>Vanilla RNN: Basic recurrent architecture</li> <li>LSTM (Long Short-Term Memory): Addresses vanishing gradient problem with gates</li> <li>GRU (Gated Recurrent Unit): Simplified version of LSTM</li> </ul> <p>Applications: Natural language processing, speech recognition, time series prediction, machine translation</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#3-transformer-architecture","title":"3. Transformer Architecture","text":"<p>What they are: Transformers diverge from traditional methods that relied on recurrent layers, instead employing self-attention mechanisms that allow for parallel processing of input data.</p> <p>How they work:</p> <ul> <li>Self-Attention: Every element in the input data connects, or pays attention, to every other element, allowing the transformer to see traces of the entire data set as soon as it starts training</li> <li>Encoder-Decoder Structure: Separate components for understanding input and generating output</li> <li>Multi-Head Attention: Multiple attention mechanisms working in parallel</li> </ul> <p>Key Innovations:</p> <ul> <li>The key innovation in transformers is the attention mechanism, allowing the model to weigh different parts of the input data, crucial for understanding context and relationships</li> <li>Parallel processing capabilities</li> <li>Better handling of long sequences</li> </ul> <p>Applications: Language models (GPT, BERT), machine translation, image generation (DALL-E), video generation (Sora)</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#4-generative-adversarial-networks-gans","title":"4. Generative Adversarial Networks (GANs)","text":"<p>What they are: GANs consist of two neural networks competing against each other - a generator that creates fake data and a discriminator that tries to distinguish real from fake data.</p> <p>How they work:</p> <ul> <li>Generator: Creates synthetic data samples</li> <li>Discriminator: Classifies data as real or fake</li> <li>Adversarial Training: Both networks improve through competition</li> </ul> <p>Variants:</p> <ul> <li>Conditional GANs (cGANs)</li> <li>Wasserstein GANs (WGANs)</li> <li>Progressive GANs</li> <li>StyleGANs</li> </ul> <p>Applications: Image generation, data augmentation, art creation, deepfakes</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#5-autoencoders","title":"5. Autoencoders","text":"<p>What they are: Neural networks that learn to compress data into a lower-dimensional representation and then reconstruct it.</p> <p>Components:</p> <ul> <li>Encoder: Compresses input into latent representation</li> <li>Decoder: Reconstructs original input from latent representation</li> <li>Bottleneck: Compressed representation layer</li> </ul> <p>Types:</p> <ul> <li>Vanilla Autoencoders</li> <li>Variational Autoencoders (VAEs)</li> <li>Denoising Autoencoders</li> <li>Sparse Autoencoders</li> </ul> <p>Applications: Dimensionality reduction, anomaly detection, data compression, feature learning</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/#sources","title":"Sources","text":"<ul> <li>Deep Learning Architectures From CNN, RNN, GAN, and Transformers - MarkTechPost</li> <li>Transformer (deep learning architecture) - Wikipedia</li> <li>Review of Deep Learning: Concepts, CNN Architectures, Challenges, Applications - Journal of Big Data</li> <li>CNN vs. RNN: How are they different? - TechTarget</li> <li>A Comprehensive Review of Deep Learning: Architectures, Recent Advances, and Applications - MDPI</li> <li>https://github.com/Devinterview-io/deep-learning-interview-questions</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/","title":"Understanding Entropy: From Information Theory to Deep Learning","text":"<p>\u201cKnowledge is power. Information is liberating. Education is the premise of progress, in every society, in every family.\u201d, Kofi Annan. From my overall experience, I always want to understand what the machine (AI) is trying to model and understand.</p> <p>This page aims at explaining very simply what is entropy and how important it is.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#what-is-entropy","title":"What is Entropy?","text":"<p>Machine Learning and Deep Learning is about learning, understanding information contained in some data and the ability to generalize this understanding. Entropy is a measure of uncertainty, randomness, or \"surprise\" in information. Think of it as answering the question: \"How much information do I need to describe what's happening?\"</p> <p>The more unpredictable something is, the higher its entropy. The more predictable something is, the lower its entropy.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#5-year-old-example-the-toy-box-game","title":"5-Year-Old Example: The Toy Box Game \ud83e\uddf8","text":"<p>Imagine you have a toy box with different colored balls:</p> <p>Low Entropy Toy Box:</p> <ul> <li>9 red balls, 1 blue ball</li> <li>If I ask you to guess what color I'll pick, you'd almost always say \"red\" and be right!</li> <li>This is predictable = LOW entropy</li> </ul> <p>High Entropy Toy Box:</p> <ul> <li>5 red balls, 5 blue balls  </li> <li>Now guessing is much harder - it's like flipping a coin!</li> <li>This is unpredictable = HIGH entropy</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#shannon-entropy","title":"Shannon Entropy","text":"<p>Shannon entropy measures the average amount of information needed to describe the outcome of a random event.</p> <p>Formula:</p> <pre><code>H(X) = -\u03a3 p(x) \u00d7 log\u2082(p(x))\n</code></pre> <p>Where:</p> <ul> <li>H(X) = Shannon entropy</li> <li>p(x) = probability of outcome x</li> <li>log\u2082 = logarithm base 2 (measures information in \"bits\")</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#5-year-old-example-guessing-games","title":"5-Year-Old Example: Guessing Games \ud83c\udfaf","text":"<p>Let's use our toy box example:</p> <p>Low Entropy Box (9 red, 1 blue):</p> <ul> <li>Probability of red: 9/10 = 0.9</li> <li>Probability of blue: 1/10 = 0.1</li> <li>Shannon entropy = -(0.9 \u00d7 log\u2082(0.9) + 0.1 \u00d7 log\u2082(0.1)) \u2248 0.47 bits</li> </ul> <p>High Entropy Box (5 red, 5 blue):</p> <ul> <li>Probability of red: 5/10 = 0.5  </li> <li>Probability of blue: 5/10 = 0.5</li> <li>Shannon entropy = -(0.5 \u00d7 log\u2082(0.5) + 0.5 \u00d7 log\u2082(0.5)) = 1.0 bits</li> </ul> <p>What this means: You need more \"yes/no questions\" on average to figure out what color was picked from the balanced box!</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#cross-entropy","title":"Cross Entropy","text":"<p>Definition: Cross entropy measures how well one probability distribution (your guess) predicts another probability distribution (the actual truth).</p> <p>Formula:</p> <pre><code>H(p,q) = -\u03a3 p(x) \u00d7 log(q(x))\n</code></pre> <p>Where:</p> <ul> <li>p(x) = true probability distribution</li> <li>q(x) = predicted probability distribution</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#kl-divergence-kullback-leibler-divergence","title":"KL Divergence (Kullback-Leibler Divergence)","text":"<p>Definition: KL divergence measures how different one probability distribution is from another. It's like asking: \"How much extra information do I need if I use the wrong distribution instead of the right one?\"</p> <p>Formula:</p> <pre><code>D_KL(P||Q) = \u03a3 p(x) \u00d7 log(p(x)/q(x))\n</code></pre> <p>Where:</p> <ul> <li>P = true distribution</li> <li>Q = approximate distribution</li> </ul> <p>Key Property: KL divergence is always \u2265 0, and equals 0 only when P = Q (distributions are identical)</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#how-they-all-connect","title":"How They All Connect \ud83d\udd17","text":"<p>Think of it like this:</p> <ol> <li>Shannon Entropy: \"How hard is it to guess what will happen?\"</li> <li>Cross Entropy: \"How surprised am I by what actually happened, given my predictions?\"</li> <li>KL Divergence: \"How much worse is my guessing strategy compared to the perfect strategy?\"</li> </ol> <p>Mathematical Relationship:</p> <pre><code>Cross Entropy = Shannon Entropy + KL Divergence\nH(p,q) = H(p) + D_KL(p||q)\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#why-this-matters-in-machine-learning-and-deep-learning","title":"Why This Matters in Machine Learning and Deep Learning \ud83e\udd16","text":"<p>Shannon entropy helps understand data complexity and the unpredictability of language itself. Cross entropy is commonly used as a loss function for classification. KL divergence helps in regularization and comparing model outputs.</p> <p>The Goal: Train models to make predictions that minimize cross entropy (be less surprised by the actual answers) and minimize KL divergence (be as close as possible to the true distribution).</p> Concept What it Measures 5-Year-Old Version Shannon Entropy Uncertainty in data \"How hard is the guessing game?\" Cross Entropy Prediction quality \"How surprised am I by the real answer?\" KL Divergence Difference between distributions \"How much worse is my strategy than the perfect one?\""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#kl-divergence-in-llms","title":"KL Divergence in LLMs","text":"<p>Application 1: Temperature Sampling</p> <p>When generating text, LLMs can adjust their \"creativity\" using temperature:</p> <p>Original model output: \"sunny\" (60%), \"cloudy\" (25%), \"rainy\" (10%), \"windy\" (5%)</p> <p>Low temperature (0.2) - Conservative:</p> <ul> <li>\"sunny\" (80%), \"cloudy\" (15%), \"rainy\" (4%), \"windy\" (1%)</li> <li>KL divergence: Small (close to original)</li> <li>Result: More predictable, \"safer\" text</li> </ul> <p>High temperature (1.5) - Creative:</p> <ul> <li>\"sunny\" (35%), \"cloudy\" (30%), \"rainy\" (20%), \"windy\" (15%)</li> <li>KL divergence: Large (very different from original)</li> <li>Result: More surprising, creative text</li> </ul> <p>Application 2: Model Alignment &amp; RLHF</p> <p>When training models to be helpful and safe:</p> <ul> <li>Base Model Distribution: Might generate toxic or unhelpful content</li> <li>Target Distribution: Should generate helpful, harmless content</li> </ul> <p>KL Divergence Constraint: Keep the aligned model close enough to the base model so it doesn't \"forget\" how to speak naturally, but far enough to be safe and helpful.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#real-world-llm-example-autocomplete","title":"Real-World LLM Example: Autocomplete","text":"<p>Scenario: You're typing an email: \"Thank you for your help with the ___\"</p> <p>What happens inside the LLM:</p> <ol> <li>Shannon Entropy Analysis:</li> <li>Model calculates: How predictable is this context?</li> <li> <p>High-frequency patterns \u2192 Lower entropy \u2192 More confident predictions</p> </li> <li> <p>Cross Entropy Training:</p> </li> <li>Model was trained on millions of similar sentences</li> <li>Learned that \"project\" (30%), \"meeting\" (20%), \"presentation\" (15%) are common</li> <li> <p>Training minimized cross entropy on real email data</p> </li> <li> <p>KL Divergence in Practice:</p> </li> <li>Beam Search: Explores multiple completions, uses KL divergence to balance between probable and diverse options</li> <li>Safety Filtering: Ensures suggestions don't diverge too much from appropriate business language</li> </ol>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#why-this-matters-for-llm-performance","title":"Why This Matters for LLM Performance","text":"<p>Better Entropy Understanding = Better Models:</p> <ol> <li>Training Efficiency: Cross entropy loss guides the model toward better predictions</li> <li>Text Generation Quality: Entropy measures help balance creativity vs. coherence  </li> <li>Model Evaluation: Perplexity (related to cross entropy) measures how \"surprised\" the model is by test data</li> <li>Safety &amp; Alignment: KL divergence helps keep models helpful while maintaining capabilities</li> </ol>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#perplexity-the-llm-metric","title":"Perplexity: The LLM Metric","text":"<p>Perplexity formula:</p> <pre><code>perplexity = exp( H(p,q) )\n</code></pre> <p>What it means: \"On average, how many reasonable word choices does the model think there are?\"</p> <p>Example:</p> <ul> <li>Cross entropy = 1.0 \u2192 Perplexity = 2 \u2192 \"Like choosing between 2 equally good options\"</li> <li>Cross entropy = 3.0 \u2192 Perplexity = 8 \u2192 \"Like choosing among 8 reasonable options\"</li> </ul> <p>Better models have lower perplexity = they're less \"confused\" by language! Perplexity, then, is essentially a measure of how many options the model finds plausible on average, with lower values indicating fewer options (more confident predictions) and higher values indicating more options (greater uncertainty).</p> <p>Advantage: One of the biggest advantages of perplexity is that it is highly intuitive and explainable in a field that is notoriously opaque</p> <p>Inconvenient: The most important limitation of perplexity is that it does not convey a model\u2019s \u201cunderstanding.\u201d Perplexity is strictly a measure of uncertainty, and a model being uncertain doesn\u2019t mean it is right or wrong. A model may be correct but unconfident or wrong but confident. So, a perplexity score isn\u2019t a measure of accuracy, just of confidence.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Entropy/#python-code-snippet","title":"Python Code snippet","text":"<p>Here's a simple python calculation using pytorch:</p> <pre><code>import torch\n\ndef calculate_perplexity(logits, target):\n    \"\"\"\n    Calculate perplexity from logits and target labels.\n\n    Args:\n    - logits (torch.Tensor): Logits output from the model (batch_size, seq_length, vocab_size).\n    - target (torch.Tensor): Ground truth labels (batch_size, seq_length).\n\n    Returns:\n    - perplexity (float): The perplexity score.\n    \"\"\"\n\n    # Convert logits to log probabilities\n    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n\n    # Gather the log probabilities for the correct target tokens\n    # log_probs has shape (batch_size, seq_length, vocab_size)\n    # target has shape (batch_size, seq_length)\n    # The gather method will pick the log probabilities of the true target tokens\n    target_log_probs = log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n\n    # Calculate the negative log likelihood\n    negative_log_likelihood = -target_log_probs\n\n    # Calculate the mean negative log likelihood over all tokens\n    mean_nll = negative_log_likelihood.mean()\n\n    # Calculate perplexity as exp(mean negative log likelihood)\n    perplexity = torch.exp(mean_nll)\n\n    return perplexity.item()\n\n# Example usage\n# Simulate a batch of logits (batch_size=2, seq_length=4, vocab_size=10)\nlogits = torch.randn(2, 4, 10)\n# Simulate ground truth target tokens\ntarget = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n\n# Calculate perplexity\nperplexity = calculate_perplexity(logits, target)\nprint(f'Perplexity: {perplexity}')\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/","title":"GPU Optimization","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#description-useful-tips-for-optimizing-pytorch-runs","title":"description: Useful tips for optimizing Pytorch runs","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#gpu-optimization","title":"GPU Optimization","text":"<p>Optimizing GPU utilization is paramount in deep learning due to the sheer volume of computations involved. Deep learning models, especially large ones, rely heavily on operations like matrix multiplications and convolutions, which can be computationally intensive. GPUs, with their massively parallel architecture consisting of thousands of cores, are specifically designed to execute these types of operations far more efficiently than Central Processing Units (CPUs). Leveraging GPUs effectively translates to substantially faster training times, which in turn allows for more rapid experimentation, iteration, and the feasibility of developing more complex and powerful models.</p> <p>The strength of GPUs lies in their nature as parallel processors. They excel at Single Instruction, Multiple Data (SIMD) tasks, where the same operation is performed simultaneously on many data elements. This paradigm is a perfect match for the tensor and matrix operations that form the backbone of deep learning computations. In contrast, CPUs are typically optimized for sequential task execution or handling a smaller number of parallel threads, making them less suitable for the large-scale parallel computations inherent in training deep neural networks.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#basic-gpu-workflow-in-pytorch","title":"Basic GPU Workflow in PyTorch","text":"<p>Understanding how to manage computations between the CPU and GPU is fundamental for leveraging PyTorch effectively. Here's a breakdown of the typical workflow:</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#1-determining-device-availability-and-setting-device","title":"1. Determining Device Availability and Setting Device","text":"<p>Before performing any GPU operations, you need to check for GPU availability and define the device you intend to use.</p> <ul> <li>Check Availability: Use <code>torch.cuda.is_available()</code> to determine if a CUDA-enabled GPU is present and usable by PyTorch.</li> <li>Set Device: Create a <code>torch.device</code> object. A common practice is to set it to <code>\"cuda\"</code> if a GPU is available, otherwise fallback to <code>\"cpu\"</code>.</li> <li>GPU Count: <code>torch.cuda.device_count()</code> returns the number of available GPUs.</li> <li>Specific GPU Selection:</li> <li>If you have multiple GPUs, you can select a specific one using its index (e.g., <code>torch.device('cuda:0')</code> for the first GPU, <code>torch.device('cuda:1')</code> for the second).</li> <li><code>torch.cuda.set_device(index)</code> can set the default GPU globally for CUDA operations (less recommended for library code as it's a global state).</li> <li>Alternatively, the <code>CUDA_VISIBLE_DEVICES</code> environment variable can control which GPUs are visible to PyTorch.</li> </ul> <p>Code Example:</p> <pre><code>import torch\n\n# Check for GPU availability\nif torch.cuda.is_available():\n    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n    # Set the device to CUDA. If multiple GPUs are available, PyTorch will default to 'cuda:0'.\n    device = torch.device(\"cuda\") \n    print(f\"Primary GPU being used: {torch.cuda.current_device()} (Indices are 0-based, this shows the current default CUDA device)\")\nelse:\n    print(\"CUDA is not available. Using CPU.\")\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#2-moving-models-to-gpu","title":"2. Moving Models to GPU","text":"<p>PyTorch models (<code>torch.nn.Module</code> subclasses) need to be explicitly moved to the desired device to perform computations on that device.</p> <ul> <li>The recommended method is <code>.to(device)</code>, which is flexible and works for any <code>torch.device</code> object (CPU or GPU).</li> <li>An older method, <code>.cuda()</code>, specifically moves the model to the default GPU. While it works, <code>.to(device)</code> is preferred for consistency and better device management.</li> </ul> <p>Code Example:</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Assuming 'device' is defined from the previous example\n# # (e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 1) # A simple linear layer\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Instantiate the model (it's on CPU by default)\nmodel = SimpleModel()\nprint(f\"Model device before moving: {next(model.parameters()).device}\")\n\n# Move the model to the selected device (GPU or CPU)\nmodel.to(device)\nprint(f\"Model device after moving: {next(model.parameters()).device}\")\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#3-moving-tensors-to-gpu","title":"3. Moving Tensors to GPU","text":"<p>Similarly to models, tensors must be on the same device as the model for computations to occur between them.</p> <ul> <li>Use the <code>.to(device)</code> method to move a tensor to the target device. The older <code>.cuda()</code> method also works for moving to the default GPU.</li> <li>Crucially, all input tensors to a model's <code>forward()</code> method must reside on the same device as the model itself. PyTorch will raise an error if devices mismatch.</li> <li>Tensors can also be created directly on a specific device (e.g., <code>torch.randn(size, device=device)</code>), which avoids an explicit move.</li> </ul> <p>Code Example:</p> <pre><code>import torch\n\n# Example input tensor (initially on CPU by default)\ninput_tensor_cpu = torch.randn(5, 10)\nprint(f\"Input tensor device before moving: {input_tensor_cpu.device}\")\n\n# Move the input tensor to the same device as the model\ninput_tensor_gpu = input_tensor_cpu.to(device)\nprint(f\"Input tensor device after moving: {input_tensor_gpu.device}\")\n\n# If the model and input_tensor_gpu are on the same device, computation can proceed\n# For demonstration, if 'model' is on 'device' and 'input_tensor_gpu' is on 'device':\n# output = model(input_tensor_gpu) \n# print(f\"Output tensor device: {output.device}\") # Will be same as 'device'\n\n# Creating a tensor directly on the device (avoids explicit move)\ntensor_direct_on_device = torch.randn(2, 3, device=device)\nprint(f\"Tensor created directly on device: {tensor_direct_on_device.device}\")\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#4-bringing-tensors-back-to-cpu","title":"4. Bringing Tensors back to CPU","text":"<p>After performing computations on the GPU, you might need to move tensors back to the CPU for various reasons:</p> <ul> <li>Interacting with libraries that expect CPU tensors (e.g., NumPy for numerical operations, Matplotlib for plotting).</li> <li>Saving tensor data to disk in a format primarily handled by CPU-based I/O.</li> <li>Performing operations that are only implemented for CPU tensors.</li> </ul> <p>Use the <code>.cpu()</code> method to transfer a tensor to the CPU. If the tensor has gradients and you want to convert it to a NumPy array, you must first <code>.detach()</code> it to remove it from the computation graph.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#key-optimization-techniques-in-pytorch","title":"Key Optimization Techniques in PyTorch","text":"<p>Beyond the basic workflow, several techniques can further optimize your PyTorch code for GPU performance, leading to faster training and better resource utilization.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#adjusting-batch-size","title":"Adjusting Batch Size","text":"<p>Batch size plays a crucial role in GPU utilization and model training dynamics.</p> <ul> <li>GPU Utilization: Larger batch sizes can lead to better parallelism by providing more data for the GPU to process simultaneously. This helps saturate the GPU cores and can improve throughput (the amount of data processed per unit of time).</li> <li>Memory Trade-off: The primary constraint for batch size is GPU memory. Larger batches require more memory to store activations, gradients, and model parameters.</li> <li>Convergence Impact: The relationship between batch size and model convergence is complex. Larger batches might lead to quicker convergence per epoch but can sometimes result in finding sharper minima, which may generalize less well. Smaller batches can introduce noise that acts as a regularizer but might take longer to converge.</li> <li>Recommendation: Experimentation is key. The optimal batch size depends heavily on the specific model architecture, the available GPU memory, and the dataset. Start with a moderate size and increase it until you approach memory limits or observe diminishing returns in speed or undesirable convergence behavior.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#leveraging-mixed-precision-training","title":"Leveraging Mixed Precision Training","text":"<p>Mixed precision training combines lower-precision formats (like FP16 or BF16) with higher-precision FP32 to accelerate training and reduce memory usage.</p> <ul> <li>Benefits: This technique offers significant speedups, especially on NVIDIA GPUs equipped with Tensor Cores, and can halve the memory footprint for parts of the model. For a detailed understanding of floating-point formats like FP16 and BF16, refer to the 'Numerical representations.md' document.</li> <li>PyTorch Implementation (<code>torch.cuda.amp</code>): PyTorch's Automatic Mixed Precision (AMP) module, <code>torch.cuda.amp</code>, simplifies this process.</li> <li><code>autocast</code>: This context manager automatically casts operations within its scope to lower-precision types (FP16 or BF16 where appropriate and safe) to leverage hardware acceleration.</li> <li><code>GradScaler</code>: Helps prevent underflow of gradients (where small gradient values become zero in FP16 due to its limited dynamic range). It scales the loss up before backpropagation, and then unscales the gradients before the optimizer step.</li> </ul> <p>Code Example (PyTorch <code>torch.cuda.amp</code>):</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Enable AMP if CUDA is available\namp_enabled = torch.cuda.is_available()\n\n# Create a gradient scaler for mixed precision\n# The 'enabled' flag allows conditional operation based on CUDA availability\nscaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n\n# Example training loop iteration\n# for data, target in data_loader:\n#     data, target = data.to(device), target.to(device) # Move data to the target device\n#     optimizer.zero_grad() # Clear previous gradients\n#\n#     # Cast operations to mixed precision (FP16/BF16 where appropriate)\n#     # 'autocast' is a context manager that enables mixed precision for the enclosed operations\n#     with torch.cuda.amp.autocast(enabled=amp_enabled):\n#         output = model(data) # Model forward pass in mixed precision\n#         loss = criterion(output, target) # Calculate loss\n#\n#     # Scale loss and call backward() to create scaled gradients\n#     # scaler.scale multiplies the loss by the current scale factor\n#     scaler.scale(loss).backward()\n#\n#     # Unscale gradients (if any were scaled) and call optimizer.step()\n#     # scaler.step also checks for inf/NaN gradients and skips optimizer.step if found\n#     scaler.step(optimizer)\n#\n#     # Update the scale for next iteration\n#     # scaler.update adjusts the scale factor for the next iteration based on gradient statistics\n#     scaler.update()\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#efficient-data-loading","title":"Efficient Data Loading","text":"<p>Data loading can become a bottleneck if the GPU is idle while waiting for data from the CPU. <code>torch.utils.data.DataLoader</code> provides key parameters to optimize this:</p> <ul> <li><code>num_workers</code>: Setting <code>num_workers &gt; 0</code> enables multi-process data loading. This means multiple worker processes load data in parallel, pre-fetching batches so they are ready when the GPU needs them. The optimal value depends on CPU cores and the nature of the data loading task, but a common starting point is the number of CPU cores.</li> <li><code>pin_memory=True</code>: When using GPUs, setting <code>pin_memory=True</code> in the <code>DataLoader</code> tells PyTorch to allocate the loaded data in \"pinned\" (page-locked) CPU memory. This allows for faster asynchronous data transfer from CPU memory to GPU memory, as pinned memory can be accessed directly by the GPU without intermediate copying to a staging area.</li> </ul> <p>Code Example:</p> <pre><code>from torch.utils.data import DataLoader, Dataset\nimport torch # Assuming device is defined, e.g. from previous sections\n\n# Example Dataset (replace with your actual dataset)\n# class MyDataset(Dataset):\n#     def __init__(self):\n#         self.data = torch.randn(1000, 10) # Example data\n#         self.labels = torch.randn(1000, 1) # Example labels\n#     def __len__(self):\n#         return len(self.data)\n#     def __getitem__(self, idx):\n#         return self.data[idx], self.labels[idx]\n\n# dataset = MyDataset() # Instantiate your dataset\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device\n\n# Optimized DataLoader\n# num_workers_val = 4 if device.type == 'cuda' else 0 # Use workers only for GPU\n# pin_memory_val = True if device.type == 'cuda' else False # Pin memory only for GPU\n\n# train_loader = DataLoader(\n#     dataset,\n#     batch_size=64,\n#     shuffle=True,\n#     num_workers=num_workers_val,  # Adjust based on your CPU cores and task\n#     pin_memory=pin_memory_val   # Speeds up CPU to GPU data transfer\n# )\n# print(f\"Using DataLoader with num_workers={train_loader.num_workers}, pin_memory={train_loader.pin_memory if device.type == 'cuda' else 'N/A (CPU)'}\")\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Gradient accumulation is a technique to simulate a larger effective batch size when GPU memory limits the actual batch size that can be processed at once.</p> <ul> <li>Process: Instead of updating model weights after each mini-batch, gradients are accumulated over several mini-batches. The optimizer step (<code>optimizer.step()</code>) is called only after a specified number of accumulation steps.</li> <li>Utility: This is useful when you want the benefits of a larger batch size (e.g., more stable gradients) but cannot fit that large batch into GPU memory.</li> <li>Optimizer Calls: <code>optimizer.zero_grad()</code> should be called at the beginning of each accumulation cycle (i.e., before processing the first mini-batch of an effective larger batch) and after <code>optimizer.step()</code>. The loss computed for each mini-batch should typically be normalized by the number of accumulation steps.</li> </ul> <p>Code Example:</p> <pre><code>import torch\nimport torch.nn as nn\n# # (Assuming model, optimizer, data_loader, device, criterion, and num_epochs are defined)\n# model = YourModel().to(device)\n# optimizer = torch.optim.Adam(model.parameters())\n# data_loader = YourDataLoader(...) \n# criterion = nn.MSELoss()\n# num_epochs = 10\n\naccumulation_steps = 4  # Accumulate gradients over 4 mini-batches to simulate 4x batch size\n\n# for epoch in range(num_epochs):\n#     optimizer.zero_grad() # Zero gradients at the start of each new effective batch/epoch\n#     for i, (inputs, labels) in enumerate(data_loader):\n#         inputs, labels = inputs.to(device), labels.to(device)\n#\n#         # Forward pass\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n#\n#         # Normalize loss to account for accumulation\n#         # This ensures the effective loss magnitude is as if it were a single larger batch\n#         loss = loss / accumulation_steps \n#\n#         # Backward pass (accumulates gradients)\n#         loss.backward()\n#\n#         # Perform optimizer step after 'accumulation_steps' mini-batches\n#         if (i + 1) % accumulation_steps == 0:\n#             optimizer.step()  # Update weights based on accumulated gradients\n#             optimizer.zero_grad()  # Reset gradients for the next accumulation cycle\n#\n#     # Handle the case where the total number of batches isn't a multiple of accumulation_steps\n#     # This ensures any remaining gradients are used for an update.\n#     if len(data_loader) % accumulation_steps != 0:\n#         optimizer.step() # Perform the final optimizer step for the epoch\n#         optimizer.zero_grad() # Clear gradients before the next epoch\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#cudnn-optimizations","title":"CuDNN Optimizations","text":"<p>CuDNN is NVIDIA's library of highly optimized primitives for deep learning operations (like convolutions). PyTorch leverages CuDNN for GPU computations.</p> <ul> <li><code>torch.backends.cudnn.benchmark = True</code>: Setting this to <code>True</code> enables CuDNN's auto-tuner. Before the first execution of a new convolutional layer (or other supported operations) with a specific input size, CuDNN will benchmark several algorithms and select the fastest one for that particular configuration.</li> <li>Use Case: Ideal when input sizes to your model (especially for convolutional layers) remain constant throughout training.</li> <li>Trade-off: There's an upfront cost for benchmarking at the beginning or when input sizes change. If input sizes vary frequently, this might hurt performance.</li> <li><code>torch.backends.cudnn.deterministic = True</code>: For reproducibility, you might want to ensure that CuDNN uses deterministic algorithms. Setting this to <code>True</code> can achieve that.</li> <li>Trade-off: Deterministic algorithms may be less performant than non-deterministic ones chosen by the auto-tuner. This ensures bitwise reproducibility across runs on the same hardware, but potentially at the cost of speed.</li> </ul> <p>Code Example:</p> <pre><code>import torch\n\nif torch.cuda.is_available():\n    # Enable CuDNN auto-tuner to find the best algorithm for the hardware\n    # This can speed up training if input sizes to layers are consistent.\n    torch.backends.cudnn.benchmark = True\n    print(f\"torch.backends.cudnn.benchmark set to {torch.backends.cudnn.benchmark}\")\n\n    # For deterministic results (can impact performance and might not always be achievable)\n    # If you need strict reproducibility, you might also need to set other random seeds (Python, NumPy, PyTorch).\n    # torch.backends.cudnn.deterministic = True\n    # print(f\"torch.backends.cudnn.deterministic set to {torch.backends.cudnn.deterministic}\")\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#profiling-and-debugging-gpu-code","title":"Profiling and Debugging GPU Code","text":"<p>Effective optimization requires understanding where your code spends its time and how it utilizes resources. Profiling and careful debugging are essential steps in this process.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#why-profiling-is-essential","title":"Why Profiling is Essential","text":"<p>Optimization efforts should always be guided by data, not just intuition. Human intuition about performance bottlenecks in complex software, especially involving hardware interactions like GPUs, is often misleading.</p> <ul> <li>Identify Actual Bottlenecks: Profilers help pinpoint the true bottlenecks in your deep learning pipeline. These could be in data loading (<code>DataLoader</code> inefficiencies), specific model operations (e.g., large matrix multiplies, custom layers), CPU-GPU data transfers, or inefficient CUDA kernel implementations.</li> <li>Focus Efforts: By identifying the most time-consuming parts, you can focus your optimization efforts where they will have the most impact.</li> <li>Save Time: Time invested in profiling can save significant development time in the long run by preventing wasted effort on optimizing non-critical code sections.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#accurately-timing-gpu-operations","title":"Accurately Timing GPU Operations","text":"<p>CUDA operations are often asynchronous. When PyTorch code on the CPU calls a GPU operation, the CPU queues the operation and returns control to the Python script almost immediately, before the GPU has necessarily completed the task.</p> <ul> <li><code>torch.cuda.synchronize(device=None)</code>: To get accurate timing for GPU code sections, you must use <code>torch.cuda.synchronize()</code>. This function blocks CPU execution until all previously queued kernels on the specified GPU (or the current GPU if <code>device</code> is <code>None</code>) have finished.</li> </ul> <p>Code Example:</p> <pre><code>import torch\nimport time\n\n# Assuming 'device' is a CUDA device # e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# And 'model' is a PyTorch model # e.g., model = YourModel().to(device)\n# And 'input_tensor' is on the same device # e.g., input_tensor = torch.randn(128, 3, 224, 224, device=device)\n\n# if device.type == 'cuda':\n#     # Incorrect timing (measures CPU dispatch time, not actual GPU execution)\n#     start_time_naive = time.time()\n#     # output = model(input_tensor) # Example operation\n#     end_time_naive = time.time()\n#     # print(f\"Naive timing: {end_time_naive - start_time_naive:.6f} seconds (may be inaccurate for GPU)\")\n\n#     # Correct timing for GPU operations\n#     torch.cuda.synchronize() # Wait for all preceding GPU work to finish\n#     start_time_sync = time.time()\n#\n#     # output_sync = model(input_tensor) # The operation to time\n#\n#     torch.cuda.synchronize() # Wait for 'model(input_tensor)' (the operation being timed) to finish\n#     end_time_sync = time.time()\n#     # print(f\"Accurate timing with synchronize(): {end_time_sync - start_time_sync:.6f} seconds\")\n# else:\n#     # print(\"CUDA not available, skipping GPU timing example.\")\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#monitoring-gpu-memory-usage","title":"Monitoring GPU Memory Usage","text":"<p>Understanding and monitoring GPU memory usage is critical for preventing out-of-memory (OOM) errors and for optimizing batch sizes to maximize GPU utilization without exceeding memory capacity.</p> <ul> <li>PyTorch Functions: PyTorch provides several functions to inspect memory usage on CUDA devices:</li> <li><code>torch.cuda.memory_allocated(device=None)</code>: Returns the current GPU memory occupied by tensors in bytes for the given (or current) device. This reflects memory used by your active tensors.</li> <li><code>torch.cuda.max_memory_allocated(device=None)</code>: Returns the peak GPU memory occupied by tensors since the beginning of the program or the last call to <code>reset_peak_memory_stats</code>.</li> <li><code>torch.cuda.memory_reserved(device=None)</code>: Returns the total GPU memory currently managed by PyTorch's caching memory allocator. This includes memory allocated for tensors plus any reserved but currently unused cached blocks.</li> <li><code>torch.cuda.max_memory_reserved(device=None)</code>: Returns the peak GPU memory managed by the caching allocator since the program start or last reset.</li> <li><code>torch.cuda.empty_cache()</code>: Releases all unused cached memory blocks from PyTorch's caching allocator back to the OS. This does not free memory occupied by active tensors. It can be useful if memory fragmentation is suspected, but frequent use can slow down subsequent allocations as PyTorch might have to re-request memory from the OS.</li> <li><code>nvidia-smi</code> Command-Line Tool: The NVIDIA System Management Interface (<code>nvidia-smi</code>) is an external command-line utility that provides real-time monitoring of NVIDIA GPU devices. It displays GPU utilization, memory usage, temperature, power consumption, and currently running processes on each GPU. It's excellent for a quick overview of GPU health and identifying which processes are consuming GPU resources.</li> </ul> <p>Code Example (PyTorch functions):</p> <pre><code>import torch\n\n# Assuming 'device' is a CUDA device # e.g., device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# if device.type == 'cuda':\n#     # Initial memory snapshot\n#     initial_allocated = torch.cuda.memory_allocated(device)\n#     initial_reserved = torch.cuda.memory_reserved(device)\n#     print(f\"Initial memory allocated: {initial_allocated / 1024**2:.2f} MB\")\n#     print(f\"Initial memory reserved by cache: {initial_reserved / 1024**2:.2f} MB\")\n#\n#     # Example: Create a large tensor\n#     # x = torch.randn(10000, 10000, device=device) # Approx 381 MB for FP32\n#     # print(f\"Memory allocated after creating tensor x: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n#     # print(f\"Memory reserved by cache after x: {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n#     # print(f\"Max memory allocated so far: {torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB\")\n#     # print(f\"Max memory reserved by cache so far: {torch.cuda.max_memory_reserved(device) / 1024**2:.2f} MB\")\n#\n#     # del x # Delete the tensor\n#     # print(f\"Memory allocated after 'del x': {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB (tensor gone, but cache might hold it)\")\n#\n#     # torch.cuda.empty_cache() # Release unused cached memory\n#     # print(f\"Memory allocated after empty_cache(): {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB\")\n#     # print(f\"Memory reserved by cache after empty_cache(): {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB\")\n# else:\n#     # print(\"CUDA not available, skipping GPU memory usage example.\")\n</code></pre> <p><code>nvidia-smi</code> Command-Line Example (run in your terminal):</p> <pre><code># To view current GPU status including memory usage, utilization, etc.\nnvidia-smi\n\n# To continuously monitor GPU status (updates every 1 second)\nwatch -n 1 nvidia-smi\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#multi-gpu-training-strategies","title":"Multi-GPU Training Strategies","text":"<p>When a single GPU is insufficient for training speed or model/batch size, PyTorch offers built-in ways to distribute training across multiple GPUs. This can significantly accelerate the training process or enable the use of larger models and batches that wouldn't fit on a single device.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#torchnndataparallel","title":"<code>torch.nn.DataParallel</code>","text":"<p><code>torch.nn.DataParallel</code> (DP) is a simpler way to achieve multi-GPU training within a single process using Python threads.</p> <ul> <li>How it Works:<ol> <li>The input batch is split along the batch dimension and distributed to the available GPUs.</li> <li>The model is replicated on each GPU.</li> <li>A forward pass is performed on each GPU with its slice of data.</li> <li>Outputs are gathered on a primary GPU (usually <code>cuda:0</code>), where the loss is computed.</li> <li>Gradients are then computed on the primary GPU and scattered back to each model replica for the backward pass and parameter updates.</li> </ol> </li> <li>Pros:</li> <li>Easy to implement, often requiring just wrapping the model.</li> <li>Cons:</li> <li>Imbalanced GPU Utilization: The primary GPU (where outputs are gathered and loss is computed) often bears a higher load.</li> <li>GIL Issues: Python's Global Interpreter Lock (GIL) can be a bottleneck due to its reliance on threading.</li> <li>Model Replication Overhead: The model is copied to each GPU in each forward pass, which can be inefficient.</li> <li>Generally Not Recommended: Generally, <code>DistributedDataParallel</code> is preferred for better performance.</li> </ul> <p>Code Example (Conceptual - how to wrap a model):</p> <pre><code>import torch\nimport torch.nn as nn\n\n# class MyModel(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.fc = nn.Linear(10,1)\n#     def forward(self, x):\n#         return self.fc(x)\n\n# model = MyModel() # Your model definition\n# # Check if multiple GPUs are available\n# if torch.cuda.is_available() and torch.cuda.device_count() &gt; 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n#     model = nn.DataParallel(model) # Wrap the model\n\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Ensure the model wrapper is on the primary device\n# model.to(device) # Move the DataParallel wrapper to the primary GPU (e.g., cuda:0)\n# # The DataParallel model itself resides on 'device' (e.g. cuda:0), which acts as the primary device for output gathering.\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#torchnnparalleldistributeddataparallel-ddp","title":"<code>torch.nn.parallel.DistributedDataParallel</code> (DDP)","text":"<p><code>torch.nn.parallel.DistributedDataParallel</code> (DDP) is the generally recommended approach for multi-GPU training, offering better performance and scalability, including multi-node (multiple machines) training.</p> <ul> <li>How it Works: DDP uses multi-processing, where one independent process is typically created for each GPU.<ol> <li>The model is replicated once on each GPU at the beginning of training.</li> <li>Each process handles a portion of the input data.</li> <li>During the backward pass, gradients are computed locally and then efficiently averaged across all processes using optimized collective communication operations (often via NCCL).</li> <li>Each process then updates its local copy of the model weights independently.</li> </ol> </li> <li>Pros:</li> <li>Faster Performance: Typically provides significant speedups over <code>DataParallel</code> due to more efficient gradient communication and no GIL bottleneck.</li> <li>Efficient GPU Utilization: Workload is generally more balanced across GPUs.</li> <li>Overcomes GIL: By using separate processes, it avoids GIL limitations.</li> <li>Standard for Distributed Training: The preferred method for most serious multi-GPU and distributed training scenarios.</li> <li>Cons:</li> <li> <p>More Setup: Requires more boilerplate code to initialize process groups, set up communication backends, and launch multiple processes.</p> </li> <li> <p>Note on Usage:     Setting up DDP is more involved than <code>DataParallel</code>. It typically requires:</p> <ol> <li>Initializing a process group (e.g., using <code>torch.distributed.init_process_group</code>).</li> <li>Ensuring each process works on its designated GPU.</li> <li>Wrapping the model with <code>DistributedDataParallel</code>.</li> <li>Using a distributed sampler for the <code>DataLoader</code> to ensure each process gets a unique part of the data.</li> <li>Launching the script using a utility like <code>torchrun</code> (recommended) or <code>torch.multiprocessing.spawn</code>. Due to its complexity and context-dependent setup (especially for different environments like single-node multi-GPU vs. multi-node), a full code example is not provided here. Please refer to the official PyTorch documentation on Distributed Training and DDP examples for detailed implementation guides.</li> </ol> </li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#when-to-choose","title":"When to Choose","text":"<ul> <li><code>torch.nn.DataParallel</code>: Consider for quick experiments or simple scenarios where ease of implementation is paramount and peak performance or scalability is not critical.</li> <li><code>torch.nn.parallel.DistributedDataParallel</code> (DDP): Recommended for most other cases, especially for serious training, achieving better performance, and scaling to multiple GPUs or even multiple machines. The initial setup effort is often outweighed by the performance gains.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#further-considerations-for-gpu-optimization","title":"Further Considerations for GPU Optimization","text":"<p>Beyond the specific techniques discussed, several other factors and practices can influence GPU performance.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#impact-of-model-architecture","title":"Impact of Model Architecture","text":"<p>The design of the neural network itself can significantly impact GPU performance.</p> <ul> <li>Some operations are inherently more GPU-friendly than others. Large matrix multiplications and standard convolutions, which form the backbone of many deep learning models, are well-suited for GPU parallel processing.</li> <li>Operations that are highly sequential, involve custom CUDA kernels that are not fully optimized, or require frequent CPU-GPU interaction (e.g., control flow decisions based on tensor values that need to be brought to CPU) can become bottlenecks.</li> <li>When designing or choosing models, especially for resource-constrained environments or performance-critical applications, consider GPU efficiency. For example, architectures using depthwise separable convolutions (common in mobile-efficient models) might offer a better trade-off between performance and accuracy compared to standard convolutions in some scenarios. Being aware of operations that are computationally expensive or less parallelizable can guide model selection.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#minimizing-cpu-gpu-synchronization","title":"Minimizing CPU-GPU Synchronization","text":"<p>As mentioned earlier, CUDA operations are asynchronous. The CPU queues operations on the GPU and then continues its own work.</p> <ul> <li>Explicit synchronization points, such as <code>torch.cuda.synchronize()</code>, force the CPU to wait until all previously queued GPU tasks on a specific device are complete. While essential for accurate timing (as discussed in profiling) or when data is immediately needed on the CPU (e.g., <code>.item()</code> or <code>.cpu()</code> on a tensor needed for a control flow decision), overuse in performance-critical loops can stall the GPU pipeline and negate the benefits of asynchronous execution.</li> <li>Minimize explicit synchronizations. Let the GPU work asynchronously as much as possible.</li> <li>Be aware that some PyTorch operations might implicitly synchronize. Profiling tools like NVIDIA Nsight Systems can help identify such synchronization points and their impact.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#leveraging-optimized-operations-and-libraries","title":"Leveraging Optimized Operations and Libraries","text":"<p>PyTorch's strength lies in its extensive library of built-in operations that are highly optimized for both CPU and GPU execution.</p> <ul> <li>Prefer PyTorch's built-in layers and functions (e.g., <code>torch.nn.Conv2d</code>, <code>torch.matmul</code>, optimized activation functions) whenever possible, as these often have underlying implementations that call highly optimized libraries like CuDNN for NVIDIA GPUs.</li> <li>Avoid re-implementing complex operations manually in Python if optimized versions are available. Custom Python loops over tensor elements, for example, will be significantly slower than equivalent vectorized PyTorch operations.</li> <li>For specific tasks or experimental features, external libraries might offer further specialized, performance-tuned implementations. For instance, libraries like NVIDIA's Apex have historically provided cutting-edge features, and higher-level frameworks like <code>fastai</code> often incorporate performance best practices by default. The core idea is to leverage existing, well-tested, and optimized code rather than writing from scratch where performance is critical.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/#input-data-properties","title":"Input Data Properties","text":"<p>The characteristics of your input data can also affect performance and memory usage.</p> <ul> <li>Input Size: Larger input dimensions (e.g., high-resolution images, long sequences) directly translate to increased memory consumption for activations and intermediate tensors, and more computational work. This might necessitate smaller batch sizes to fit within GPU memory.</li> <li>Variable Input Sizes: If inputs within a batch have varying sizes (e.g., sentences of different lengths in NLP), they often need to be padded to a common size to be processed in a batch. Excessive padding can lead to wasted computation on padding tokens. While techniques like bucketing (grouping inputs of similar sizes into batches) or using packed sequences (for RNNs) can mitigate this, they add complexity. Where feasible, batching inputs of similar sizes or using models/techniques robust to variable sizes can be beneficial.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/","title":"Numerical representations","text":"<p>Numerical representation plays a crucial role in ML and DL. Even with bigger model, we're always trying to us the most optimized consumption and this goes through how we represent our vectors. A big point of optimisation is reducing the size but also the precision of the vectors (sometimes for different purpose like training or inference).</p> <p>You've got a brief overview of numerical representations</p> <p>A floating-point number, crucial for representing real numbers in computers, is typically composed of three parts: the sign bit, the exponent, and the mantissa. Understanding these components is key because different applications and hardware platforms utilize various floating-point formats (like FP32, FP16, BF16, etc.), each with distinct characteristics regarding range, precision, and computational cost. This knowledge is vital for optimizing performance and ensuring numerical stability, especially in fields like deep learning and scientific computing.</p> <p>These three components work together to represent a wide range of real numbers by expressing a number in scientific notation (mantissa * base^exponent), with the sign bit indicating its polarity.</p> <p>Loading a model using the format it was intended for is very important as not doing it results in loss of quality</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#fp64-double-precision-floating-point","title":"FP64 (Double-Precision Floating-Point)","text":"<ul> <li>Full Name: Double-Precision Floating-Point</li> <li>Total Bits: 64</li> <li>Sign Bits: 1</li> <li>Exponent Bits: 11</li> <li>Mantissa Bits: 52</li> <li>Characteristics:</li> <li>Range: Widest. Can represent a vast range of numbers, from very small to very large.</li> <li>Precision: Highest. Offers a high degree of accuracy for calculations.</li> <li>Trade-offs: Consumes more memory and bandwidth compared to lower precision formats. Computations can be slower on hardware not optimized for FP64.</li> <li>Common Use Cases/Hardware: Scientific computing, numerical analysis, financial modeling, and other applications requiring high accuracy. It's the default floating-point type in NumPy and often used in CPU-based computations.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#fp32-single-precision-floating-point","title":"FP32 (Single-Precision Floating-Point)","text":"<ul> <li>Full Name: Single-Precision Floating-Point</li> <li>Total Bits: 32</li> <li>Sign Bits: 1</li> <li>Exponent Bits: 8</li> <li>Mantissa Bits: 23</li> <li>Characteristics:</li> <li>Range: Wide. Offers a good balance for general-purpose computations.</li> <li>Precision: High. Sufficient for many applications, including graphics and standard deep learning model parameters.</li> <li>Trade-offs: Standard performance and memory usage.</li> <li>Common Use Cases/Hardware: Widely used as a standard for many applications, including traditional computer graphics, physics simulations, and as a common format for storing weights and activations in deep learning models. General-purpose GPU computing.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#fp16-half-precision-floating-point","title":"FP16 (Half-Precision Floating-Point)","text":"<ul> <li>Full Name: Half-Precision Floating-Point</li> <li>Total Bits: 16</li> <li>Sign Bits: 1</li> <li>Exponent Bits: 5</li> <li>Mantissa Bits: 10</li> <li>Characteristics:</li> <li>Range: Limited. More susceptible to overflow (number too large) and underflow (number too small) compared to FP32.</li> <li>Precision: Limited. Can lead to loss of information for very fine details.</li> <li>Trade-offs: Significantly reduces memory footprint and bandwidth requirements (half of FP32). Can offer substantial speedups on compatible hardware (e.g., NVIDIA Tensor Cores, mobile GPUs) due to higher throughput. Requires careful handling, often using techniques like loss scaling in deep learning to prevent numerical instability.</li> <li>Common Use Cases/Hardware: Deep learning training and inference, especially on modern GPUs (like NVIDIA with Tensor Cores) and mobile devices where memory and power efficiency are critical.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#bf16-bfloat16-floating-point","title":"BF16 (BFloat16 Floating-Point)","text":"<ul> <li>Full Name: Brain Floating-Point Format (BFloat16)</li> <li>Total Bits: 16</li> <li>Sign Bits: 1</li> <li>Exponent Bits: 8 (same as FP32)</li> <li>Mantissa Bits: 7</li> <li>Characteristics:</li> <li>Range: Wide, comparable to FP32. This makes it more resilient to overflow and underflow issues than FP16, especially during deep learning training.</li> <li>Precision: Low. With only 7 mantissa bits, the precision is significantly reduced compared to FP16 and FP32. This can affect tasks requiring fine-grained details but is often acceptable for deep learning models.</li> <li>Trade-offs: Offers memory savings similar to FP16 and can provide performance benefits. Its FP32-like range simplifies conversion from FP32 models.</li> <li>Common Use Cases/Hardware: Primarily used for deep learning training and inference, especially on Google TPUs (Tensor Processing Units) and increasingly supported by newer CPUs and GPUs from NVIDIA and Intel.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#tf32-tensorfloat-32","title":"TF32 (TensorFloat-32)","text":"<ul> <li>Full Name: TensorFloat-32</li> <li>Total Bits: TF32 is not a storage format in the same way as FP32 or FP16. For specific operations like matrix multiplications and convolutions on compatible hardware, inputs (which are typically FP32) are internally processed using a 19-bit representation.</li> <li>Sign Bits: 1 (for internal computation)</li> <li>Exponent Bits: 8 (for internal computation, same as FP32)</li> <li>Mantissa Bits: 10 (for internal computation, same as FP16)</li> <li>Characteristics:</li> <li>Range: Wide, same as FP32.</li> <li>Precision: Limited, effectively the precision of FP16 for the operations it accelerates.</li> <li>Trade-offs: Aims to provide a significant speedup over FP32 for deep learning matrix operations with minimal to no code changes and often without a noticeable loss in accuracy for many workloads. Input and output data for TF32-accelerated operations remain in FP32.</li> <li>Common Use Cases/Hardware: Accelerating deep learning training and inference computations (specifically matrix math) on NVIDIA Ampere architecture GPUs (e.g., A100) and newer. It's often enabled by default in deep learning libraries like PyTorch and TensorFlow for compatible operations on supported hardware.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#summary-of-floating-point-formats","title":"Summary of Floating-Point Formats","text":"Feature FP64 FP32 FP16 BF16 TF32 (for matmul) Total Bits 64 32 16 16 N/A (19-bit compute path) Sign Bits 1 1 1 1 1 Exponent Bits 11 8 5 8 8 Mantissa Bits 52 23 10 7 10 Range Widest Wide Limited Wide Wide Precision Highest High Limited Low Limited Primary Use SciComp General DL Speed DL TPU DL Nvidia Speedup"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Mixed precision training is a technique used to accelerate the training of deep learning models by using a combination of lower-precision floating-point formats (like FP16 or BF16) and higher-precision formats (like FP32).</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#advantages","title":"Advantages","text":"<p>Employing mixed precision training offers several key benefits:</p> <ul> <li>Speed: Computations, particularly matrix multiplications and convolutions, can be significantly faster. This is due to specialized hardware support (e.g., Tensor Cores in NVIDIA GPUs) for lower-precision types, which can execute these operations at a much higher throughput than FP32. Peak FP16/BF16 performance can be several times higher than FP32 on modern accelerators. Additionally, reduced data movement (less data to read and write) contributes to speedups.</li> <li>Memory Reduction: Using lower-precision types like FP16 or BF16 halves the memory footprint for those values compared to FP32. This allows for training larger models (more parameters), using larger batch sizes (which can improve training stability and speed), or processing larger input sizes (e.g., higher resolution images).</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#how-it-generally-works-conceptual","title":"How it Generally Works (Conceptual)","text":"<p>Mixed precision training isn't simply about casting all model parameters and computations to a lower-precision format, as this can lead to numerical instability and accuracy loss. Instead, it involves a more nuanced approach:</p> <ul> <li>Master Weights in FP32: A master copy of the model weights is typically kept in FP32. This ensures that weight updates, which are often small, are accumulated with high precision, preventing loss of information.</li> <li>FP16/BF16 Computations: During the forward and backward passes, weights and activations are cast to FP16 or BF16 for computation. This is where the speed and memory benefits are realized.</li> <li>Loss Scaling (Primarily for FP16): FP16 has a more limited dynamic range compared to FP32. This means that very small gradient values (common in deep learning) might become zero in FP16 (a phenomenon called underflow), hindering learning. To counteract this, loss scaling is employed. The loss value is multiplied by a scaling factor before backpropagation, which scales up the gradients. Before the weight update, the gradients are scaled back down to their original magnitude. This helps preserve small gradient values that would otherwise be lost. BF16, having a similar dynamic range to FP32, typically does not require loss scaling.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#practical-implementation-example","title":"Practical Implementation Example","text":"<p>Modern deep learning libraries have significantly simplified the implementation of mixed precision training through Automatic Mixed Precision (AMP) utilities.</p> <ul> <li>Frameworks like PyTorch (with <code>torch.cuda.amp</code>) and TensorFlow (with <code>tf.keras.mixed_precision</code>) provide AMP features. These tools automatically manage the casting of operations to appropriate data types (FP32, FP16, or BF16 based on the operation and hardware) and handle loss scaling where necessary (primarily for FP16). This allows developers to enable mixed precision with minimal code changes, often just a few lines.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Numerical%20representations/#further-reading","title":"Further Reading","text":"<p>For a detailed guide on mixed precision in PyTorch, see What Every User Should Know About Mixed Precision Training in PyTorch.</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Pytorch%20GPU%20Setup/","title":"Pytorch GPU Setup","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Pytorch%20GPU%20Setup/#description-useful-tips-for-optimizing-pytorch-runs","title":"description: Useful tips for optimizing Pytorch runs","text":""},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Pytorch%20GPU%20Setup/#when-to-move-from-cpu-to-gpu","title":"When to move from CPU to GPU","text":"<p>With the rise of Large Models (Language, Vision, Speech...) it becomes absolutely necessary to swtich from CPU to GPU. While some \"small\" embedding models (say 1 billion parameters) can run on CPU, some models require GPU even without training and fine-tuning. We will see later some techniques to optimize (i.e. reduce) the use of GPU.</p> <p>However, in order to run a lot of components in this repository, it is better to setup GPU usage as early as possible. It will be just faster to run some code :).</p>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Pytorch%20GPU%20Setup/#how-to-test-your-current-setup","title":"How to test your current setup","text":"<p>For nvidia installation and pytorch setup you will need to install</p> <pre><code>$pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>To test your current setup</p> <pre><code>import torch\nimport os\n\ndef display_cuda_info():\n    \"\"\"Display comprehensive CUDA and GPU information\"\"\"\n    print(f\"CUDA is available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"CUDA version: {torch.version.cuda}\")\n        print(f\"Current GPU ID: {torch.cuda.current_device()}\")\n        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\n        # Display information for each GPU\n        for i in range(torch.cuda.device_count()):\n            print(f\"\\nGPU {i} info:\")\n            print(f\"Name: {torch.cuda.get_device_name(i)}\")\n            print(f\"Memory allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n            print(f\"Memory cached: {torch.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n\ndef switch_gpu(gpu_id):\n    \"\"\"Switch to specified GPU\"\"\"\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is not available\")\n\n    if gpu_id &gt;= torch.cuda.device_count():\n        raise ValueError(f\"GPU {gpu_id} not found. Available GPUs: 0-{torch.cuda.device_count()-1}\")\n\n    # Set environment variable\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n    # Set PyTorch default device\n    torch.cuda.set_device(gpu_id)\n\n    print(f\"Switched to GPU {gpu_id}: {torch.cuda.get_device_name(gpu_id)}\")\n\ndef move_model_to_gpu(model, gpu_id):\n    \"\"\"Move a PyTorch model to specified GPU\"\"\"\n    if not torch.cuda.is_available():\n        raise RuntimeError(\"CUDA is not available\")\n\n    switch_gpu(gpu_id)\n    return model.cuda()\n</code></pre> <p>You can test with a small pytorch model:</p> <pre><code># Test with a dumb model\nmodel = torch.nn.Linear(10, 10)  # Simple example model\ntry:\n    model = move_model_to_gpu(model, 0)  # Move to GPU 0\n    print(\"Model successfully moved to GPU\")\nexcept Exception as e:\n    print(f\"Error moving model to GPU: {e}\")\n</code></pre> <p>Alternatively, you can display nvidia information using the command:</p> <pre><code>!nvidia-smi\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%201%20-%20Deep%20Learning/Pytorch%20GPU%20Setup/#get-your-hands-dirty","title":"Get your hands dirty","text":"<p>https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/","title":"What is encoding ?","text":"<p>The encoder part in the original transformer is responsible for understanding and extracting the relevant information from the input text. It then outputs a continuous representation (embedding) of the input text that is passed to the decoder.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#what-is-decoding","title":"What is decoding ?","text":"<p>To illustrate and understand decoding, you can follow this nice post from Maxime Labonne. It explores how does a model like GPT2 \"produces\" the text.</p> <p>Actually the tokenizer translates each token from the input text into a corresponding token ID. Then the LLMs calculate logits which are scores assigned to every possible token in the vocabulary. They are then converted into probabilities using a softmax function. Eventually, how these probabilities are used to select the next token is part of the decoding strategy.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#encoder-only-models","title":"Encoder only models","text":"<p>BERT (Bidirectional Encoder Representations from Transformers) is an encoder-only architecture based on the Transformer's encoder module. The BERT model is pretrained on a large text corpus using masked language modeling (illustrated in the figure below) and next-sentence prediction tasks. The main idea behind masked language modeling is to mask (or replace) random word tokens in the input sequence and then train the model to predict the original masked tokens based on the surrounding context.</p> <p>Bert are bidirectional which means they can learn representations of the text by attending to words on both sides.</p> <p>These models are good for tasks that require understanding the input like sentence classification or named entity recognition.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#decoder-only-models","title":"Decoder only models","text":"<p>Over the years, researchers have built upon the original encoder-decoder transformer architecture and developed several decoder-only models that have proven to be highly effective in various natural language processing tasks. The most notable models include the GPT family. The GPT (Generative Pre-trained Transformer) series are decoder-only models pre-trained on large-scale unsupervised text data and finetuned for specific tasks such as text classification, sentiment analysis, question-answering, and summarization. Compared to Bert, they can only use context from the previous tokens to predict the next token.</p> <p>They are good at generative tasks like generate text or image.</p> <p>For instance, GPT-2 uses byte pair encoding (BPE) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a masked self-attention layer which means GPT-2 can\u2019t attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT\u2019s [mask] token because, in masked self-attention, an attention mask is used to set the score to 0 for future tokens.</p> <p>The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#encoder-decoder-models-or-sequence-to-sequence","title":"Encoder-decoder models or sequence-to-sequence","text":"<p>They are good at generative tasks that require an input such as translation or summarization Next to the traditional encoder and decoder architectures, there have been advancements in the development of new encoder-decoder models that leverage the strengths of both components. These models often incorporate novel techniques, pre-training objectives, or architectural modifications to enhance their performance in various natural language processing tasks Recently, these models have lost popularity towards decoder only models.</p> <p>https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#cross-encoder-bi-encoder","title":"Cross encoder / bi-encoder","text":"<p>https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/ https://weaviate.io/blog/cross-encoders-as-reranker</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#speech-and-audio-data","title":"Speech and Audio data","text":"<p>Whisper is an encoder-decoder (sequence-to-sequence) transformer pretrained on 680,000 hours of labeled audio data. This amount of pretraining data enables zero-shot performance on audio tasks in English and many other languages. The decoder allows Whisper to map the encoders learned speech representations to useful outputs, such as text, without additional fine-tuning. Whisper just works out of the box.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#image-classification","title":"Image classification","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/#httpshuggingfacecoblogmlabonnedecoding-strategies","title":"https://huggingface.co/blog/mlabonne/decoding-strategies","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/","title":"LLM Architecture","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/#non-determinism-in-llms","title":"Non determinism in LLMs","text":"<p>If you're intestested, you can go through the entire articile of Thinking Machines. You can use their code to make your LLM deterministic: https://github.com/thinking-machines-lab/batch_invariant_ops</p> <p>Here's the TLDR: You can bring down the temperature to 0, the non-determinism can still exist because of kernels behavior.</p> <p>\u2022 The Problem: LLM inference produces different outputs for identical inputs, even at temperature 0, making reproducible results nearly impossible. \u2022 Common Misconception: The widely believed \"concurrency + floating point\" hypothesis suggests GPU parallelism causes nondeterminism, but this misses the real issue. \u2022 Root Cause: Floating-point non-associativity means (a+b)+c \u2260 a+(b+c), causing different results when numbers are added in different orders. \u2022 Real Culprit (Simple Example): Imagine you ask ChatGPT \"What's 2+2?\" When the server is busy with 100 other users, your request gets processed in a batch of 101. When it's quiet, your request gets processed alone in a batch of 1. Even though it's the same question, the math operations inside the model happen differently because of the different batch sizes - like doing homework alone vs. in a crowded classroom where the teacher has to handle everyone differently. \u2022 Forward Pass Reality: Individual LLM forward passes are actually deterministic, but the system becomes nondeterministic due to varying batch contexts. \u2022 Solution Strategy: Use fixed reduction strategies regardless of batch size, avoiding split-reduction techniques that change based on parallelism needs. \u2022 Performance Trade-off: Batch-invariant kernels sacrifice some performance (about 20% slower) but maintain mathematical correctness. \u2022 Real Impact: Tests showed 80 unique completions from 1000 identical requests, but batch-invariant kernels produced identical results every time. \u2022 Broader Benefits: True determinism enables genuine on-policy reinforcement learning and eliminates the need for off-policy corrections in training.</p> <p>Most Modern LLMs use decoder-only architecture.</p> <p>https://github.com/rasbt/LLMs-from-scratch</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/#positional-encoding","title":"Positional encoding","text":"<p>https://github.com/harrisonpim/positional-embeddings?tab=readme-ov-file</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/#mixture-of-experts","title":"Mixture of Experts","text":"<p>https://huggingface.co/blog/moe#fine-tuning-moes https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts</p> <p>New release of expert parallelism using transformers directly: https://huggingface.co/blog/faster-transformers https://www.linkedin.com/posts/akshay-pachaar_youre-in-an-ml-engineer-interview-at-mistralai-activity-7383487927080857600-v5yN?utm_source=share&amp;utm_medium=member_android&amp;rcm=ACoAAAVV2dEBAuuJCv1jGmfAXdBgR9YAUI0StlM</p> <p>https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/#build-from-scratch","title":"Build from scratch","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/#-qwen3-from-scratch-httpsgithubcomrasbtllms-from-scratchtreemainch0511_qwen3","title":"- Qwen3 from scratch: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/","title":"Overview of embedding model","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#description-definition-and-overview-of-embedding-models","title":"description: Definition and overview of embedding models","text":"<p>Add check overview: https://huggingface.co/spaces/hesamation/primer-llm-embedding?section=what_makes_a_good_embedding?</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#what-are-embeddings","title":"What are embeddings ?","text":"<p>Embeddings are at the code of Language, Vision and Speech models. Simply put, it is the process of encoding an object (text, image, audio ...) into a vector representations. Once represented as a vector, we can calculate metrics that renders the quality of such an embedding model compared to new objects of the same family. Vectors are usually high-dimensional and we want to transform them into our lower-dimensional representation, which we call embeddings or also latent space. This latent space captures the important features and similarities in a simpler form. For example, an embedding model might take a 2000-word document and represent it in 300-dimensional space, meaning simply 300 numbers in a list, which is much smaller than the original space but still retains the meaningful relationships between words.</p> <p>Embedding models understand the relationships between words, sentences or objects. The objective is to define, measure and keep a meaningful relationship between these objects.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#types-of-embeddings","title":"Types of embeddings","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#sparse-embeddings","title":"\ud835\udde6\ud835\uddfd\ud835\uddee\ud835\uddff\ud835\ude00\ud835\uddf2 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00","text":"<p>Think keyword-based representations where most values are zero. Great for exact matching but limited for semantic understanding.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#dense-embeddings","title":"\ud835\uddd7\ud835\uddf2\ud835\uddfb\ud835\ude00\ud835\uddf2 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00","text":"<p>The most common type - every dimension has a value. These capture semantic meaning really well, and come in many different lengths.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#quantized-embeddings","title":"\ud835\udde4\ud835\ude02\ud835\uddee\ud835\uddfb\ud835\ude01\ud835\uddf6\ud835\ude07\ud835\uddf2\ud835\uddf1 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00","text":"<p>Compressed versions of dense embeddings that reduce memory usage by using fewer bits per dimension. Perfect when you need to save storage space.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#binary-embeddings","title":"\ud835\uddd5\ud835\uddf6\ud835\uddfb\ud835\uddee\ud835\uddff\ud835\ude06 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00","text":"<p>Ultra-compressed embeddings using only 0s and 1s. Super fast for similarity calculations but with reduced accuracy.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#variable-dimensions-matryoshka","title":"V\ud835\uddee\ud835\uddff\ud835\uddf6\ud835\uddee\ud835\uddef\ud835\uddf9\ud835\uddf2 \ud835\uddd7\ud835\uddf6\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb\ud835\ude00 (\ud835\udde0\ud835\uddee\ud835\ude01\ud835\uddff\ud835\ude06\ud835\uddfc\ud835\ude00\ud835\uddf5\ud835\uddf8\ud835\uddee)","text":"<p>These embeddings let you use just the first 8, 16, 32, etc. dimensions while still retaining most of the information. This ability comes during model training: earlier dimensions capture more information than later ones. You can truncate a 3072-dimension vector to 512 dimensions and still get great performance.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#multi-vector-colbert","title":"\ud835\udde0\ud835\ude02\ud835\uddf9\ud835\ude01\ud835\uddf6-\ud835\udde9\ud835\uddf2\ud835\uddf0\ud835\ude01\ud835\uddfc\ud835\uddff (\ud835\uddd6\ud835\uddfc\ud835\uddf9\ud835\uddd5\ud835\uddd8\ud835\udde5\ud835\udde7)","text":"<p>Instead of one vector per object, you get many vectors that represent different parts of your object (like tokens for text, patches for images). This enables \"late interaction\" - comparing individual parts of texts rather than whole documents. Way more nuanced than single-vector approaches.</p> <p>\ud835\udde6\ud835\uddfc \ud835\ude04\ud835\uddf5\ud835\uddf6\ud835\uddf0\ud835\uddf5 \ud835\ude00\ud835\uddf5\ud835\uddfc\ud835\ude02\ud835\uddf9\ud835\uddf1 \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddf0\ud835\uddf5\ud835\uddfc\ud835\uddfc\ud835\ude00\ud835\uddf2?</p> <ul> <li>Dense for general semantic search.</li> <li>Matryoshka when you need flexible performance/cost trade-offs.</li> <li>Multi-vector for precise text matching.</li> <li>Quantized/Binary when storage and speed matter most.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#what-are-embeddings-using-transformer-techniques","title":"What are embeddings using transformer techniques","text":"<p>This cohere's blog explains why embedding is useful and given the complexity of human language how sentence embedding actually works in simple terms.</p> <p>A very famous library (from Hugging Face) known to train embedding model is \"Sentence Transformer\" (https://sbert.net/) Here's an example with a few sentences encoded using the \"all-MiniLM\" model.</p> <pre><code>from sentence_transformers import SentenceTransformer\n\n# 1. Load a pretrained Sentence Transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# The sentences to encode\nsentences = [\n    \"The weather is lovely today.\",\n    \"It's so sunny outside!\",\n    \"He drove to the stadium.\",\n]\n\n# 2. Calculate embeddings by calling model.encode()\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 384]\n\n# 3. Calculate the embedding similarities\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n# tensor([[1.0000, 0.6660, 0.1046],\n#         [0.6660, 1.0000, 0.1411],\n#         [0.1046, 0.1411, 1.0000]])\n</code></pre> <p>The embedding model will always produce embeddings of the same fixed size. You can then compute the similarity of complex objects by computing the similarity of the respective embeddings. The more dimension, the more \"information\" we can store in the vector. However, the more computationaly intensive it will be. So research is aiming at having the best trade-off between size and performance. Here's an example of embedding model with a dimension reduction while keeping performance relatively equivalent: (https://huggingface.co/blog/matryoshka)</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#embedding-model-evaluation-and-metrics","title":"Embedding Model evaluation and metrics","text":"<p>When running embeddings, the performance is often evaluated using the cosine similarity.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#fine-tuning-an-embedding-model","title":"Fine tuning an embedding model","text":"<p>The main reasons you want to fine tune an embedding model are:</p> <ul> <li>to bring context and vocabulary to your embedding model. Given an encoding strategy, you may or may not have the right context in the original model.</li> <li>to have domain specific performance: the same sentence in different domain may relate differently and have different surroundings</li> <li>to improve the semantic similarity of your retriever</li> </ul> <p>Example on how to fine tune with sentence transformers</p> <p>In this example Philipp Schmidt demonstrates a 7% improvement (77% to 83%) fine-tuning the embedding model in the context of RAG. Moreover, using the \"matryoshka\" methodology defined above, you can reduce the embedding dimension by a factor of 10 while keeping the same performance.</p> <p>So fine-tuning is a key friend when you need topic and nlp context because it will also helps you to trade some compute performance against the business performance. However, from a practical point of view, do not start by fine-tuning. Start simply by evaluating your solution/pipeline and define criteria for evaluation. Once you reach a certain threshold comes the time you can focus on each component and twick those.</p> <p>https://huggingface.co/blog/embedding-quantization</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#multi-vector-embeddings","title":"Multi vector embeddings","text":"<p>Single Vector Embeddings In single vector embeddings, each document or text chunk is represented by one dense vector in high-dimensional space. This is the traditional approach where:</p> <p>Each document gets tokenized and encoded into a single fixed-size vector (e.g., 768 or 1536 dimensions) The entire semantic meaning of the document is compressed into this one vector Vector databases store and search using these single representations Similarity search finds the closest vectors using cosine similarity or other distance metrics</p> <p>Multi-Vector Embeddings Multi-vector embeddings represent each document using multiple vectors rather than just one. This approach:</p> <p>Splits documents into smaller chunks or uses different embedding strategies Creates multiple vectors per document to capture different aspects or granularities Can represent hierarchical information (document-level, paragraph-level, sentence-level) Allows for more nuanced retrieval by matching against the most relevant vector representation</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#vectordbs","title":"VectorDBs","text":"<p>docs/img/vectordb_weaviate.jpeg</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/#resources","title":"Resources","text":"<p>https://encord.com/blog/embeddings-machine-learning/?utm_source=linkedin https://docs.weaviate.io/weaviate/tutorials/multi-vector-embeddings https://weaviate.io/blog/muvera</p> <p>single_vs_multi</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/","title":"LLM Tokenizers: ew and most recent techniques","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#what-are-tokenizers","title":"What Are Tokenizers?","text":"<p>Tokenizers are fundamental components that serve as the bridge between human language and machine-readable data. They break down text into discrete components called \"tokens\"  which are numerical representations that Large Language Models (LLMs) can understand. Tokenization occurs as a preprocessing step in the LLM pipeline:</p> <pre><code>Raw Text \u2192 Tokenizer \u2192 Token IDs \u2192 Embeddings \u2192 Model Processing \u2192 Output\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#how-tokenizers-work","title":"How Tokenizers Work","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#basic-process","title":"Basic Process","text":"<ol> <li>Text Input: Raw text is provided to the tokenizer</li> <li>Segmentation: Text is broken down into smaller units (tokens)</li> <li>Vocabulary Mapping: Each token is mapped to a unique numerical ID</li> <li>Numerical Output: The sequence of token IDs is passed to the model</li> </ol>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#token-types","title":"Token Types","text":"<ul> <li>Character-level: Individual characters become tokens</li> <li>Word-level: Whole words become tokens  </li> <li>Subword-level: Parts of words become tokens (most common in modern LLMs)</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#most-common-tokenization-techniques","title":"Most Common Tokenization Techniques","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#1-byte-pair-encoding-bpe","title":"1. Byte Pair Encoding (BPE)","text":"<p>How it works: BPE is a data compression technique adapted for NLP that starts with individual characters and iteratively merges the most frequent pair of tokens to form new, longer tokens.</p> <p>Process:</p> <ul> <li>Initialize vocabulary with individual characters</li> <li>Find the most frequent pair of consecutive tokens</li> <li>Merge this pair into a new token</li> <li>Repeat until desired vocabulary size is reached</li> </ul> <p>Used by: GPT models, RoBERTa</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#2-wordpiece","title":"2. WordPiece","text":"<p>How it works: Similar to BPE but uses likelihood-based merging instead of frequency-based merging. WordPiece chooses symbol pairs that result in the largest increase in likelihood upon merging.</p> <p>Key differences from BPE:</p> <ul> <li>Uses likelihood maximization for pair selection</li> <li>Places \"##\" at the beginning of subword tokens</li> <li>Greedy algorithm leveraging likelihood instead of count frequency</li> </ul> <p>Used by: BERT, DistilBERT</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#3-sentencepiece","title":"3. SentencePiece","text":"<p>How it works: A re-implementation of subword units that treats the input as a raw input stream without pre-tokenization. It supports both BPE and Unigram algorithms and can handle any language without requiring language-specific preprocessing.</p> <p>Key features:</p> <ul> <li>Language-agnostic approach</li> <li>No pre-tokenization required</li> <li>Supports multiple segmentation algorithms</li> <li>Handles whitespace as regular characters</li> </ul> <p>Used by: T5, ALBERT, XLNet</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#4-unigram-language-model","title":"4. Unigram Language Model","text":"<p>How it works: Unlike BPE and WordPiece which build vocabulary bottom-up, Unigram starts with a large vocabulary and progressively removes tokens that have the least impact on the overall likelihood of the training data.</p> <p>Process:</p> <ul> <li>Start with large initial vocabulary</li> <li>Iteratively remove tokens with minimal impact on likelihood</li> <li>Continue until desired vocabulary size is reached</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#state-of-the-art-considerations","title":"State-of-the-Art Considerations","text":"<p>Recent techniques combine multiple tokenization strategies. Tokenizer can be trained on domain specific data</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#performance-factors","title":"Performance Factors","text":"<ul> <li>Vocabulary Size: Larger vocabularies can capture more semantic information but increase computational cost</li> <li>Compression Ratio: Balance between token efficiency and semantic preservation  </li> <li>Language Coverage: Ability to handle multiple languages and special characters</li> <li>Out-of-Vocabulary Handling: Graceful degradation for unseen text</li> </ul> <p>So all sequences have the same length, you can add padding to the short sentences with masked attention and shorten (truncate) longer sequences. Most recent techniques deal with that kind of limits.</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#encoding-and-decoding","title":"Encoding and decoding","text":"<p>Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained. The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it. Again, we need to use the same vocabulary used when the model was pretrained.</p> <p>Decoding is going the other way around: from vocabulary indices, we want to get a string. Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#tokenizing-numbers-a-long-history","title":"Tokenizing numbers: a long history","text":"<p>https://www.artfish.ai/p/how-would-you-tokenize-or-break-down</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Tokenizers/#sources","title":"Sources","text":"<ul> <li>The Technical User's Introduction to LLM Tokenization - February 28, 2024</li> <li>Introduction to LLM Tokenization - Airbyte - September 3, 2024</li> <li>Tokenization in large language models, explained - May 2, 2024</li> <li>Hugging Face Tokenizers Course</li> <li>WordPiece: Subword-based tokenization algorithm - Towards Data Science - January 24, 2025</li> <li>Google SentencePiece GitHub Repository</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Transformers/","title":"Transformers","text":""},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Transformers/#transformer-architecture","title":"Transformer architecture","text":"<p>The original transformer architecture: </p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Transformers/#play-with-transformers","title":"Play with transformers","text":"<p>Yes transformers are fun ! Here is a way to play with them: https://poloclub.github.io/transformer-explainer/</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Transformers/#attention-layers","title":"Attention layers","text":"<p>A key point of transformer models is that they are built with attention layers. The original technical paper Attention is all you Need! will provide you with a deep dive on this.</p> <p>Evolution of this layer: flash attention etc...</p> <p>There are two main approaches for training a transformer model:</p> <ul> <li>Masked language modeling (MLM): Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word).</li> <li>Causal language modeling (CLM): Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The model can only use context from the left (previous tokens) to predict the next token.</li> </ul> <p>Language models work by being trained to predict the probability of a word given the context of surrounding words. This gives them a foundational understanding of language that can generalize to other tasks.</p> <p>You will find more details and resources from the great LLM Hugging Face course</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Transformers/#merging","title":"Merging","text":"<p>https://huggingface.co/blog/mlabonne/merge-models</p>"},{"location":"GenAI%20Engineering/Chapter%202%20-%20LLM%20Pipeline/Transformers/#resources","title":"Resources","text":"<p>https://ml-course.github.io/master/notebooks/08%20-%20Transformers.html</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/","title":"What are adapters and why are they important ?","text":"<p>Adapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates \u2206W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources. This is an additional module injected to a frozen base model. Each PEFT method can use one or more types of adapters. https://huggingface.co/docs/peft/conceptual_guides/adapter This blog offers more technical details: https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#feature-extraction","title":"feature extraction","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#fine-tuning-techniques","title":"Fine tuning techniques","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#supervised-fine-tuning-sft","title":"Supervised Fine Tuning (SFT)","text":"<p>Given access to a pre-trained model, full fine-tuning, which involves adjusting all the model\u2019s weights for a new task, is a viable approach. However, this method can be resource-intensive and may pose risks such as overfitting, especially with smaller datasets, or catastrophic forgetting. Supervised finetuning, involves training the model with instruction-output pairs, where the instruction serves as the input and the output is the model\u2019s desired response.</p> <p>This stage uses smaller datasets compared to pretraining, as creating instruction-output pairs requires significant effort, often involving humans or another high-quality LLM. The process focuses on refining the model\u2019s ability to produce specific outputs based on given instructions.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#what-is-the-difference-between-transfer-learning-and-sft","title":"What is the difference between transfer learning and SFT ?","text":"<p>Transfer learning focuses on how to transfer the knowledge gained from one task to accelerate learning for a new, related task or domain. The key benefits is the small amount of data required to perform transfer learning with the little !!!!! probability for overfitting given the amount of data Another transfer learning option is feature based (embeddings then task e.g. classification)</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#what-are-the-limits-of-fine-tuning","title":"What are the limits of fine-tuning ?","text":"<ul> <li>knowledge that is too far (new language) than pre-training can lead to poor results</li> <li>Fine tuning on a new knowledge could increase hallucinations and could erase some knowledge that generalized well (catastrophic forgetting)</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#why-memory-is-such-a-big-deal","title":"Why memory is such a big deal ?","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#how-to-estimate-the-memory-consumption-for-sft","title":"How to estimate the memory consumption for SFT ?","text":"<p>Estimated memory consumption: parameters + gradients + optimizer states + activations</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#estimate-memory-requirements-for-full-fine-tuning-llm","title":"Estimate memory requirements for full fine-tuning LLM ?","text":"<p>We'll go through a common example: Mistral Instruct 7 Billion</p> <ul> <li>Model parameters memory: assuming it occupies 4 bytes (32-bits), the model needs 7e9 * 4 bytes = 28GB</li> <li>Gradient calculation: similar to the model memory for float32 i.e. 28 GB</li> <li>Backward pass: need to compute and store intermediate activations for backpropagation: Number of layers size of activations per layer batch size = Memory required for storing intermediate activations. It can be estimated for Mistral (not public) to 54.98 GB</li> <li>Optimizer step: estimated to 109.96 GB for float 32</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#alternatives-to-fine-tuning","title":"Alternatives to fine tuning","text":"<p>Alternatives to tuning a model (In context learning ; Zero/Few shot inference)</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#pruning-and-distillation","title":"Pruning and distillation","text":"<p>https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model https://snorkel.ai/blog/llm-distillation-demystified-a-complete-guide/</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#domain-tuning","title":"Domain tuning","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#transfer-learning","title":"Transfer learning","text":"<p>\u00b2</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#data-preparation-for-large-model-tuning","title":"Data preparation for large model tuning","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#full-fine-tuning","title":"full fine tuning","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/#long-context-scaling","title":"Long context scaling","text":"<p>https://www.gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Frameworks/","title":"Frameworks","text":"<p>llama.cpp litgpt vllm https://github.com/SylphAI-Inc/LLM-engineer-handbook?tab=readme-ov-file#fine-tuning</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Frameworks/#how-do-we-implement-all-these-techniques","title":"How do we implement all these techniques ?","text":"<p>transformers Bits&amp;bytes https://huggingface.co/docs/trl/index https://huggingface.co/docs/transformers/accelerate Unsloth</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/","title":"Inference Optimizations","text":"<p>2 bottlenecks: compute-bound (computation needed) and memory bandwidth (moving data between CPU and GPU for instance) Batch (different than ML cause we cannot anticipate) vs online inference Metrics to optimize: latency, time to first token (TTFT, time per output token(TPOT), time between tokens and inter token latency Throughput (tokens per second) or Request Per Minute (RPM) New metric: Goodput aka satisfaction of the Software Level Objective GPU utilization (for Nvidia nvidia-smi): percentage of TIME during which the GPU is actively processing tasks \u2014&gt; so not useful MFU (model flops/s utilization) : ratio of observed throughput relative to the theoretical max. Throughput MBU  = Model bandwidth utilization</p> <p>Model Optimization</p> <ul> <li>Model compression: quantization, distillation, pruning (process to remove entire node or put parameters to 0) is less common</li> <li>Autoregressive decoding bottlenecks (can generate one token after the other) fixed are:</li> <li>speculative decoding: generate draft model (smaller) sequentially and validate tokens in parallel by final model</li> <li>Inference with reference: when an overlap between input context and output tokens answer (code, chance\u2026) save input tokens from being generated hence send draft tokens to the model</li> <li>Parallel decoding: generate K future tokens \u2014&gt; verify for coherence and consistency \u2014&gt; if some fails regenerate failing ones</li> <li>Attention mechanism optimisation:<ul> <li>KV (key-value) cache I.e. reuse vector generated in step t-1</li> <li>Modify attention mechanism</li> <li>KV cache reduction</li> <li>Writing kernels for attention computation: for instance flash attention uses a fused kernel</li> </ul> </li> <li>Kernels (pieces of codes optimized for specific hardware accelerators such as GPU or TPU)</li> <li>Optimize kerner on Nvidia or AMD chip makers</li> <li>Optimize framework on tensor flow or PyTorch for different accelerators</li> <li>Optimization techniques: vectorization, parallelization, loop tiling, operator fusion</li> </ul> <p>Inference Serving Optimization</p> <ul> <li>Batching: continuous is the most used</li> <li>Decouple pre-fill and decoding</li> <li>Prompt caching</li> <li>Parallelism</li> </ul> <p>Inference optimization: Main steps are: tokenizing, compute key and value Paris and generate output tokens</p> <ul> <li>KV cache</li> <li>Continuous batching (native in TGI, Nvidia rt, vllm)</li> <li>Speculative decoding: compute multiple tokens simultaneously with a smaller model and validate with a bigger model</li> <li>Optimized attention mechanisms: paged attention, flash attention-2</li> <li>Model parralelism: distribute and computer requirements across multiple GPUs</li> <li>Data parallelism: duplicate model into multiple GPUs and parallels concurrent requests but mainly used for training given limitation in communication overhead limits</li> <li>Pipeline parallelism: partition model layer\u2019s across different GPU (no replication)</li> <li>Tensor parallelism: distribute computation of LLM layers across multiple devices. Not applicable to all layers of a NN</li> <li>Quantization</li> </ul> <p>Metrics for LLM:</p> <ul> <li><code>Time to First Token (TTFT): The time it takes for the first token to be generated</code></li> <li><code>Time between Tokens (TBT): The interval between each token generation</code></li> <li><code>Tokens per Second (TPS): The rate at which tokens are generated</code></li> <li><code>Time per Output Token (TPOT): The time it takes to generate each output token</code></li> <li><code>Total Latency: The total time required to complete a response</code></li> </ul> <p>https://github.com/stas00/ml-engineering/tree/master/inference</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#llm-inference-optimization-techniques","title":"LLM Inference Optimization techniques","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#api-vs-model-serving","title":"API vs Model serving","text":"<p>The inference optimization techniques should be relevant to the type of serving you choose. It is clear that invoking an LLM model served on a cluster or invoking an API lead to different trade-offs and optimization. Yet, the main metrics one can follow are similar.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#embrace-async","title":"Embrace async","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#llm-serving-optimization","title":"LLM serving optimization","text":"<p>There are 3 main strategies of optimized inference (data, model and system).</p> <p>Source: A Survey on Efficient Inference for Large Language Models</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#model-optimization","title":"Model optimization","text":"<p>There are two main steps:</p> <ul> <li>prefill: processing the full prompt length at once and cache intermediate steps (kv caching). It contributes to little latency.</li> <li>decode: new tokens generation based on all (or partially) previous tokens. Harder to paralllelized to it is the part that takes the most time</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#quantization","title":"Quantization","text":"<p>Quantization is a technique that reduces the precision of the tensors in the model. A few library like bitsandbites, trl, unsloth... allows you to run quantize your model so it's faster at inference.</p> <p>Read more in this article: https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#system-level-optimization","title":"System-level optimization","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#batch-inference","title":"Batch inference","text":"<p>As the decoding stage is taking time, batching multiple queries together improve the utilization and enables processing multiple requests at once.</p> <p>Source: https://www.baseten.co/blog/continuous-vs-dynamic-batching-for-ai-inference/</p> <p>Continuous batching can be considered a variation of Dynamic Batching, only that it works at the token level instead of the request level. For continuous batching, a layer of the model is applied to the next token of each request. In this manner, the same model weights could generate the N\u2019th token of one response and the Nx100 token of another. Once a sequence in a batch has completed generation, a new sequence can be inserted in its place, yielding higher GPU utilization than static batching.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#decoding-methods","title":"Decoding methods","text":"<p>As decoding is taking the most time, optimizing this step is the most important. In time, users moved through a few methods:</p> <ul> <li>greedy decoding: you take the most probably token at each step which is clearly a limit</li> <li>beam search: you take the n most likely tokens to be sampled to select a token from the top k likely options</li> <li>speculative decoding: you do not need all previous tokens you rather make educated guesses about future tokens all within a forward pass. Eventually, a verification mechanism ensure the robustness.</li> </ul> <p>You can find more details about decoding techniques here</p> <p>Speculative decoding explained here</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#key-value-cache-and-optimized-attention-mechanism","title":"Key-value cache and optimized attention mechanism","text":"<p>Key-value caching are now embedded into most transformers based library. As the decode phase generates a single token at each time step. Still, each token depends on all previous tokens' key and value tensors (including the input tokens\u2019 KV tensors computed at prefill and any new KV tensors computed until the current time step).</p> <p>To avoid recomputing all these tensors for all tokens at each time step, it\u2019s possible to cache them in GPU memory.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#parallelization-optimization-techniques","title":"Parallelization Optimization Techniques","text":"<p>One way to reduce the per-device memory footprint of the model weights is to distribute the model over several GPUs. This enables the running of larger models or batches of inputs. Based on how the model weights are split, there are three common ways of parallelizing the model: pipeline, sequence, and tensor.</p> <p>Pipeline parallelism involves sharding the model (vertically) into chunks, each comprising a subset of layers executed on a separate device. Thus, each device's memory requirement for storing model weights is effectively quartered.</p> <p>Limitation: Because this execution flow is sequential, some devices may remain IDLE while waiting for the output of the previous layers.</p> <p>Tensor parallelism involves sharding (horizontally) individual layers of the model into smaller, independent blocks of computation that can be executed on different devices. In transformer models, the Attention Blocks and MLP (normalization) layers will benefit from Tensor Parallelism because large LLMs have multiple Attention Heads. To speed up the computation of Attention Matrices, which is done independently and in parallel, we could split them into one per device.</p> <p>Tensor parallelism has limitations. It requires layers to be divided into independent, manageable blocks. This does not apply to operations like LayerNorm and Dropout, which are replicated across the tensor-parallel group. These layers are computationally inexpensive but require considerable memory to store activations.</p> <p>To mitigate this bottleneck, sequence parallelism partitions these operations along the \u201csequence dimension\u201d where the Tensor Parallelised layers are, making them memory efficient.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#data-level-optimization","title":"Data-level optimization","text":"<p>Prompt compression and prompt caching are discussed in the next sections as they are eligible for API inference.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#api-inference","title":"API inference","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#prompt-caching","title":"Prompt caching","text":"<p>Model prompts often contain repetitive content, like system prompts and common instructions. routes API requests to servers that recently processed the same prompt, making it cheaper and faster than processing a prompt from scratch (as explained above). This can reduce latency by up to 80% and cost by up to 75% according to OpenAI. Recent model all offers this behavior by default.</p> <p>Tips: Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.</p> <p>The following elements are cached:</p> <ul> <li>messages</li> <li>images</li> <li>tool use</li> <li>structured outputs</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#prompt-compression","title":"Prompt compression","text":"<p>Prompt compression is the process of systematically reducing the number of tokens fed into a large language model to retain or closely match the output quality comparable to that of the original, uncompressed prompt. This leads to cost reduction, speed improvement and token limits optimization.</p> <p>One of the main challenges in prompt compression is maintaining the essential context while reducing prompt length. When too much information is removed, the LLM may: misinterpret the user\u2019s intent, provide vague or irrelevant responses, omit critical details necessary for accurate answers</p> <p>Mitigation strategies are:</p> <ul> <li>Start first by basic summarization before using more advanced techniques</li> <li>Set a compression threshold to preserve most context</li> <li>In some cases, you just need entities to be served directly in the prompt. Use pydantic to extract and keep defined format.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#metrics-to-monitor","title":"Metrics to monitor","text":"<ul> <li>Time to First Token (TTFT): The time it takes for the first token to be generated</li> <li>Time between Tokens (TBT): The interval between each token generation</li> <li>Tokens per Second (TPS) or throughput: The rate at which tokens are generated</li> <li>Time per Output Token (TPOT): The time it takes to generate each output token</li> <li>Total Latency: The total time required to complete a response</li> <li>GPU Utilization (models using GPU): percentage of time during which the GPU is actively processing tasks</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/#sources","title":"Sources","text":"<p>Great resource on LLM optimization: https://multimodalai.substack.com/p/understanding-llm-optimization-techniques KV cache explained: https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms https://bentoml.com/llm/inference-optimization continuous batching : https://huggingface.co/blog/continuous_batching</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Model_choice/","title":"How to choose the right model","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Model_choice/#main-considerations","title":"Main considerations","text":"<p>Model Size VRAM (FP16) VRAM (4-bit) Cloud Options Local Hardware Best Use Cases 1\u20133B 4\u20136 GB ~2 GB AWS g4dn.xlarge, basic GPU instances RTX 3060, laptop GPUs Basic chat, text classification, autocomplete 7\u20138B 14\u201316 GB ~6\u20138 GB AWS g5.xlarge, RunPod RTX 4090 RTX 4080/4090, A6000 General-purpose assistants, summarization, coding 13\u201314B 26\u201328 GB ~12\u201316 GB AWS g5.2xlarge, multi-instance RTX 4090 (quantized only) Stronger reasoning, better instruction following 70B+ 140 GB+ ~35\u201340 GB AWS p4d.24xlarge, A100 clusters Multi-GPU setups (expensive) SOTA reasoning, enterprise applications</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Quantization/","title":"Understanding quantization","text":"<p>*** Add cleaned notebooks from deeplearning AI ****</p> <p>Top explanation: https://docs.weaviate.io/weaviate/concepts/vector-quantization</p> <p>https://blog.gopenai.com/exploring-bits-and-bytes-awq-gptq-exl2-and-gguf-quantization-techniques-with-practical-examples-74d590063d34</p> <p>https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96</p> <p>https://huggingface.co/blog/embedding-quantization#try-it-yourself</p> <p>Quantization considerations:</p> <ul> <li>4-bit quantization reduces memory to ~25% of original - a 7B model drops from ~14GB to ~3.5GB</li> <li>8-bit quantization halves memory requirements with minimal quality loss</li> </ul> <p>https://bentoml.com/llm/getting-started/llm-quantization https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/ https://yousefhosni.medium.com/overview-of-llm-quantization-techniques-where-to-learn-each-of-them-0d8599acfec8</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/","title":"PEFT","text":"<p>Parameter-efficient fine-tuning is particularly used in the context of large-scale pre-trained models (such as in NLP), to adapt that pre-trained model to a new task without drastically increasing the number of parameters. It arises because regular fine-tuning can be costly, time consuming, require great amount of data, can lead to overfitting. Moreover, introducing additional layers or parameters during fine-tuning can drastically increase computational requirements and memory consumption.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#how-does-it-work","title":"How does it work ?","text":"<p>PEFT works by freezing most of the pretrained language model\u2019s parameters and layers while adding a few trainable parameters, known as adapters, to the final layers for predetermined downstream tasks. PEFT adapts a pre-trained model to downstream tasks more efficiently. It reduces the # of trainable parameters reducing the memory needed by introducing adapters (More details on HF courses)</p> <p>PEFT is often used during transfer learning, where models trained in one task are applied to a second related task. For example, a model trained in image classification might be put to work on object detection. If a base model is too large to completely retrain or if the new task is different from the original, PEFT can be an ideal solution</p> <p>What are the available PEFT techniques ?  Source: https://vinija.ai/nlp/parameter-efficient-fine-tuning/#which-peft-technique-to-choose-a-mental-model</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#when-should-i-use-peft-over-other-methods","title":"When should I use PEFT over other methods ?","text":"<p>Parameter-efficient fine-tuning is useful due the following reasons:</p> <ul> <li>Reduced computational costs (requires fewer GPUs and GPU time).</li> <li>Faster training times (finishes training faster).</li> <li>Lower hardware requirements (works with cheaper GPUs with less VRAM).</li> <li>Better modeling performance. It allows the model to adjust for the most relevant parameters</li> <li>Less storage (majority of weights can be shared across different tasks).</li> <li>no catastrophic forgetting. Catastrophic forgetting happens when LLMs lose or \u201cforget\u201d the knowledge gained during the initial training process as they are retrained or tuned for new use cases. Because PEFT preserves most of the initial parameters, it also safeguards against catastrophic forgetting.</li> <li>Transformer models tuned with PEFT are much less prone to overfitting as most of their parameters remain static.</li> <li>Lower data need than full fine tuning (you can use 1 000 observations and already engage with improvement)</li> </ul> <p>The next sections describe the main PEFT families.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#soft-prompt-techniques","title":"Soft prompt techniques","text":"<p>As opposed to hard prompt (user input going into the model) soft prompt modifies how the model processes input tokens. Hard prompts are manually handcrafted text prompts with discrete input tokens; the downside is that it requires a lot of effort to create a good prompt. soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset; the downside is that they aren\u2019t human readable because you aren\u2019t matching these \u201cvirtual tokens\u201d to the embeddings of a real word If you want to know more about these techniques, Hugging Face has some fundamentals about it PEFT Prompting Guide</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#when-should-i-use-it","title":"When should I use it ?","text":"<p>Prompt Tuning is a good choice when you have a large pre-trained LLM but want to fine-tune it for multiple different downstream tasks at inference time with minimal computational resources. It is also useful when you want to generate diverse and high-quality text outputs based on specific prompts When you want to fine-tune a pre-trained LLM for a specific downstream task and have limited computational resources when you want to modify the representation learned by the pre-trained model for a particular task.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#ia3","title":"IA3","text":"<p>(useful for multi-task fine tuning )</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#loraqlora-which-is-one-of-the-most-used-techniques-to-date","title":"Lora/QLora which is one of the most used techniques to date","text":"<p>It decomposes Weights matrices into 2 matrices of lower dimension. Rank r the smaller the better optimized but the potentially higher loss of information \u2014&gt; Lora is not applied at pre-training yet only in FT. it is applied to all query, key, value and output projection. Usually r between 4 and 64 is efficient ; also need to parameter alpha = how much the product should contribute to the new metric during merging. Merging is necessary either prior to inference or during inference to serve the new combined weights. If you have multi-lora serving it\u2019s better to serve at runtime but that means more latency (merging on the run) Another great explanation for LoRA can be found here : https://lightning.ai/pages/community/tutorial/lora-llm/</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#when-should-i-use-it_1","title":"When should I use it ?","text":"<p>LoRA is a good choice when you want to fine-tune a pre-trained LLM for a specific downstream task that requires task-specific attention patterns. It is also useful when you have limited computational resources and want to reduce the number of trainable parameters in the model. Specifically:</p> <ul> <li>Memory Efficiency is Desired but Not Critical: LoRA offers substantial savings in terms of parameters and computational requirements. If you\u2019re looking to achieve a balanced reduction in trainable parameters without diving into the complexities of quantization, LoRA is an ideal choice.</li> <li>Real-time Application: LoRA ensures no added inference latency, making it suitable for real-time applications.</li> <li>Task-Switching is Required: LoRA can share the pretrained model across multiple tasks, reducing the need for maintaining separate models for each task.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#serving-loraqlora","title":"Serving Lora/Qlora","text":"<p>In short, LoRA, short for Low-Rank Adaptation (Hu et al 2021), adds a small number of trainable parameters to the model while the original model parameters remain frozen.</p> <p>Lora/QLora decompose Weights matrices into 2 matrices of lower dimension. Rank r the smaller the better optimized but the potentially higher loss of information \u2014&gt; Lora is not applied at pre-training yet only in FT. it is applied to all query, key, value and output projection. Usually r between 4 and 64 is efficient ; also need to parameter alpha = how much the product should contribute to the new metric during merging. Merging is necessary either prior to inference or during inference to serve the new combined weights. If you have multi-lora serving it\u2019s better to serve at runtime but that means more latency (merging on the run)</p> <p>https://aws.amazon.com/fr/blogs/machine-learning/efficient-and-cost-effective-multi-tenant-lora-serving-with-amazon-sagemaker/ https://github.com/predibase/lorax</p> <p>Source from Lora paper https://github.com/microsoft/LoRA?tab=readme-ov-file</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#what-is-qlora","title":"What is QLora ?","text":"<p>Quantized-LoRA takes LoRA a step further by also quantizing the weights of the LoRA adapters (smaller matrices) to lower precision (e.g., 4-bit instead of 8-bit). This further reduces the memory footprint and storage requirements. The aim is to achieve similar effectiveness (i.e. model performance) while reducing even more the memory needed. The downside is that it requires more training time than LoRA method.</p> <p> Source: https://arxiv.org/pdf/2305.14314.pdf</p> <p>I like this deep dive in the paper and explanations about QLoRA: https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/ In summary:</p> <ul> <li>the 4-bit quantization does not exactly mean that your weights go 4bits. It is a compression technique that allows us to store using less memory. It is used during fine tuning and inference. However, you still need to decompress the weights back to 16-bit to run the model. It doesn\u2019t make the forward or backward calculations any easier or faster, or require any less memory. Math is still done at 16-bit precision, and all of the activations, gradients, and other optimizer states are all still stored as 16-bit floats.</li> <li>because of that, we have to store metadata which forces us to store more data. So it rathers achieve a 3.76x compression rate</li> <li>It works by leveraging three key aspects of neural network weights:</li> <li>Weight values are normally distributed.</li> <li>Large weight values are the most important (and 4Q preserves large weight values with high precision).</li> <li>Tiny weight values are irrelevant (and 4Q just rounds these to zero).</li> </ul> <p>The vanilla formula to estimate the amount of GPU required to serve an LLM: ( A Number of parameters in the model * 4Bytes) / (32 / amount of bits that should be used for loading i.e. 16, 8 or 4) * 1.2 Note that 1.2 represents only a 20% overhead of loading additional metadata in the GPU memory</p> <p>For instance, a 7 Billion parameter model using 4 bytes (32-bits) needs around 17 GB of memory.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#tips-for-fine-tuning-llms-using-lora","title":"Tips for fine tuning LLMs using LoRA","text":"<ul> <li>Consistency in LLM Training: Despite the inherent randomness in training models on GPUs, the outcomes of LoRA experiments remain consistent across multiple runs, which is promising for comparative studies.</li> <li>QLoRA Compute-Memory Trade-offs: Quantized LoRA (QLoRA) offers a 33% reduction in GPU memory usage at the cost of a 33% increase in runtime, proving to be a viable alternative to regular LoRA when facing GPU memory constraints.</li> <li>Learning Rate Schedulers: Using learning rate schedulers like cosine annealing can optimize convergence during training and avoid overshooting the loss minima. While it has a notable impact on SGD optimizer performance, it makes less difference when using Adam or AdamW optimizers.</li> <li>Choice of Optimizers: The optimizer choice (Adam vs. SGD) doesn\u2019t significantly impact the peak memory demands of LLM training, and swapping Adam for SGD may not provide substantial memory savings, especially with a small LoRA rank (r).</li> <li>Impact of Multiple Training Epochs: Iterating multiple times over a static dataset in multi-epoch training may not be beneficial and could deteriorate model performance, possibly due to overfitting.</li> <li>Applying LoRA Across Layers: Enabling LoRA across all layers, not just the Key and Value matrices, can significantly increase model performance, though it also increases the number of trainable parameters and memory requirements.</li> <li>LoRA Hyperparameters: Adjusting the LoRA rank (r) and selecting an appropriate alpha value are crucial. A heuristic that yielded good results was setting alpha at twice the rank\u2019s value, with r=256 and alpha=512 being the best setting in one particular case.</li> <li>Fine-tuning Large Models: LoRA allows for fine-tuning 7 billion parameter LLMs on a single GPU with 14 GB of RAM within a few hours. However, optimizing an LLM to excel across all benchmark tasks may be unattainable with a static dataset.</li> <li>Importance of Dataset: The dataset used for fine-tuning is critical, and data quality is very important. Experiments showed that a curated dataset with fewer examples (like LIMA) could yield better performance than larger datasets (like Alpaca).</li> <li>Avoiding Overfitting: To prevent overfitting, one could decrease the rank or increase the dataset size, adjust the weight decay rate, or consider increasing the dropout value for LoRA layers.</li> <li>Factors Influencing Memory Usage: Model size, batch size, the number of trainable LoRA parameters, and dataset size can influence memory usage. Shorter training sequences can lead to substantial memory savings.</li> </ul> <p>Sources: https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms https://lightning.ai/pages/community/lora-insights/</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#rlhf","title":"RLHF","text":"<p>Reinforcement Learning from Human Feedback (RLHF) comes first from Reinforcement learning fields. It composes with a try and error process where, given an environment, an agent makes some predefined actions in this environment based on observations. Actions are defined through a policy and will imply consequences which will be a reward (positive or negative) that will modify the state of the environment.</p> <ul> <li>A reward model is learnt from the human feedback</li> <li>Policy optimization: with the learned reward model, standard RM algos (PPO for instance) are used to optimize the policy which generates new behavior</li> <li>Iterative improvement: new behaviors lead to refinement of the reward model PB: computationnaly expensive and unstable leading to a simpler approach: direct preference optimization -</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#reinforcement-learning-through-human-feedback-rlhf-a-crash-course","title":"Reinforcement Learning through Human Feedback (RLHF) - A Crash Course","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#overview","title":"Overview","text":"<p>Reinforcement Learning through Human Feedback (RLHF) is a technique that integrates human input to align AI models with human preferences, particularly effective for tasks where defining clear metrics is challenging. Unlike Direct Preference Optimization (DPO), which directly uses preference data, RLHF involves training a reward model based on human feedback, which is then used in reinforcement learning to optimize the model.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#key-components-of-rlhf","title":"Key Components of RLHF","text":"<ol> <li>Reward Model Training: Humans provide feedback on AI-generated outputs, which is used to train a reward model. This model assigns scores to the outputs, indicating how well they meet human preferences.</li> <li>Reinforcement Learning: The reward model guides the AI's learning process, often using algorithms like Proximal Policy Optimization (PPO), to improve the model's performance iteratively.</li> <li>Iterative Process: The model generates outputs, receives feedback, and refines its behavior based on the reward model, continuing until desired performance is achieved.</li> </ol>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#advantages-of-rlhf","title":"Advantages of RLHF","text":"<ul> <li>Generalization: RLHF is effective across diverse tasks, capturing human nuances that are hard to encode algorithmically.</li> <li>Rich Feedback: It leverages human expertise to refine AI outputs, ensuring alignment with complex, context-dependent preferences.</li> <li>Versatility: Suitable for tasks like text generation, where human judgment is essential for evaluating quality.</li> <li>Better control: we can discourage toxic (or any other unwanted behaviour) by modifying the reward</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#disadvantages-of-rlhf","title":"Disadvantages of RLHF","text":"<ul> <li>High Resource Requirement: Collecting extensive human feedback can be time-consuming and costly.</li> <li>Consistency Challenges: Variability in human feedback can lead to inconsistencies in the reward model.</li> <li>Complexity: Involves training both a reward model and the AI policy, increasing complexity.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#comparison-with-dpo","title":"Comparison with DPO","text":"<ul> <li>DPO: Simplifies the process by directly using preference data without a reward model, making it computationally efficient.</li> <li>RLHF: Offers potential for capturing more nuanced preferences but introduces complexity and resource intensity.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#choosing-between-rlhf-and-dpo","title":"Choosing Between RLHF and DPO","text":"<ul> <li>RLHF: Ideal for complex tasks where nuanced human preferences are crucial.</li> <li>DPO: Suitable for simpler tasks or when resources are limited, offering a streamlined approach.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#optimization-tips-for-rlhf","title":"Optimization Tips for RLHF","text":"<ol> <li>Feedback Quality: Ensure high-quality, consistent human feedback to enhance the reward model's effectiveness.</li> <li>Active Learning: Prioritize feedback collection on outputs that provide the most information to improve the model efficiently.</li> <li>Algorithm Exploration: Investigate newer reinforcement learning algorithms beyond PPO to enhance performance and scalability.</li> </ol> <p>Among other techniques you may find: PPO and DPO. Proximal Policy Optimization (PPO) was one of the first highly effective techniques for RLHF. It uses a policy gradient method to update the policy based on the reward from a separate reward model</p> <p>https://huggingface.co/blog/rlhf https://toloka.ai/blog/rlhf-ai/ https://toloka.ai/blog/proximal-policy-optimization/</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#dpo","title":"DPO","text":"<p>Fine tuning with preference alignment</p> <ul> <li>Creating preference datasets follows the same process as instruction dataset but with for each instruction a chosen answer and a rejected answer</li> <li>DPO datasets require fewer samples than instruction datasets</li> <li>DPO is less destructive than SFT</li> <li>Derives a closed form expression for the optimal policy under the standard RLHF objective of max. Expected reward subject to a KL divergence constraint against a reference policy</li> <li>Implemented as a binary cross-entropy loss function</li> <li>More computationally efficient than traditional RLHF (when trained with adapters, frozen and trained models don\u2019t need to be separated)</li> <li>More stable during training and less sensitive to hyper parameter</li> </ul> <p>Data Preference Optimization (DPO) is a fine-tuning technique used in Large Language Models (LLMs) that directly optimizes the model based on human preferences [1][2][3][4][5]. This approach eliminates the need for a separate reward model and instead uses comparative feedback data to refine the LLM. DPO has been shown to be a powerful approach to enhancing machine learning models by directly incorporating user feedback, enabling the creation of models that better align with human expectations and needs [1].</p> <p>The key benefits of DPO include its simplicity, efficiency, and direct control over LLM behavior [3]. By eliminating the need for a complex reward model, DPO significantly reduces the computational cost of fine-tuning, making it a valuable asset for developers looking to quickly upgrade their language models [2]. Additionally, DPO allows users to have a more direct influence on the LLM's behavior, guiding the model towards specific goals and ensuring it aligns with their expectations [3].</p> <p>DPO has been compared to other fine-tuning techniques, such as Reinforcement Learning from Human Feedback (RLHF) [2][3]. While RLHF allows for more complex and nuanced reward structures, DPO's simpler approach can be beneficial for tasks requiring rapid iteration and feedback loops [3]. DPO has also been shown to outperform RLHF in certain scenarios, particularly regarding sentiment control and response quality in tasks like summarization and dialogue [3].</p> <p>However, DPO also has its challenges, such as the risk of overfitting [4][5]. To prevent overfitting, it is essential to collect diverse high-quality data that covers a wide range of preferences and scenarios [5]. Additionally, DPO may not be suitable for tasks that require precise control over the LLM's output, where RLHF's flexibility in defining rewards may be beneficial [3].</p> <p>In conclusion, DPO is a promising fine-tuning technique that offers a simple, efficient, and direct approach to optimizing LLMs based on human preferences [1][2][3][4][5]. While it has its challenges, DPO has the potential to make LLM fine-tuning faster, cheaper, and more stable, driving innovation in the field of artificial intelligence [2]. The choice between DPO and other fine-tuning techniques, such as RLHF, depends on the specific task, available resources, and desired level of control [3].</p> <p>References:</p> <p>[1] Source 1: Collecting Preference Data [2] Source 2: Can DPO scale to a real preference dataset? [3] Source 3: D.P.O: A Simple and Direct Approach [4] Source 4: Alignment without Reinforcement Learning [5] Source 5: What is DPO?</p> <p>https://medium.com/@mauryaanoop3/detailed-guide-on-dpo-fine-tuning-027815d15837 https://huggingface.co/blog/dpo-trl https://medium.com/@sinarya.114/d-p-o-vs-r-l-h-f-a-battle-for-fine-tuning-supremacy-in-language-models-04b273e7a173 https://toloka.ai/blog/direct-preference-optimization/ https://huggingface.co/blog/pref-tuning</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#direct-preference-optimization-dpo-explained","title":"Direct Preference Optimization (DPO) Explained","text":"<p>Direct Preference Optimization (DPO) is a method for fine-tuning large language models (LLMs) to align their outputs with human preferences. Unlike traditional reinforcement learning (RL)-based approaches, DPO simplifies the process by directly incorporating preference data into the training process. This reduces complexity and computational requirements while maintaining or improving performance. Here's an overview, comparison with other methods, and guidance on when to use DPO.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#overview-of-dpo","title":"Overview of DPO","text":"<p>DPO aligns LLMs with human preferences by using preference data from human evaluators. It directly optimizes the model's policy based on this data, eliminating the need for complex reward modeling or extensive hyperparameter tuning. This streamlined approach has been shown to match or exceed the performance of more complex methods like Reinforcement Learning from Human Feedback (RLHF), particularly in tasks such as sentiment control, summarization, and dialogue generation [1][2].</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#advantages-of-dpo","title":"Advantages of DPO","text":"<ul> <li>Simplicity: DPO avoids the complexity and instability of RLHF by directly using preference data in supervised learning [2][3].</li> <li>Computational Efficiency: It reduces computational overhead by avoiding the need for multiple model instances and extensive hyperparameter tuning [3].</li> <li>Performance: DPO has demonstrated comparable or superior performance in aligning LLMs with human preferences, particularly in controlling the sentiment of generated outputs [1][2].</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#differences-from-other-fine-tuning-techniques","title":"Differences from Other Fine-Tuning Techniques","text":"<ol> <li>Reinforcement Learning from Human Feedback (RLHF): RLHF involves multiple steps, including reward modeling and policy optimization, which introduces complexity and computational overhead. DPO, on the other hand, streamlines this process by directly optimizing the model using preference data [2][3].</li> <li>Conditional Supervised Fine-tuning: This method relies on conditional prompts to guide the model's outputs. While effective, it may require additional fine-tuning and does not directly use preference data for optimization [3].</li> <li>Other RL-Based Methods: These methods often involve complex reward functions and multiple training iterations. DPO simplifies this by directly incorporating preference data, reducing the need for extensive hyperparameter tuning and multiple model instances [2][3].</li> </ol>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#when-to-use-dpo","title":"When to Use DPO","text":"<p>DPO is particularly relevant in scenarios where simplicity, computational efficiency, and effective alignment with human preferences are priorities. It is ideal for:</p> <ul> <li>Resource-Constrained Environments: When computational resources are limited, DPO's reduced overhead makes it a practical choice.</li> <li>Rapid Deployment: Its straightforward implementation allows for quicker fine-tuning and deployment of models aligned with human preferences.</li> <li>Preference-Based Tasks: For tasks requiring nuanced control over model outputs, such as sentiment management or dialogue generation, DPO's performance advantages make it a strong candidate [1][2].</li> </ul> <p>https://medium.com/@lmpo/direct-preference-optimization-a-novel-approach-to-language-model-alignment-1f829d4ac306</p> <p>sebastien links: https://github.com/rasbt/LLMs-from-scratch/tree/main/ch07/04_preference-tuning-with-dpo</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#group-relative-policy-optimization-grpo","title":"Group Relative Policy Optimization (GRPO)","text":"<p>GRPO groups similar samples together and compares them as a group. The group-based approach provides more stable gradients and better convergence properties compared to other methods.</p> <p>GRPO does not use preference data like DPO, but instead compares groups of similar samples using a reward signal from a model or function.</p> <p>GRPO is flexible in how it obtains reward signals - it can work with a reward model (like PPO does) but doesn\u2019t strictly require one. This is because GRPO can incorporate reward signals from any function or model that can evaluate the quality of responses.</p> <p>GRPO produces multiple solutions at once and group them together. The evaluation is made at the group with multiple evaluation possible. The good answer are kept while the bad answer are used to move away the model from these. For stability, KL divergence metrics are used to tune how fast moving away from existing solutions.</p> <p>It looks at multiple solutions together rather than comparing just two at a time The group-based normalization helps prevent issues with reward scaling The KL penalty acts like a safety net, ensuring the model doesn\u2019t forget what it already knows while learning new things</p> <p>Limitations: Generation Cost: Generating multiple completions (4-16) for each prompt increases computational requirements compared to methods that generate only one or two completions. Batch Size Constraints: The need to process groups of completions together can limit effective batch sizes, adding complexity to the training process and potentially slowing down training. Reward Function Design: The quality of training heavily depends on well-designed reward functions. Poorly designed rewards can lead to unintended behaviors or optimization for the wrong objectives. Group Size Tradeoffs: Choosing the optimal group size involves balancing diversity of solutions against computational cost. Too few samples may not provide enough diversity, while too many increase training time and resource requirements. KL Divergence Tuning: Finding the right balance for the KL divergence penalty requires careful tuning - too high and the model won\u2019t learn effectively, too low and it may diverge too far from its initial capabilities.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#instruction-tuning","title":"Instruction tuning","text":"<p>Instruction Tuning Instruction fine-tuning (IFT) is a type of SFT leveraged in LLMs to improve their ability to follow instructions and generate more accurate and relevant responses. This technique involves training the model on a dataset of prompts followed by ideal responses, guiding the model to better understand and execute various types of instructions. (FLAN) was the first to introduce instruction tuning which finetunes the model on a large set of varied instructions that use a simple and intuitive description of the task, such as \u201cClassify this movie review as positive or negative,\u201d or \u201cTranslate this sentence to Danish.\u201d \u201cCreating a dataset of instructions from scratch to fine-tune the model would take a considerable amount of resources. Therefore, we instead make use of templates to transform existing datasets into an instructional format.\u201d (source) The image below shows a representation of how the instruction dataset is generated via templates from the original FLAN paper:</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#how-did-we-evolve-between-different-techniques","title":"How did we evolve between different techniques ?","text":"<p>For most advanced readers who want to have a deeper understanding on the how did we evolve between all of these techniques, I recommend https://lightning.ai/pages/community/article/understanding-llama-adapters/</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/#how-to-choose-one-technique-over-the-other","title":"How to choose one technique over the other ?","text":"<p>https://docs.unsloth.ai/get-started/fine-tuning-guide/lora-hyperparameters-guide</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/","title":"Training Optimization techniques","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#model-parallelism-techniques","title":"Model parallelism techniques","text":"<p>When you are GPU poor, you must use as much as possible techniques to optimize your training. There is a very good and complete Ultra scale playbook that goes through these techniques and offer some nice overview. You can simulate the memory required for llama architecture families before and after optimization. This is the workload effort to train llama 8B before applying any optimization technique (using mixed precision):</p> <p></p> <p>And this is the effort using 3 data parallel streams and 2 tensors streams. Quite a significant change !</p> <p></p> <p>Obviously, this requires having significant compute and GPU to parallelize the workload. Here's a ery useful cheatsheet from their blog:</p> <p></p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#data-parallelism","title":"Data parallelism","text":"<p>We use several GPUs (we call the replicas \u201cmodel instances\u201d) and run forward and backward passes on different micro-batches of data in parallel on each GPU. Using a different micro-batch for each GPU means we\u2019ll have different gradients on each GPU, so to keep the model instances in sync across the different GPUs, we'll average the gradients from the model instances using an operation called \u201call-reduce.\u201d This operation takes place during the backward pass, before the optimizer step.</p> <p>This involves our first \u201cdistributed communication\u201d primitive, all-reduce, which handles the synchronization and communication between GPU instances and nodes.</p> <p>The feature distributedDataParallel is a built-in feature in PyTorch see (https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) is a way to test this.</p> <p>There are two main approaches to splitting: parallelism (tensor, context, or pipeline parallelism) and sharding (DeepSpeed ZeRO or PyTorch FSDP). Both approaches are somewhat orthogonal and can actually be combined!</p> <p>Deepspeed Zero methods has three possible optimization stages:</p> <ul> <li>ZeRO-1: optimizer state partitioning</li> <li>ZeRO-2: optimizer state + gradient partitioning</li> <li>ZeRO-3: optimizer state + gradient + parameter partitioning</li> </ul> <p>It significantly improves training. DP only works if a layer of the model fits in a single GPU, and ZeRO can only partition the parameters, gradients, and optimizer states, not the activation memory. This is a clear limitation that we want to overcome.</p> <p>Another core painpoint of this technique is that it requires heavy parameter communication.</p> <p>So it's time to explore tensor parallelim (TP) which proposes to shard parameters, gradients, optimizer states, AND activations across devices without requiring any communication of model parameters between GPUs.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#tensor-parallelism","title":"Tensor parallelism","text":"<p>Training on long input sequences requires huge amounts of GPU memory. This technique splits the processing of a single sequence across multiple GPUs. Each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single gpu, each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on horizontal level.</p> <p>Tensor parallelism basically splits matrices into rows and columns so calculation can be split across GPUs independently. Results of each sub calculation is either gathered or reduced to a final matrix.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#sequence-parallelism","title":"Sequence parallelism","text":"<p>Self-Attention, which is the key component of Transformers, suffers from quadratic memory requirements with respect to the sequence length, therefore when sequence length gets to a certain length, even a batch size of 1 might not be able to fit onto a single GPU and require additional partitioning along the sequence dimension. And once this is done, the sequence can be of any length. As this type of parallelism is orthogonal to the other parallelization types described in this document, it can be combined with any of them, leading to 4D, ZeRO-DP+SP and other combinations.</p> <p>Sequence parallelism (SP) involves splitting the activations and computations for the parts of the model not handled by tensor parallelism, such as dropout and LayerNorm, but along the input sequence dimension rather than the hidden dimension.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#context-parallelism","title":"Context parallelism","text":"<p>Context parallelism is the same thing as sequence parallelism as we split the input sequence (mandatory for some long context window models). However, we apply sequence parallelism in layers not covered by TP while context applies on layers with TP.</p> <p>Both sequence and context length address optimization for the length of your input. However, very frequently, this is the number of parameters that is the biggest bottleneck. Not enough ? Let's see Pipeline parallelism !</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#pipeline-parallelism","title":"Pipeline parallelism","text":"<p>Pipeline Parallelism (PP) is almost identical to a naive MP, but it solves the GPU idling problem, by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process. Essentially, you split model layers across GPUs.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#experts-parallelism","title":"Experts parallelism","text":"<p>When Mixture-Of-Experts (MoE) is used (in particular during inference) one could give each expert its own accelerator (or a few if one isn't enough), which is referred to as Expert Parallelism (EP). This adds another dimension for parallelization and can significantly speed things up for large batches that are likely to hit all of the experts. Instead of communicating model weights, in EP tokens are being communicated instead. EP leads to a more efficient compute as matrix multiplication then deal with bigger inputs.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#summary-of-5d-parralelism-techniques","title":"Summary of 5D parralelism techniques","text":"Method Memory savings apply specifically on... Parallel/sharding dimension Disadvantage DP Activations (reduce local batch size) Batch Limited by max batch size PP Model parameters Model layers Idle bubble and complex schedules TP+SP Model parameters and activations Hidden dimension/sequence length Requires high-bandwidth communication CP Activations Sequence length Adds communication overhead in attention modules EP Experts parameters Experts dimension Requires MoE layers, adds routing communication overhead ZeRO-1 Optimizer states Sharded among DP replicas Params communication overhead ZeRO-2 Optimizer states and gradients Sharded among DP replicas Params communication overhead ZeRO-3 Optimizer states, gradients, and model parameters Sharded among DP replicas Params communication overhead"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#other-optimization-techniques","title":"Other Optimization techniques","text":"<ul> <li>Gradient accumulation enables larger effective batch sizes</li> <li>Memory efficient optimizers</li> <li>Activation checkpointing trades computation for memory by recalculating certain activations</li> </ul> <p>https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf https://github.com/stas00/ml-engineering/tree/master/training</p> <ul> <li>Model checkpoints</li> <li>Federated learning</li> <li>Data parralelism (fully sharded data parallelism)</li> <li>Model parralelism</li> <li>distributed training</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#frameworks-to-accelerate","title":"Frameworks to accelerate","text":"<ul> <li>Pytorch: it has a lot of built-in features that one can use directly</li> <li>accelerators: useful for optimizer?</li> <li>unsloth:</li> <li>nanotron (https://github.com/huggingface/nanotron/tree/main):</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#example-of-training-process-including-data-refinement","title":"Example of training process including data refinement","text":"<p>https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1 https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/ https://magazine.sebastianraschka.com/p/instruction-pretraining-llms</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/#sources","title":"Sources","text":"<p>Picotron: for educational purposes, you can train a llama model using parralelism techniques: https://github.com/huggingface/picotron</p> <p>Sharding definition ?  FSDP ??</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/","title":"Model Merging","text":"<p>Model merging It is another method than simultaneous and sequential multitask fine-tuning. It fine tunes the model on different tasks separately but in parallel and then merge the different models together It is also one way to do federated learning Different than ensemble method as it mixes some parameters for constituents models together. A few methods (experimentals):</p> <ul> <li>summing: linear combination of model weights usually. One can also create \u00ab\u00a0task vectors\u00a0\u00bb (from same model base, extract vectors for a specific task giving idiosyncrasies). If not the same base model, projections can be used to sum 2 different combinations</li> <li>Layer stacking: stack layers from different models and requires final fine-tuning once merged. It can be used for MixtureOfExperts; It can also be used to run depth wise upscaling (increase the size of the model)</li> <li>Concatenation:</li> </ul> <p>https://magazine.sebastianraschka.com/p/research-papers-in-january-2024?open=false#%C2%A7understanding-model-merging-and-weight-averaging</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#llm-architectures","title":"LLM architectures","text":"<p>Transformers etc https://poloclub.github.io/transformer-explainer/</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#mixture-of-experts","title":"Mixture of Experts","text":"<p>https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#what-are-pre-trained-or-foundation-models","title":"What are pre-trained or foundation models ?","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#what-is-pre-training","title":"What is pre-training ?","text":"<p>Data preparation: Pre-training requires massive datasets (e.g., Llama 3.1 was trained on 15 trillion tokens) that need careful curation, cleaning, deduplication, and tokenization. Modern pre-training pipelines implement sophisticated filtering to remove low-quality or problematic content. Distributed training: Combine different parallelization strategies: data parallel (batch distribution), pipeline parallel (layer distribution), and tensor parallel (operation splitting). These strategies require optimized network communication and memory management across GPU clusters. Training optimization: Use adaptive learning rates with warm-up, gradient clipping and normalization to prevent explosions, mixed-precision training for memory efficiency, and modern optimizers (AdamW, Lion) with tuned hyperparameters. Monitoring: Track key metrics (loss, gradients, GPU stats) using dashboards, implement targeted logging for distributed training issues, and set up performance profiling to identify bottlenecks in computation and communication across devices.</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#what-is-continued-pre-training","title":"What is continued pre-training ?","text":"<p>Continued Pre-Training refers to the cost effective alternative to pre-training. In this process, we further train a base pre-trained LLM on a large corpus of domain-specific text documents. This augments the model\u2019s general knowledge with specific information from the particular domain. Like pre-training it is performed in a self supervised manner (i.e. no labels) using for example some texts for a particular subject.</p> <p>There are huge benefits because you can have totally new dataset but it is at risk of catastrophic forgetting (the fact that previous knowledge would be forgotten).</p>"},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#what-is-post-training","title":"What is post-training ?","text":""},{"location":"GenAI%20Engineering/Chapter%203%20-%20Fine%20Tuning%20techniques/Training/#post-training-datasets","title":"Post training datasets","text":"<p>Storage &amp; chat templates: Because of the conversational structure, post-training datasets are stored in a specific format like ShareGPT or OpenAI/HF. Then, these formats are mapped to a chat template like ChatML or Alpaca to produce the final samples the model is trained on. Synthetic data generation: Create instruction-response pairs based on seed data using frontier models like GPT-4o. This approach allows for flexible and scalable dataset creation with high-quality answers. Key considerations include designing diverse seed tasks and effective system prompts. Data enhancement: Enhance existing samples using techniques like verified outputs (using unit tests or solvers), multiple answers with rejection sampling, Auto-Evol, Chain-of-Thought, Branch-Solve-Merge, personas, etc. Quality filtering: Traditional techniques involve rule-based filtering, removing duplicates or near-duplicates (with MinHash or embeddings), and n-gram decontamination. Reward models and judge LLMs complement this step with fine-grained and customizable quality control.</p> <p>https://github.com/mlabonne/llm-datasets https://github.com/NVIDIA/NeMo-Curator https://distilabel.argilla.io/dev/sections/pipeline_samples/</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/","title":"RAG vs SFT ?","text":"<ul> <li>advantage of RAG is it cannot hallucinate</li> <li>Advantage of fine tuning is you chan twick the behavior , bias etc</li> <li>Obvious is you need a minimum of good quality data</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/#main-type-of-rag-architectures","title":"Main type of RAG architectures","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/#classical-rag","title":"Classical RAG","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/#graph-rag","title":"Graph RAG","text":"<p>https://weaviate.io/blog/graph-rag</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/#agentic-rag","title":"Agentic RAG","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/#rag-frameworks","title":"Rag Frameworks","text":"<p>https://github.com/KalyanKS-NLP/rag-zero-to-hero-guide?tab=readme-ov-file</p> <p>RAG overview:</p> <p>docs/img/rag_pipeline.gif</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/","title":"RAG Optimization techniques","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#parser-optimization-techniques","title":"Parser optimization techniques","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#pre-retrieval-work","title":"Pre-retrieval work","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#metadata","title":"Metadata","text":"<p>Metadata is probably one of the most underrated element in improving a RAG system. It consists in adding metadata and index those for your documents. It is important because it brings context on:</p> <ul> <li>dates, time, version of your document that can be confusing for your LLM</li> <li>outer context: team origination, objectives, business logic</li> <li>summarizing the key elements of the document with specific keywords to have a faster multi-step retriever (using filtering)</li> <li>etc...</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#chunking-strategies","title":"Chunking strategies","text":"<p>See docs/Chapter 6 - LLM based solutions/Chunking strategies.md for details on chunking.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#fine-tune-embedding-models","title":"Fine tune embedding models","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#during-retriever-phase","title":"During retriever phase","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#query-routing","title":"Query routing","text":"<p>Routing a query gives the option to have multiple options to handle this query. You can have multiple LLMs with each specifically suited for a request type, you can have different tooling or wish for having deterministic answer in some cases. The simplest example is also a best practice: you need to understand if the user query is within the context of your solution. If not, you may want to answer a specified text or redirect to specific content. This can be done within a system prompt or an expert LLM.</p> <p>In a more complex world, you can have an AI agents that can efficiently route your query reasoning on the best option. This might significantly increase your latency.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#query-re-writing","title":"Query re-writing","text":"<p>Although LLMs \"understand\" NLP, very often user query is not written in an ideal way for an LLM. It can miss some context, have relevant typo issues leading to semantic errors. Query rewriting transforms the original user query into a more effective version for information retrieval. Instead of just doing retrieve-then-read, applications now do a rewrite-retrieve-read approach. This technique restructures oddly written questions so they can be better understood by the system, removes irrelevant context, introduces common keywords that improve matching with correct context, and can split complex questions into simpler sub-questions. Concretely, this step restructure unclear questions, remove useless or inefficient context and introduces common terminology that can increase the likelihood of matching documents.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#query-expansion","title":"Query expansion","text":"<p>While query rewriting, enables more semantic meaning between the user and the LLM/embedding, expansion generates multiple queries from a single input. It can significantly enhance the retriever phase by expanding the context or diversify the search to neighbouring meaning. It's helpful when the document informations are dense and complex.</p> <p>However, it may bring latency overhead if used intensively.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#query-decomposition","title":"Query decomposition","text":"<p>Sometimes the user query has multiple sub-queries in it or is quite complex to dig into the VectorDB directly. That's where query decomposition comes in: it breaks the original query into simpler ones that may require multiple sources.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#metadata-filtering","title":"Metadata filtering","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#hybrid-search","title":"Hybrid search","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#graph-dbs","title":"Graph DBs","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/#post-retriever","title":"Post retriever","text":"<ul> <li>Use prompt compression techniques.</li> <li>Filter out irrelevant chunks to avoid adding noise to the augmented prompt</li> <li>Use re-ranker</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.3%20Chunking%20strategies/","title":"Chunking strategies","text":"<p>Having the relevant chunking imports a lot in the outcome of your solution. If you cut too early, you're missing some context but if you take too much information then you might lose some specific semantic or increase the latnecy of your solution.</p> <p>But do you even need chunking ? With LLMs having more and more context, it really depends on the size of your document(s). If yes, it's desirable to start with a deterministic or simple strategy to stabilize other elements of the solution first. In the reality, some solution require hierarchical or multi-steps chunking/processing because of the complexity of the document.</p> <p>If you process research papers, they have indeed the same structure even though they might have very specific components. But in a global enterprise, you can have in your database a wide variety of documents to handle.</p> <ul> <li>\ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01 \ud835\udde6\ud835\ude01\ud835\uddff\ud835\ude02\ud835\uddf0\ud835\ude01\ud835\ude02\ud835\uddff\ud835\uddf2: Are you working with highly structured content (code, JSON, Markdown) or unstructured narrative text? Structure-aware chunking preserves logical organization.</li> <li>\ud835\uddd7\ud835\uddf2\ud835\ude01\ud835\uddee\ud835\uddf6\ud835\uddf9 \ud835\udddf\ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddf9: Do you need to retrieve specific, granular facts or broader conceptual summaries? This determines whether you need smaller, focused chunks or larger, context-rich ones.</li> <li>\ud835\udde4\ud835\ude02\ud835\uddf2\ud835\uddff\ud835\ude06 \ud835\uddd6\ud835\uddfc\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2\ud835\ude05\ud835\uddf6\ud835\ude01\ud835\ude06: Simple questions benefit from targeted chunks. Complex queries often need more surrounding context to generate accurate responses.</li> <li>\ud835\udde3\ud835\uddf2\ud835\uddff\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\ude03\ud835\ude00. \ud835\udde4\ud835\ude02\ud835\uddee\ud835\uddf9\ud835\uddf6\ud835\ude01\ud835\ude06: Simple methods like fixed-size chunking are fast and easy to implement. Advanced techniques like semantic or LLM-based chunking deliver better quality but require more compute.</li> </ul> <p>Pro tips: Learn how to feed an LLM the right information at the right time, connecting it to external data, live tools, and memory.</p> <p></p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/","title":"Vector Similarity Search Methods","text":"<p>https://www.pinecone.io/learn/series/faiss/hnsw/</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#major-categories-of-vector-similarity-search-methods","title":"Major Categories of Vector Similarity Search methods","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#1-graph-based-methods-current-state-of-the-art","title":"1. Graph-Based Methods (Current State-of-the-Art)","text":"<p>HNSW (Hierarchical Navigable Small World)</p> <ul> <li>Strengths: Super fast search speeds and fantastic recall, excellent for high-dimensional data</li> <li>Best for: In-memory applications with moderate dataset sizes (&lt;50M vectors), real-time search requirements</li> <li>Weaknesses: High memory consumption, expensive index construction</li> <li>Performance: Typically achieves &gt;95% recall with sub-millisecond query times</li> </ul> <p>DiskANN/Vamana</p> <ul> <li>Strengths: Designed to handle vast datasets efficiently by operating primarily on disk</li> <li>Best for: Billion-scale datasets, memory-constrained environments</li> <li>Weaknesses: Construction is much slower than HNSW, limited update support</li> <li>Performance: Can handle billion-point datasets with good recall-QPS trade-offs</li> </ul> <p>NSW (Navigable Small World)</p> <ul> <li>Strengths: Simpler than HNSW, good theoretical foundations</li> <li>Best for: Medium-scale applications where simplicity matters</li> <li>Weaknesses: Generally outperformed by HNSW in practice</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#2-quantization-based-methods","title":"2. Quantization-Based Methods","text":"<p>Product Quantization (PQ)</p> <ul> <li>Strengths: Excellent memory efficiency, good for high-dimensional vectors</li> <li>Best for: Large datasets with memory constraints, batch processing</li> <li>Weaknesses: Lower accuracy than graph methods, requires careful parameter tuning</li> </ul> <p>ScaNN (Scalable Nearest Neighbors)</p> <ul> <li>Strengths: Google's optimized quantization with learned quantization</li> <li>Best for: Large-scale production systems, batch queries</li> <li>Weaknesses: Complex implementation, less flexible than graph methods</li> </ul> <p>Optimized Product Quantization (OPQ)</p> <ul> <li>Strengths: Better accuracy than vanilla PQ through rotation optimization</li> <li>Best for: Similar to PQ but when higher accuracy is needed</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#3-tree-based-methods","title":"3. Tree-Based Methods","text":"<p>Annoy (Approximate Nearest Neighbors Oh Yeah)</p> <ul> <li>Strengths: Simple, deterministic builds, good for static datasets</li> <li>Best for: Recommendation systems, small to medium datasets</li> <li>Weaknesses: Don't seem to have a sweet spot just yet, generally outperformed by modern methods</li> </ul> <p>Random Projection Trees</p> <ul> <li>Strengths: Good theoretical guarantees, works well in moderate dimensions</li> <li>Best for: Medium-dimensional data, when interpretability matters</li> <li>Weaknesses: Performance degrades in very high dimensions</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#4-hash-based-methods","title":"4. Hash-Based Methods","text":"<p>Locality Sensitive Hashing (LSH)</p> <ul> <li>Strengths: Theoretical guarantees, simple implementation</li> <li>Best for: Streaming data, distributed systems, sparse vectors</li> <li>Weaknesses: Parameter tuning complexity, often outperformed by modern methods</li> </ul> <p>Learning to Hash</p> <ul> <li>Strengths: Data-adaptive, can learn optimal hash functions</li> <li>Best for: Specific domains with training data available</li> <li>Weaknesses: Requires training phase, domain-specific</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#5-hybrid-and-emerging-methods","title":"5. Hybrid and Emerging Methods","text":"<p>HNSW-IF (Inverted File + HNSW)</p> <ul> <li>Strengths: Takes advantage of the speed/recall of HNSW in combination with the disk scalability of inverted indices</li> <li>Best for: Large-scale applications needing both speed and memory efficiency</li> <li>Implementation: Available in systems like Vespa</li> </ul> <p>GPU-Accelerated Methods</p> <ul> <li>Examples: BANG, GPU-DiskANN, RAPIDS cuVS</li> <li>Strengths: Leveraging massive parallelism for billion-scale search</li> <li>Best for: High-throughput applications with GPU resources available</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#choosing-the-right-algorithm","title":"Choosing the Right Algorithm","text":"<p>For Real-time Applications (&lt;1ms latency):</p> <ul> <li>HNSW for datasets up to 50M vectors</li> <li>GPU-accelerated methods for larger datasets with available hardware</li> </ul> <p>For Large-Scale Applications (&gt;100M vectors):</p> <ul> <li>DiskANN for disk-based storage with high recall requirements</li> <li>Quantization methods (ScaNN, PQ) for memory-efficient solutions</li> </ul> <p>For Memory-Constrained Environments:</p> <ul> <li>Product Quantization variants</li> <li>DiskANN for disk-based approach</li> <li>LSH for distributed scenarios</li> </ul> <p>For High-Dimensional Data (&gt;1000 dimensions):</p> <ul> <li>HNSW generally performs best</li> <li>Learned quantization methods like ScaNN</li> <li>Avoid tree-based methods</li> </ul> <p>For Dynamic Datasets (frequent updates):</p> <ul> <li>HNSW supports incremental updates well</li> <li>LSH-based methods for streaming scenarios</li> <li>Avoid DiskANN due to poor update support</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#current-state-of-the-art-2024-2025","title":"Current State-of-the-Art (2024-2025)","text":"<p>The field is converging on graph-based methods as the dominant approach, with HNSW being the most widely adopted. HNSW following Milvus' Knowhere and Zilliz Cloud's performance is very competitive compared to other ANN methods. However, for specific use cases:</p> <ul> <li>Billion-scale: DiskANN and GPU-accelerated variants</li> <li>Production systems: Hybrid approaches like HNSW-IF</li> <li>Edge devices: Lightweight quantization methods</li> <li>Streaming data: Advanced LSH variants</li> </ul> <p>The trend is toward hybrid methods that combine the strengths of different approaches, such as using quantization for memory efficiency while maintaining graph connectivity for search performance.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/#resources","title":"Resources","text":"<p>https://www.pinecone.io/learn/series/faiss/hnsw/</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/","title":"Retriever strategies in Vector Database Retrieval","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#overview","title":"Overview","text":"<p>When building LLM applications that query vector databases, understanding the difference between Bi-Encoders and Cross-Encoders is crucial for optimizing both performance and accuracy.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#bi-encoders","title":"Bi-Encoders","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#what-are-bi-encoders","title":"What are Bi-Encoders?","text":"<p>Bi-Encoders process sentences independently to produce fixed-size vector embeddings. Each sentence is encoded separately, allowing for efficient similarity comparisons using cosine similarity or dot product.  Architecture:</p> <ul> <li>Query and documents are encoded independently</li> <li>Produces sentence embeddings (vectors) that can be stored and indexed</li> <li>Similarity calculated using cosine similarity between embeddings</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#how-bi-encoders-work-in-vector-databases","title":"How Bi-Encoders Work in Vector Databases","text":"<pre><code>from sentence_transformers import SentenceTransformer\n\n# Load bi-encoder model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Encode documents (done once during indexing)\ndocuments = [\"The cat sat on the mat\", \"Dogs are loyal animals\", \"Python is a programming language\"]\ndoc_embeddings = model.encode(documents)\n\n# Encode user query\nquery = \"What animals are mentioned?\"\nquery_embedding = model.encode([query])\n\n# Find most similar documents using cosine similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\nsimilarities = cosine_similarity(query_embedding, doc_embeddings)\n</code></pre> <p>Advantages:</p> <ul> <li>Fast retrieval (pre-computed embeddings)</li> <li>Scalable to millions of documents</li> <li>Efficient vector database operations</li> </ul> <p>Disadvantages:</p> <ul> <li>Limited context interaction between query and document</li> <li>Lower accuracy compared to cross-encoders</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#cross-encoders","title":"Cross-Encoders","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#what-are-cross-encoders","title":"What are Cross-Encoders?","text":"<p>Cross-Encoders process query-document pairs simultaneously, allowing for rich interaction between the two texts. They produce a single relevance score (0-1) rather than separate embeddings.</p> <p>Architecture:</p> <ul> <li>Query and document are concatenated and processed together</li> <li>No individual sentence embeddings produced</li> <li>Direct similarity score output</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#cross-encoder-implementation","title":"Cross-Encoder Implementation","text":"<pre><code>from sentence_transformers.cross_encoder import CrossEncoder\n\n# Load cross-encoder model\nmodel = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\n# Score query against multiple documents\nquery = \"What animals are mentioned?\"\ndocuments = [\"The cat sat on the mat\", \"Dogs are loyal animals\", \"Python is a programming language\"]\n\n# Create query-document pairs\npairs = [[query, doc] for doc in documents]\n\n# Get relevance scores\nscores = model.predict(pairs)\nprint(scores)  # [0.8, 0.9, 0.1] - higher scores = more relevant\n</code></pre> <p>Advantages:</p> <ul> <li>Higher accuracy due to query-document interaction</li> <li>Better understanding of semantic relationships</li> <li>Superior performance on ranking tasks</li> </ul> <p>Disadvantages:</p> <ul> <li>Computationally expensive (O(Q\u00d7D) comparisons)</li> <li>Cannot pre-compute embeddings</li> <li>Not suitable for large-scale initial retrieval</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#combining-bi-encoders-and-cross-encoders-the-re-ranking-pipeline","title":"Combining Bi-Encoders and Cross-Encoders: The Re-ranking Pipeline","text":"<p>The optimal approach combines both architectures in a two-stage pipeline:</p> <ul> <li>Use bi-encoder for fast, scalable retrieval of top-k candidates from vector database.</li> <li>Use cross-encoder to re-rank the top candidates for maximum accuracy.</li> </ul> <p></p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom sentence_transformers.cross_encoder import CrossEncoder\nimport numpy as np\n\n# Initialize models\nbi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\ncross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n\ndef retrieve_and_rerank(query, documents, top_k=100, rerank_top_k=10):\n    \"\"\"\n    Two-stage retrieval and re-ranking pipeline\n    \"\"\"\n\n    # Stage 1: Bi-encoder retrieval\n    query_embedding = bi_encoder.encode([query])\n    doc_embeddings = bi_encoder.encode(documents)\n\n    # Calculate similarities and get top-k\n    similarities = np.dot(query_embedding, doc_embeddings.T)[0]\n    top_indices = np.argsort(similarities)[::-1][:top_k]\n\n    # Stage 2: Cross-encoder re-ranking\n    candidate_docs = [documents[i] for i in top_indices]\n    pairs = [[query, doc] for doc in candidate_docs]\n\n    # Get cross-encoder scores\n    cross_scores = cross_encoder.predict(pairs)\n\n    # Re-rank based on cross-encoder scores\n    reranked_indices = np.argsort(cross_scores)[::-1][:rerank_top_k]\n\n    # Return final results\n    final_results = []\n    for idx in reranked_indices:\n        original_idx = top_indices[idx]\n        final_results.append({\n            'document': documents[original_idx],\n            'bi_encoder_score': similarities[original_idx],\n            'cross_encoder_score': cross_scores[idx],\n            'original_index': original_idx\n        })\n\n    return final_results\n\n# Example usage\nquery = \"How do machine learning models work?\"\ndocuments = [\n    \"Machine learning models learn patterns from data\",\n    \"Neural networks are a type of ML model\",\n    \"Cats are popular pets\",\n    \"Deep learning uses multiple layers\",\n    \"Python is used for ML development\"\n]\n\nresults = retrieve_and_rerank(query, documents, top_k=3, rerank_top_k=2)\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#re-ranker-in-the-context-of-llm-applications","title":"Re-ranker in the Context of LLM Applications","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#what-is-a-re-ranker","title":"What is a Re-ranker?","text":"<p>A re-ranker is typically a cross-encoder model specifically trained for improving the relevance ranking of retrieved documents. In LLM applications, re-rankers serve as the bridge between initial retrieval and final answer generation.</p>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#re-ranking-pipeline-in-rag-systems","title":"Re-ranking Pipeline in RAG Systems","text":"<pre><code>def rag_with_reranking(user_query, vector_db, llm_model):\n    \"\"\"\n    Complete RAG pipeline with re-ranking\n    \"\"\"\n\n    # 1. Initial retrieval from vector database (bi-encoder)\n    initial_results = vector_db.similarity_search(user_query, k=50)\n\n    # 2. Re-rank with cross-encoder\n    reranked_results = rerank_documents(user_query, initial_results, top_k=5)\n\n    # 3. Prepare context for LLM\n    context = \"\\n\".join([doc.content for doc in reranked_results])\n\n    # 4. Generate answer with LLM\n    prompt = f\"Context: {context}\\n\\nQuestion: {user_query}\\n\\nAnswer:\"\n    answer = llm_model.generate(prompt)\n\n    return answer, reranked_results\n</code></pre>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#performance-comparison","title":"Performance Comparison","text":"Aspect Bi-Encoder Cross-Encoder Combined Pipeline Speed Very Fast Slow Fast (optimized) Accuracy Good Excellent Excellent Scalability High Low High Memory Usage Low High Moderate Use Case Initial retrieval Final ranking Production RAG"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#best-practices","title":"Best Practices","text":""},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#when-to-use-each-approach","title":"When to Use Each Approach","text":"<p>Bi-Encoder Only:</p> <ul> <li>Large-scale similarity search</li> <li>Real-time applications requiring low latency</li> <li>Limited computational resources</li> </ul> <p>Cross-Encoder Only:</p> <ul> <li>Small document collections (&lt; 1000 docs)</li> <li>Offline processing where accuracy is paramount</li> <li>Fine-grained relevance scoring</li> </ul> <p>Combined Pipeline (Recommended):</p> <ul> <li>Production RAG systems</li> <li>Large document collections with accuracy requirements</li> <li>Applications with moderate latency tolerance</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#implementation-tips","title":"Implementation Tips","text":"<ol> <li> <p>Optimize Top-K Values: Balance between recall (bi-encoder top-k) and computational cost (cross-encoder re-ranking)</p> </li> <li> <p>Model Selection: Choose domain-specific models when available (e.g., <code>ms-marco</code> for passage retrieval)</p> </li> <li> <p>Caching: Cache bi-encoder embeddings but compute cross-encoder scores dynamically</p> </li> <li> <p>Batch Processing: Process cross-encoder pairs in batches for efficiency</p> </li> </ol>"},{"location":"GenAI%20Engineering/Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/#late-interaction-retrieval-mode","title":"Late interaction retrieval mode","text":"<p>https://weaviate.io/blog/late-interaction-overview</p>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/","title":"Comprehensive RAG System Evaluation Framework","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#1-classic-nlp-metrics","title":"1. Classic NLP Metrics","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#11-token-level-metrics","title":"1.1 Token-Level Metrics","text":"<p>BLEU (Bilingual Evaluation Understudy)</p> <ul> <li>Measures n-gram overlap between generated and reference text</li> <li>Formula: BLEU = BP \u00d7 exp(\u03a3(wn \u00d7 log(pn)))</li> <li>Example: For a medical RAG system, BLEU-4 score of 0.65 indicates good lexical overlap with reference medical answers</li> <li>Limitations: Doesn't capture semantic meaning, sensitive to word order</li> </ul> <p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</p> <ul> <li>ROUGE-1: Unigram overlap</li> <li>ROUGE-2: Bigram overlap  </li> <li>ROUGE-L: Longest common subsequence</li> <li>Example: Legal document RAG achieving ROUGE-L of 0.72 shows good structural similarity</li> <li>Best for: Summarization tasks within RAG</li> </ul> <p>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</p> <ul> <li>Considers synonyms, stemming, and word order</li> <li>Formula: METEOR = (1-penalty) \u00d7 harmonic_mean(precision, recall)</li> <li>Example: Technical documentation RAG with METEOR of 0.58 captures semantic similarity better than BLEU</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#12-semantic-similarity-metrics","title":"1.2 Semantic Similarity Metrics","text":"<p>BERTScore</p> <ul> <li>Uses pre-trained BERT embeddings for semantic similarity</li> <li>Provides precision, recall, and F1 scores</li> <li>Example: Customer support RAG with BERTScore F1 of 0.85 indicates strong semantic alignment</li> <li>Advantage: Captures contextual meaning beyond surface-level similarity</li> </ul> <p>Sentence-BERT Cosine Similarity</p> <ul> <li>Measures semantic similarity between sentence embeddings</li> <li>Range: -1 to 1 (higher = more similar)</li> <li>Example: Educational RAG achieving 0.78 cosine similarity suggests good conceptual alignment</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#2-rag-specific-performance-metrics","title":"2. RAG-Specific Performance Metrics","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#21-retrieval-component-metrics","title":"2.1 Retrieval Component Metrics","text":"<p>Hit Rate (Recall@k)</p> <ul> <li>Percentage of queries where at least one relevant document appears in top-k results</li> <li>Formula: Hit Rate@k = (# queries with \u22651 relevant doc in top-k) / (total queries)</li> <li>Example: Legal RAG with Hit Rate@5 = 0.92 means 92% of queries retrieve relevant legal precedents</li> <li>Target: &gt;90% for most applications</li> </ul> <p>Mean Reciprocal Rank (MRR)</p> <ul> <li>Average of reciprocal ranks of first relevant document</li> <li>Formula: MRR = (1/|Q|) \u00d7 \u03a3(1/rank_i)</li> <li>Example: E-commerce RAG with MRR = 0.78 indicates first relevant product typically appears at rank 1.28</li> <li>Interpretation: Higher values indicate relevant documents appear earlier</li> </ul> <p>Normalized Discounted Cumulative Gain (NDCG@k)</p> <ul> <li>Accounts for relevance grades and position bias</li> <li>Formula: NDCG@k = DCG@k / IDCG@k</li> <li>Example: Research paper RAG with NDCG@10 = 0.85 shows good ranking quality</li> <li>Advantage: Considers graded relevance (not just binary)</li> </ul> <p>Precision@k and Recall@k</p> <ul> <li>Precision@k: Fraction of top-k documents that are relevant</li> <li>Recall@k: Fraction of relevant documents in top-k</li> <li>Example: Medical RAG with Precision@3 = 0.89 means 89% of top-3 results are medically relevant</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#22-generation-component-metrics","title":"2.2 Generation Component Metrics","text":"<p>Faithfulness/Groundedness</p> <ul> <li>Measures how well generated answers are supported by retrieved context</li> <li>Methods: NLI-based models, fact-checking algorithms</li> <li>Example: News RAG with faithfulness score 0.91 means 91% of claims are supported by sources</li> <li>Critical for: Preventing hallucinations</li> </ul> <p>Answer Relevance</p> <ul> <li>Evaluates how well the answer addresses the original question</li> <li>Can use semantic similarity between question and answer</li> <li>Example: HR policy RAG with relevance score 0.88 indicates answers stay on-topic</li> </ul> <p>Context Utilization</p> <ul> <li>Measures how effectively the model uses retrieved context</li> <li>Formula: Overlap between answer and context / Total context length</li> <li>Example: Technical manual RAG with 65% context utilization suggests efficient information extraction</li> </ul> <p>Completeness</p> <ul> <li>Evaluates whether the answer covers all aspects of the question</li> <li>Often requires human evaluation or structured templates</li> <li>Example: Multi-part questions receiving completeness score of 0.82</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#23-end-to-end-rag-metrics","title":"2.3 End-to-End RAG Metrics","text":"<p>Answer Correctness</p> <ul> <li>Overall accuracy of the final answer</li> <li>Combines factual correctness and semantic similarity</li> <li>Example: Quiz-based evaluation showing 78% correctness for educational RAG</li> </ul> <p>Answer Similarity</p> <ul> <li>Semantic similarity between generated and ground truth answers</li> <li>Uses embeddings-based metrics (cosine similarity, BERTScore)</li> <li>Example: Customer service RAG achieving 0.81 answer similarity</li> </ul> <p>Context Precision</p> <ul> <li>Fraction of retrieved context that is actually relevant to the question</li> <li>Formula: Relevant context chunks / Total retrieved chunks</li> <li>Example: Legal research RAG with context precision of 0.73</li> </ul> <p>Context Recall</p> <ul> <li>Fraction of relevant context that was successfully retrieved</li> <li>Formula: Retrieved relevant chunks / Total relevant chunks in corpus</li> <li>Example: Scientific literature RAG with context recall of 0.85</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#3-system-performance-metrics-for-llm-and-rag-solution","title":"3. System Performance Metrics for LLM and RAG Solution","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#31-latency-metrics","title":"3.1 Latency Metrics","text":"<p>End-to-End Response Time</p> <ul> <li>Total time from query to final answer</li> <li>Components: Retrieval time + Generation time + Processing overhead</li> <li>Example: Customer support RAG targeting &lt;2 seconds for 95th percentile</li> <li>Benchmark: Consumer applications typically need &lt;1 second, enterprise can tolerate 2-5 seconds</li> </ul> <p>Retrieval Latency</p> <ul> <li>Time to search and retrieve relevant documents</li> <li>Factors: Index size, query complexity, hardware specifications</li> <li>Example: 10M document corpus with 150ms average retrieval time</li> <li>Optimization: Vector databases, caching, index optimization</li> </ul> <p>Generation Latency</p> <ul> <li>Time for LLM to generate response given context</li> <li>Measured in tokens/second or time to first token</li> <li>Example: 7B parameter model generating 45 tokens/second</li> <li>Variables: Model size, context length, hardware acceleration</li> </ul> <p>Time to First Token (TTFT)</p> <ul> <li>Latency before first token is generated</li> <li>Critical for streaming applications</li> <li>Example: Chatbot with TTFT of 200ms for better user experience</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#32-throughput-metrics","title":"3.2 Throughput Metrics","text":"<p>Queries Per Second (QPS)</p> <ul> <li>Number of queries the system can handle per second</li> <li>Example: Production RAG system handling 100 QPS during peak hours</li> <li>Factors: Model size, batch processing, parallel processing</li> </ul> <p>Concurrent Users</p> <ul> <li>Maximum simultaneous users the system can support</li> <li>Example: Enterprise RAG supporting 500 concurrent users</li> <li>Measurement: Load testing with realistic usage patterns</li> </ul> <p>Tokens Per Second (Generation)</p> <ul> <li>Rate of token generation during inference</li> <li>Example: Optimized RAG generating 60 tokens/second per user</li> <li>Important for: Long-form content generation</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#33-resource-utilization-metrics","title":"3.3 Resource Utilization Metrics","text":"<p>Memory Usage</p> <ul> <li>RAM consumption during inference</li> <li>Components: Model weights, context caching, vector indices</li> <li>Example: 7B parameter RAG using 16GB RAM with 4-bit quantization</li> <li>Monitoring: Peak memory, memory leaks, garbage collection</li> </ul> <p>GPU Utilization</p> <ul> <li>Percentage of GPU compute being used</li> <li>Target: 70-90% for optimal efficiency</li> <li>Example: Multi-GPU setup with 85% average utilization</li> <li>Optimization: Batch processing, model parallelism</li> </ul> <p>CPU Usage</p> <ul> <li>Processor utilization for non-GPU tasks</li> <li>Includes: Text processing, vector operations, I/O operations</li> <li>Example: Retrieval-heavy RAG using 60% CPU during peak</li> </ul> <p>Storage I/O</p> <ul> <li>Disk read/write operations for document retrieval</li> <li>Metrics: IOPS, throughput, latency</li> <li>Example: Document database with 10K IOPS capability</li> <li>Optimization: SSD storage, caching strategies</li> </ul> <p>Perplexity: Language model confidence Semantic Similarity: Embedding-based similarity Exact Match: String matching</p>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#4-bias-and-safety-evaluation","title":"4. Bias and Safety Evaluation","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#bias-detection","title":"Bias Detection","text":"<ul> <li>Bias Metric: Comprehensive bias evaluation across protected attributes</li> <li>Gender Bias: Specific gender-related bias detection</li> <li>Political Bias: Political leaning detection</li> <li>Racial Bias: Racial discrimination detection</li> <li>Religious Bias: Religious prejudice detection</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#safety-toxicity","title":"Safety &amp; Toxicity","text":"<ul> <li>Toxicity: Harmful content detection</li> <li>Hate Speech: Hate speech identification</li> <li>Violence: Violence-related content detection</li> <li>Self-Harm: Self-harm content identification</li> <li>Sexual Content: Inappropriate sexual content detection</li> <li>Misinformation: False information detection</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/#responsible-ai","title":"Responsible AI","text":"<ul> <li>Fairness: Equal treatment across groups</li> <li>Transparency: Explainability of decisions</li> <li>Privacy: PII leakage detection</li> <li>Robustness: Adversarial attack resistance</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/","title":"\"Accuracy is Not All You Need\" - One-Page Summary","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/#problem-statement","title":"Problem Statement","text":"<p>When Large Language Models (LLMs) are compressed using techniques such as quantization, the predominant evaluation method relies on measuring accuracy across various benchmarks. If the compressed model's accuracy remains close to the baseline model, researchers assume negligible quality degradation. However, this paper challenges this assumption, arguing that accuracy alone is insufficient for comprehensive model evaluation, particularly for compressed LLMs.</p>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/#key-arguments","title":"Key Arguments","text":""},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/#1-limitations-of-accuracy-only-evaluation","title":"1. Limitations of Accuracy-Only Evaluation","text":"<ul> <li>Even when accuracy metrics remain stable post-compression, significant behavioral changes in model outputs may occur</li> <li>Traditional accuracy metrics fail to capture nuanced differences in model behavior, especially for tasks involving free-form text generation</li> <li>Models can maintain high accuracy while exhibiting different response patterns, confidence levels, and output distributions</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/#2-proposed-alternative-metrics","title":"2. Proposed Alternative Metrics","text":"<p>KL-Divergence:</p> <ul> <li>Measures the difference between probability distributions of the original and compressed models</li> <li>Captures changes in model confidence and output probability distributions</li> <li>Provides insight into how compression affects the model's internal decision-making process</li> </ul> <p>Flips Metric:</p> <ul> <li>Novel metric that tracks when model predictions change from correct to incorrect (and vice versa) after compression</li> <li>Reveals instances where models change their answers even when overall accuracy remains high</li> <li>Particularly valuable for understanding model reliability and consistency</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/#3-correlation-analysis","title":"3. Correlation Analysis","text":"<p>The paper demonstrates that KL-Divergence and flips metrics are well-correlated, suggesting they capture similar underlying changes in model behavior that accuracy metrics miss.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/","title":"Generative modelling beyond sequences","text":"<p>So far, everything we\u2019ve looked has been focused on text and sequence prediction with language models, but many other \u201cgenerative AI\u201d techniques require learning distributions with less of a sequential structure (e.g. images). Here we\u2019ll examine a number of non-Transformer architectures for generative modeling, starting from simple mixture models and culminating with diffusion.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/#distribution-modeling","title":"Distribution Modeling","text":"<p>Recalling our first glimpse of language models as simple bigram distributions, the most basic thing you can do in distributional modeling is just count co-occurrence probabilities in your dataset and repeat them as ground truth. This idea can be extended to conditional sampling or classification as \u201cNaive Bayes\u201d (blog post video), often one of the simplest algorithms covered in introductory machine learning courses.</p> <p>The next generative model students are often taught is the Gaussian Mixture Model and its Expectation-Maximization algorithm; Gaussian Mixture Models + Expectation-Maximization algorithm. This blog post and this video give decent overviews; the core idea here is assuming that data distributions can be approximated as a mixture of multivariate Gaussian distributions. GMMs can also be used for clustering if individual groups can be assumed to be approximately Gaussian.</p> <p>While these methods aren\u2019t very effective at representing complex structures like images or language, related ideas will appear as components of some of the more advanced methods we\u2019ll see. https://mpatacchiola.github.io/blog/2020/07/31/gaussian-mixture-models.html</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/#variational-auto-encoders","title":"Variational Auto-Encoders","text":"<p>Auto-encoders and variational auto-encoders are widely used for learning compressed representations of data distributions, and can also be useful for \u201cdenoising\u201d inputs, which will come into play when we discuss diffusion. Some nice resources:</p> <p>\u201cAutoencoders\u201d chapter in the \u201cDeep Learning\u201d book blog post from Lilian Weng video from Arxiv Insights blog post from Prakash Pandey on both VAEs and GANs</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/#generative-adversarial-nets","title":"Generative Adversarial Nets","text":"<p>The basic idea behind Generative Adversarial Networks (GANs) is to simulate a \u201cgame\u201d between two neural nets \u2014 the Generator wants to create samples which are indistinguishable from real data by the Discriminator, who wants to identify the generated samples, and both nets are trained continuously until an equilibrium (or desired sample quality) is reached. Following from von Neumann\u2019s minimax theorem for zero-sum games, you basically get a \u201ctheorem\u201d promising that GANs succeed at learning distributions, if you assume that gradient descent finds global minimizers and allow both networks to grow arbitrarily large. Granted, neither of these are literally true in practice, but GANs do tend to be quite effective (although they\u2019ve fallen out of favor somewhat in recent years, partly due to the instabilities of simultaneous training).</p> <p>Resources:</p> <p>\u201cComplete Guide to Generative Adversarial Networks\u201d from Paperspace \u201cGenerative Adversarial Networks (GANs): End-to-End Introduction\u201d by Deep Learning, Ch. 20 - Generative Models (theory-focused)</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/#conditional-gans","title":"Conditional GANs","text":"<p>Conditional GANs are where we\u2019ll start going from vanilla \u201cdistribution learning\u201d to something which more closely resembles interactive generative tools like DALL-E and Midjourney, incorporating text-image multimodality. A key idea is to learn \u201crepresentations\u201d (in the sense of text embeddings or autoencoders) which are more abstract and can be applied to either text or image inputs. For example, you could imagine training a vanilla GAN on (image, caption) pairs by embedding the text and concatenating it with an image, which could then learn this joint distribution over images and captions. Note that this implicitly involves learning conditional distributions if part of the input (image or caption) is fixed, and this can be extended to enable automatic captioning (given an image) or image generation (given a caption). There a number of variants on this setup with differing bells and whistles. The VQGAN+CLIP architecture is worth knowing about, as it was a major popular source of early \u201cAI art\u201d generated from input text.</p> <p>Resources:</p> <p>\u201cImplementing Conditional Generative Adversarial Networks\u201d blog from Paperspace \u201cConditional Generative Adversarial Network \u2014 How to Gain Control Over GAN Outputs\u201d by Saul Dobilas \u201cThe Illustrated VQGAN\u201d by LJ Miranda \u201cUsing Deep Learning to Generate Artwork with VQGAN-CLIP\u201d talk from Paperspace</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/#diffusion-models","title":"Diffusion Models","text":"<p>One of the central ideas behind diffusion models (like StableDiffusion) is iterative guided application of denoising operations, refining random noise into something that increasingly resembles an image. Diffusion originates from the worlds of stochastic differential equations and statistical physics \u2014 relating to the \u201cSchrodinger bridge\u201d problem and optimal transport for probability distributions \u2014 and a fair amount of math is basically unavoidable if you want to understand the whole picture. For a relatively soft introduction, see \u201cA friendly Introduction to Denoising Diffusion Probabilistic Models\u201d by Antony Gitau. If you\u2019re up for some more math, check out \u201cWhat are Diffusion Models?\u201d for more of a deep dive. If you\u2019re more interested in code and pictures (but still some math), see \u201cThe Annotated Diffusion Model\u201d</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/","title":"Understanding Multimodal Large Language Models","text":"<p>Multimodal Large Language Models (LLMs) represent a significant advancement in artificial intelligence, extending the capabilities of traditional LLMs beyond text processing. These models can understand, interpret, and generate information from a variety of data types, or \"modalities,\" such as text, images, speech, and more. Their development is crucial for creating AI systems that can interact with the world in a more human-like manner and tackle a broader spectrum of complex tasks. This document will explore several key modalities, including text as a baseline, image understanding, speech processing (both speech-to-text and text-to-speech), and advanced vision capabilities like image segmentation.</p> <p>For a comprehensive technical dive into how multimodal LLMs are built and a review of recent models, Sebastian Raschka's article \"Understanding Multimodal LLMs\" is an excellent resource.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#core-concepts-in-multimodal-architectures","title":"Core Concepts in Multimodal Architectures","text":"<p>The fundamental challenge in building multimodal LLMs lies in the fact that LLMs are inherently text-based. To integrate other modalities like images or speech, specialized methods are required to convert or fuse these different data types into a format that the LLM can process and reason about alongside text.</p> <p>There are several ways to design these multimodal LLMs, but two common high-level approaches are prevalent, as detailed in Raschka's article:</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#unified-embedding-single-decoder-approach","title":"Unified Embedding / Single Decoder Approach","text":"<p>This method focuses on converting non-textual data into embedding vectors that are compatible with the LLM's existing text embeddings. These multimodal embeddings are then typically concatenated or otherwise combined with text embeddings and processed together by a single, often pre-trained, LLM decoder. Key components involved in this approach usually include:</p> <ul> <li>Modality Encoders: Specialized encoders for each non-text modality. For example, image encoders like Vision Transformers (ViT) or CNN-based architectures (often leveraging pre-trained models like those from CLIP) are used to extract features from images.</li> <li>Projector Layers: These are typically linear layers or small Multi-Layer Perceptrons (MLPs) that map the output embeddings from the modality encoders to the same dimensionality as the LLM's text embeddings. This alignment allows the different types of embeddings to be seamlessly combined and fed into the LLM.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#cross-modality-attention-approach","title":"Cross-Modality Attention Approach","text":"<p>In this approach, information from different modalities is fused more deeply within the transformer architecture itself, primarily using cross-attention mechanisms. Instead of simply concatenating embeddings at the input stage, the LLM's attention layers can directly attend to features or representations from other modalities (e.g., image patch features) at various stages of processing. This is analogous to the original Transformer architecture used for machine translation, where the decoder part attended to the encoder's output (representing the source language) via cross-attention. In a multimodal context, the \"encoder\" can be thought of as the modality-specific encoder (e.g., an image encoder), and the LLM decoder can then cross-attend to these features, allowing for a more integrated fusion of information.</p> <p>The choice of architecture impacts how modalities are integrated, the overall complexity of the model, the amount of new parameters that need to be trained, and potentially its performance characteristics on different types of multimodal tasks.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#exploring-key-modalities","title":"Exploring Key Modalities","text":""},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#text-models-baseline","title":"Text Models (Baseline)","text":"<p>Traditional Large Language Models (LLMs) like OpenAI's GPT series (e.g., GPT-3, GPT-4), Meta's Llama family (e.g., Llama 2, Llama 3), Google's Gemini (in its text-only capacity or as a base for multimodal versions), Anthropic's Claude models, and Mistral AI's models (e.g., Mistral 7B, Mixtral models) are primarily unimodal when focusing on their text processing capabilities. They serve as the foundation upon which many multimodal systems are built.</p> <ul> <li>Input Modality: Text (sequences of characters, words, or tokens).</li> <li>Core Functionality:</li> <li>Text Generation: Creating coherent and contextually relevant text (e.g., stories, articles, dialogues).</li> <li>Reading Comprehension &amp; Question Answering: Understanding text passages and answering questions based on them.</li> <li>Summarization: Condensing long texts into shorter summaries while preserving key information.</li> <li>Translation: Converting text from one language to another.</li> <li>Code Generation: Generating programming code based on natural language descriptions.</li> <li>How They Work (Briefly): These models use transformer architectures to process sequences of text tokens. They learn statistical patterns and relationships in language data during pretraining on vast text corpora, enabling them to predict subsequent tokens in a sequence or understand the semantic meaning of the input.</li> <li>Real-Life Usage:</li> <li>Chatbots and Conversational AI: Powering interactive agents like OpenAI's ChatGPT (based on GPT models), Google's Gemini experiences, Anthropic's Claude chatbot, and other customer service bots.</li> <li>Writing Assistance: Tools for grammar correction, style improvement, content generation (e.g., marketing copy, emails).</li> <li>Search Engines: Enhancing query understanding and providing more relevant results.</li> <li>Software Development: Assisting with code completion, documentation, and even simple program generation.</li> </ul> <p>Understanding these text-only models is crucial as they often form the core reasoning engine in more complex multimodal systems, with other modalities being \"translated\" or aligned to integrate with this linguistic foundation.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#image-modality-vision-understanding","title":"Image Modality (Vision Understanding)","text":"<p>Integrating images as an input modality allows LLMs to \"see\" and interpret visual information, bridging the gap between linguistic and visual understanding.</p> <ul> <li>Input Modality: Images (pixels, typically processed as patches).</li> <li>How It's Integrated (Common Approaches):</li> <li>Image Encoders: Raw images are processed by specialized vision models, often Vision Transformers (ViTs) or Convolutional Neural Networks (CNNs). Pretrained encoders like those from CLIP (Contrastive Language-Image Pre-Training) by OpenAI are commonly used to generate meaningful image embeddings. These encoders convert images into a sequence of vectors representing different parts or aspects of the image.</li> <li>Projector Layers: As discussed in the \"Core Concepts\" section, a projector layer (usually a small neural network, e.g., a Multi-Layer Perceptron or MLP) is often used to map the image embeddings from the vision encoder's output space to the LLM's text embedding space. This alignment is crucial for the LLM to process these visual tokens alongside text tokens.</li> <li>Concatenation with Text: In unified decoder architectures, these projected image embeddings are treated as a sequence of special \"image tokens\" and are often concatenated with text token embeddings before being fed into the LLM.</li> <li>Core Functionality:</li> <li>Image Captioning: Generating descriptive text that explains the content of an image.</li> <li>Visual Question Answering (VQA): Answering questions about an image (e.g., \"What color is the car?\", \"Are there any dogs in the picture?\").</li> <li>Image Classification/Tagging: Assigning one or more labels or tags to an image based on its content.</li> <li>Object Recognition (within context): Identifying objects in an image and understanding their relationships, often guided by textual prompts or questions.</li> <li>Real-Life Usage:</li> <li>Accessibility: Generating image descriptions for visually impaired users.</li> <li>Content Moderation: Identifying inappropriate or harmful visual content.</li> <li>Visual Search &amp; E-commerce: Allowing users to search for products using images or find visually similar items.</li> <li>Robotics &amp; Autonomous Systems: Enabling robots to understand their environment visually to navigate and interact.</li> <li>Education: Explaining diagrams, charts, and historical images.</li> <li>Key Models &amp; Providers:</li> <li>CLIP (OpenAI): While not an end-to-end VLM itself, its image and text encoders are foundational for many models that understand images.</li> <li>ViT (Google Brain team, now Google DeepMind): Vision Transformer, another foundational architecture for image encoding.</li> <li>Flamingo (Google DeepMind): An earlier influential model demonstrating VQA and image captioning with a cross-attention approach to fuse visual features.</li> <li>BLIP / BLIP-2 (Salesforce Research): Models focused on bootstrapping vision-language pretraining, effective for VQA and captioning.</li> <li>Fuyu-8B (Adept AI): Noted for its simpler architecture directly processing image patches.</li> <li>IDEFICS (Hugging Face): An open-source reproduction of Flamingo, useful for image-text tasks.</li> <li>Many models mentioned by Raschka, such as Molmo (Allen AI) and NVLM (NVIDIA), also fit here when discussing their image understanding capabilities.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#speech-modality-speech-to-text-and-text-to-speech","title":"Speech Modality (Speech-to-Text and Text-to-Speech)","text":"<p>Integrating speech allows for natural voice-based interaction with LLMs, encompassing both understanding spoken language and generating audible responses.</p> <ul> <li>Input/Output Modalities:</li> <li>Speech-to-Text (STT): Audio waveforms as input.</li> <li>Text-to-Speech (TTS): Text sequences as input, audio waveforms as output.</li> <li>How It's Integrated:</li> <li>Speech Encoders (for STT): These models (e.g., OpenAI's Whisper, Meta AI's Wav2Vec 2.0, Google's Conformer-based architectures used in their ASR models) process raw audio. They typically convert audio into a sequence of feature vectors (embeddings) that represent phonetic or acoustic information. These embeddings can then be:<ul> <li>Directly used by a specialized decoder to produce text.</li> <li>Fed into an LLM (possibly via a projector layer, similar to image modality) to allow the LLM to \"understand\" the spoken content and generate a text response or perform a task.</li> </ul> </li> <li>Speech Decoders/Vocoders (for TTS): To generate speech from text:<ul> <li>An LLM might generate the textual response.</li> <li>This text is then fed into a Text-to-Speech (TTS) model. TTS models often consist of two parts:         1. A spectrogram generator (e.g., Google's Tacotron 2, Microsoft/FastSpeech team's FastSpeech) that converts text into a mel-spectrogram (a visual representation of sound).         2. A vocoder (e.g., Google DeepMind's WaveNet, HiFi-GAN (various researchers), MelGAN (various researchers)) that converts the mel-spectrogram into an audible waveform.</li> <li>Some newer end-to-end models can generate speech more directly.</li> </ul> </li> <li>Core Functionality:</li> <li>Speech-to-Text (STT) / Automatic Speech Recognition (ASR): Transcribing spoken language into written text.</li> <li>Text-to-Speech (TTS): Converting written text into natural-sounding spoken language.</li> <li>Spoken Language Understanding (SLU): Understanding the intent and content of spoken queries, often involving STT followed by NLU (Natural Language Understanding by an LLM).</li> <li>Voice-Controlled Systems: Enabling interaction with devices and applications using voice commands.</li> <li>Key Differences:</li> <li>STT is about understanding audio and converting it to a symbolic representation (text).</li> <li>TTS is about generating audio from a symbolic representation (text).</li> <li>Multimodal LLMs can use STT as an input mechanism and TTS as an output mechanism to create conversational voice agents.</li> <li>Real-Life Usage:</li> <li>Voice Assistants: Siri, Google Assistant, Amazon Alexa rely heavily on STT and TTS.</li> <li>Transcription Services: Converting lectures, meetings, and dictations into text.</li> <li>Accessibility Tools: Screen readers for visually impaired users (TTS), voice input for users with motor impairments (STT).</li> <li>Customer Service: Automated voice responses and call routing.</li> <li>Content Creation: Generating voiceovers for videos, podcasts, and audiobooks.</li> <li>In-car Systems: Voice control for navigation and entertainment.</li> <li>Key Models &amp; Systems/Providers:</li> <li>Whisper (OpenAI): A highly effective open-source model for Automatic Speech Recognition (ASR).</li> <li>Wav2Vec 2.0 (Meta AI): A framework for self-supervised learning of speech representations, forming the basis for many ASR systems.</li> <li>Conformer (Google): An architecture combining CNNs and Transformers, widely used in Google's speech recognition services.</li> <li>Tacotron 2 (Google) &amp; WaveNet (Google DeepMind): Foundational models for high-quality Text-to-Speech synthesis; many newer TTS systems build upon these concepts.</li> <li>FastSpeech / FastSpeech 2 (Microsoft Research Asia / various): TTS models known for faster speech generation.</li> <li>HiFi-GAN / MelGAN (various research groups): Popular GAN-based vocoders for generating high-fidelity audio from mel-spectrograms.</li> <li>SeamlessM4T (Meta AI): A comprehensive multilingual and multitask model covering speech-to-text, text-to-speech, speech-to-speech translation, and text-to-text translation.</li> <li>Google Voice Assistant, Amazon Alexa, Apple's Siri: These commercial voice assistants integrate sophisticated STT and TTS pipelines, often using proprietary models based on architectures like those mentioned above.</li> <li>ElevenLabs, Coqui.ai: Examples of companies providing advanced TTS and voice cloning technologies.</li> <li>The Llama 3.2 multimodal architecture (as per Raschka's article) also includes provisions for speech modality, indicating future integration in such LLMs.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#vision-language-models-vlms-deeper-integration","title":"Vision-Language Models (VLMs - Deeper Integration)","text":"<p>While basic image understanding capabilities like captioning are foundational, Vision-Language Models (VLMs) represent a more advanced class of multimodal systems. These models aim for a deeper, more contextual understanding and reasoning ability that spans both visual and textual information. They often build upon the same core architectural concepts (unified embeddings or cross-attention) but are trained on more diverse datasets and tasks that require sophisticated interplay between vision and language.</p> <ul> <li>Input Modalities: Primarily Images/Video and Text.</li> <li>How They Differ from Basic Image Modality Integration:</li> <li>Complexity of Tasks: VLMs go beyond simple object recognition or captioning. They are designed for tasks requiring multi-step reasoning, understanding nuanced instructions related to visual content, and generating more detailed, context-aware textual outputs grounded in visual data.</li> <li>Instruction Following: Many advanced VLMs are instruction-tuned, meaning they can follow complex natural language instructions that refer to elements or concepts within an image or video.</li> <li>Knowledge Integration: They often leverage the LLM's vast world knowledge and reasoning capabilities to interpret visual scenes more effectively. For example, identifying not just objects, but also their relationships, potential affordances, or implied actions.</li> <li>Output Modalities: While primarily text output, some VLMs might also generate other outputs like bounding boxes or segmentation masks in response to queries (though dedicated segmentation models are more specialized for the latter).</li> <li>Core Functionality:</li> <li>Advanced Visual Question Answering (VQA): Answering complex questions that require deeper reasoning about image content, relationships between objects, and implied information (e.g., \"Why might the person in the image be feeling happy?\" or \"What is likely to happen next?\").</li> <li>Visual Dialogue: Engaging in multi-turn conversations about an image or video.</li> <li>Instruction Following with Visual Grounding: Performing tasks based on textual instructions that refer to specific parts or aspects of an image (e.g., \"Describe the object to the left of the red car,\" or \"If I move the blue block on top of the green one, what happens?\").</li> <li>Image/Video-based Text Generation: Writing stories, reports, or detailed descriptions based on visual input.</li> <li>Optical Character Recognition (OCR) in Context: Reading and understanding text embedded in images (e.g., street signs, labels on products) and using that information for broader reasoning.</li> <li>Real-Life Usage:</li> <li>Enhanced AI Assistants: More capable virtual assistants that can understand and discuss images or what a user is seeing (e.g., through a smartphone camera).</li> <li>Education &amp; Training: Interactive learning tools that can explain diagrams, scientific figures, or historical images in detail and answer student questions.</li> <li>Medical Image Analysis: Assisting radiologists by describing medical scans, answering questions about anomalies, or summarizing findings (though expert oversight is crucial).</li> <li>Content Creation &amp; Augmentation: Generating rich descriptions for products in e-commerce, creating alternative text for complex images, or even drafting articles based on visual information.</li> <li>Robotics and Embodied AI: Enabling robots to better understand and interact with their environment based on visual input and natural language commands.</li> <li>Key Models &amp; Providers:</li> <li>GPT-4V (OpenAI): A highly capable VLM known for its strong performance on various vision-language tasks, including complex reasoning and instruction following.</li> <li>Gemini (Google DeepMind): Google's flagship multimodal model series, designed to understand and reason across text, code, images, audio, and video.</li> <li>LLaVA / LLaVA-NeXT (various researchers, e.g., from UW Madison, Microsoft Research): Popular open-source approaches for building VLMs by connecting vision encoders (like CLIP's ViT) with LLMs (like Llama) using a simple projector, known for good performance with efficient training.</li> <li>Llama 3.2 Multimodal (Meta AI): As mentioned in Raschka's article, these models (11B and 90B parameters) use a cross-attention approach and are designed for image-text tasks.</li> <li>Qwen2-VL (Alibaba Cloud): Also from Raschka's article, notable for its \"Naive Dynamic Resolution\" mechanism to handle images of varying resolutions.</li> <li>NVLM (NVIDIA): Explores decoder-only, cross-attention, and hybrid approaches for VLMs, as detailed in Raschka's article.</li> <li>Pixtral 12B (Mistral AI): Mistral's first multimodal model, using a unified embedding decoder approach.</li> <li>CogVLM (THUDM - Tsinghua University): An open-source VLM known for its strong performance on visual grounding and dialogue tasks.</li> <li>IDEFICS (Hugging Face): While also useful for basic image-text tasks, its capabilities extend into VLM territory.</li> <li>Video-LLaMA, Video-ChatGPT (various researchers): Examples of models extending VLM concepts to video understanding, enabling tasks like video summarization and Q&amp;A about video content.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#segment-anything-advanced-vision-segmentation","title":"'Segment Anything' / Advanced Vision Segmentation","text":"<p>Beyond recognizing objects or describing scenes, a critical aspect of visual understanding is segmentation: identifying the precise pixel-level boundaries of objects and regions within an image. Models like Meta AI's Segment Anything Model (SAM) have revolutionized this space by enabling highly generalized, promptable segmentation.</p> <ul> <li>Input Modality: Images, along with various types of prompts (text descriptions, points, bounding boxes, or even masks from previous segmentation steps).</li> <li>How It's Integrated &amp; Key Architectural Components:</li> <li>Vision Backbone/Image Encoder: A powerful image encoder (often a Vision Transformer, e.g., ViT-H for SAM) processes the input image to extract robust visual features. This encoder is typically pretrained on a large dataset.</li> <li>Prompt Encoder: Encodes various user prompts (points, boxes, text) into embedding vectors. For text prompts, a text encoder like CLIP's might be used.</li> <li>Mask Decoder: This lightweight decoder takes the image embeddings and prompt embeddings as input and efficiently predicts segmentation masks for the objects or regions indicated by the prompts. SAM's decoder, for example, can output multiple valid masks for ambiguous prompts, along with confidence scores.</li> <li>Core Functionality:</li> <li>Zero-Shot Segmentation: The ability to segment objects or regions without having been explicitly trained on those specific object categories. The model generalizes from its broad pretraining.</li> <li>Promptable Segmentation: Users can guide the segmentation process by providing:<ul> <li>Points: Clicking on an object to segment it.</li> <li>Bounding Boxes: Drawing a box around an object.</li> <li>Text Prompts: Describing what to segment (e.g., \"segment all the cats\").</li> <li>Coarse Masks: Providing a rough mask to refine.</li> </ul> </li> <li>Ambiguity Handling: For ambiguous prompts (e.g., a point that could belong to multiple nested objects), models like SAM can generate multiple valid masks, allowing the user to select the desired one.</li> <li>Automatic Mask Generation: Some models can also generate masks for all detected objects in an image automatically.</li> <li>Integration with VLMs: The detailed segmentation masks can serve as precise inputs or grounding for Vision-Language Models, enabling them to reason about and interact with specific, user-defined image regions.</li> <li>Differences from Traditional Segmentation &amp; Other Vision Tasks:</li> <li>Generalization: Unlike traditional segmentation models that are often trained for a fixed set of object categories, \"Segment Anything\" models aim for universal segmentation capabilities.</li> <li>Promptability: The interactive, prompt-based nature is a key differentiator, making them highly versatile.</li> <li>Output Detail: Provides pixel-level masks, which are more detailed than bounding boxes (object detection) or image-level labels (classification).</li> <li>Real-Life Usage:</li> <li>Data Annotation: Rapidly annotating images for training other computer vision models by generating accurate masks with minimal human effort.</li> <li>Image Editing Software: Advanced selection tools (e.g., \"magic wand\" on steroids), background removal, object manipulation.</li> <li>Scientific Research: Analyzing microscopy images, satellite imagery (e.g., segmenting land cover, water bodies), or medical scans (e.g., identifying cells, tumors, anatomical structures).</li> <li>Creative Content Creation: Compositing images, creating visual effects.</li> <li>Robotics &amp; Autonomous Systems: Enhancing scene understanding by allowing robots to precisely identify and delineate objects for interaction or navigation.</li> <li>Augmented Reality (AR): More realistic placement and interaction of virtual objects with the real world by understanding precise object boundaries.</li> <li>Key Model &amp; Provider:</li> <li>Segment Anything Model (SAM) (Meta AI): The foundational model in this category, developed by Meta AI. SAM is renowned for its remarkable zero-shot generalization and promptable segmentation capabilities, largely due to its training on the extensive SA-1B dataset containing over a billion masks. Its architecture (ViT image encoder, prompt encoder, and mask decoder) has set a benchmark for generalist image segmentation tools.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#understanding-modality-vs-learned-capabilities","title":"Understanding Modality vs. Learned Capabilities","text":"<p>It's important to distinguish between a model's inherent modalities (the types of data it can process) and the specific skills or patterns it learns through training.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#what-is-a-modality","title":"What is a Modality?","text":"<p>A modality refers to the fundamental type(s) of data a model is architected to accept as input and/or produce as output. Examples include text-only models, image-text models (which can take images and text as input and might output text), audio-text models, and so on. Modality is largely determined by the model's architecture, including its specific encoders (for input) and decoders (for output). For instance, an image-text model needs an image encoder to process visual information and a way to integrate these image features with its language processing components, which might involve a shared or separate decoder.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#what-are-learned-capabilities-skillspatterns","title":"What are Learned Capabilities (Skills/Patterns)?","text":"<p>A learned capability or skill (e.g., coding proficiency, summarization, question answering, specific knowledge domains, particular writing styles) is a behavior or expertise the model acquires through its training data and process. These skills are developed within the model's given modalities. For example, code generation is a skill learned by a text-modal LLM; it doesn't mean the model has a separate \"code modality\" but rather that it has learned the patterns of code within the text modality.</p>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#how-specialized-skills-are-achieved-within-a-modality","title":"How Specialized Skills are Achieved within a Modality","text":"<p>Models develop specialized skills through rigorous training processes:</p> <ul> <li>Pretraining: Models learn general patterns, world knowledge, and foundational capabilities (like language structure for text LLMs) from exposure to vast and diverse datasets relevant to their modalities. For example, text LLMs are pretrained on massive text corpora, while VLMs are pretrained on large datasets of image-text pairs.</li> <li>Fine-tuning: To develop more specialized skills or adapt to specific tasks/domains, a pretrained base model is further trained (fine-tuned) on smaller, targeted datasets.</li> <li>Example (Coding): A text-modal LLM can be fine-tuned on a large corpus of source code, programming tutorials, and technical documentation. This doesn't change its modality (it still processes text), but it becomes highly proficient at understanding and generating code, effectively learning the \"pattern\" of coding.</li> <li>Other Examples: Fine-tuning for medical knowledge, legal document analysis, specific conversational styles, etc.</li> <li>Instruction Tuning: This is a powerful fine-tuning technique where models are trained on datasets of (instruction, desired_output) pairs. This teaches the model to become highly responsive to user instructions and perform a wide array of tasks described in natural language, making them more versatile and useful general-purpose assistants. Instruction tuning is key to how a base LLM learns to \"do\" many different things like translation, summarization, Q&amp;A, creative writing, etc., all typically within its original modality.</li> <li>Reinforcement Learning from Human Feedback (RLHF): Often used after instruction tuning, RLHF further refines model behavior to align better with human preferences regarding helpfulness, honesty, and harmlessness. It helps in fine-tuning the quality and style of the learned skills.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/#example-text-modality-llm-for-coding","title":"Example: Text Modality LLM for Coding","text":"<p>Let's explicitly address the example of an LLM demonstrating coding abilities:</p> <ul> <li>A Large Language Model like OpenAI's GPT-4 or Meta's Llama is fundamentally a text-modal system. Its architecture is designed to process and generate sequences of text.</li> <li>When such a model demonstrates strong coding abilities, it's not because it has a separate 'code modality.' Instead, it has learned the patterns and structures of programming languages through its training.</li> <li>This is typically achieved by including a vast amount of source code from public repositories (like GitHub), programming textbooks, and coding discussions in its pretraining data. Further specialized versions might be additionally fine-tuned specifically on code-related tasks or through instruction tuning with coding prompts.</li> <li>So, the LLM uses its text-processing capabilities to treat code as a specialized form of text. Its skill in coding is a highly developed pattern learned within its native text modality.</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Data%20for%20LLM/","title":"What are great data ?","text":"<ul> <li>Accuracy: factual correctness and relevance of samples</li> <li>diversity: wide range of use cases covering different topics, contexts, text lengths, writing styles</li> <li>complexity: include complex, multi-step reasoning problems and challenging tasks If your number of jobs. Is too low, use open-source instruction dataset. 70B model can be limited to 1k high quality samples , on the other hand, smaller models need more sample as they need to learn a simpler representation</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Data%20for%20LLM/#creating-an-instruction-dataset","title":"Creating an instruction dataset","text":"<ul> <li>Data curation:</li> <li>Task specific: get examples of pair dataset</li> <li>Domain specific: gather and validate quality and relevance of data</li> <li>Rule based filtering, length filtering, format checking, and keyword exclusion add to the quality of data saved for fine tuning</li> <li>Data deduplication: remove exact match, fuzzy match (for instance using minxish deduplication) or semantically similar data</li> <li>Data decontamination: data leakage between train, valves and test sets</li> <li>Data quality evaluation: human, LLM as a judge, reward model</li> <li>Data exploration: manual, statistical analysis, clustering</li> <li>Data generation: generate synthetic data</li> <li>Data augmentation: increase quantity and quality of data (from pre-existing data)</li> <li>In depth evolving: enhance complexity of existing instructions</li> <li>In breadth evolving: expand diversity of the dataset</li> <li>Ultra feedback method: instead of modify instructions, if modifies responses</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/LLMSECOps/","title":"LLMSECOps","text":"<p>https://huggingface.co/docs/safetensors/en/index</p>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Prompt%20Engineering%20Optimization/","title":"Prompt Engineering Optimization","text":"<p>https://www.promptingguide.ai/</p>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/","title":"Open Source Frameworks","text":""},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#model-inference","title":"Model Inference","text":"<p>You'll need to choose whether you want to run model inference locally (windows or MAC) or on cloud/premise infrastructure. You can switch from one environment to another but generally speaking, you need to run small language models (SLMs) quantized or not locally. You (usually) have only 1 GPU locally while starting quite fast having multiple on cloud so the considerations are different.</p> <p>There are many different options, so you need to ask yourself the following questions:</p> <ul> <li>do I need a managed serving engine or do I want to customize the serving ?</li> <li>do I want to serve my own LLM or am I happy with what opensource has to offer ?</li> </ul>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#local-inference","title":"Local inference","text":"<p>Ollama is one of my favorite. It is a managed framework that runs easily on both windows and MAC. It is an abstraction layer over llama.cpp, GGML and GGUF exposing an OpenAI-compatible interface. You can customize your model with ollama-compatible modelfiles and run quantized model. It comes with a lot of models available in a 1 line command. Ollama manages the orchestration while leaving the below layer to C++ llama.cpp library.</p> <p>Llama.cpp offers more flexibility has it is a layer under Ollama. It has a UI (https://docs.openwebui.com/getting-started/quick-start/starting-with-llama-cpp/) interface to chat directly with the model. It comes with a CLI and server integrated. It also contains a library to quantize models locally, transform weights into a compatible format like GGUF etc... It is recommended if you need more customization while keeping a managed way to optimize the model loading and inference.</p>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#cloud-inference","title":"Cloud inference","text":"<p>You can use the previous frameworks to serve your model on CLoud/Premise. However, the objectives are usually not the same. On cloud, you'll probably have more compute, would like to optimize your resources and would need a more resilient architecture as you won't switch every now and then.</p>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#overview","title":"Overview","text":"<p>vLLM is one of the best in class that is a great trade-off between simplicity of serving and production ready inference scalability.</p> <p>RedHat ran a benchmark comparing vLLM to Ollama: https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking#comparison_1__default_settings_showdown In summary, vLLM is a clear winner as it scales on throughputs and responsiveness with concurrent requests.</p> <p>It also ran a benchmark comparing vLLM to llama.cpp and found out that vLLM is still outperforming both on throughtputs and responsiveness for high concurrent requests. https://developers.redhat.com/articles/2025/09/30/vllm-or-llamacpp-choosing-right-llm-inference-engine-your-use-case</p> <p>So what can be vLLM's weakness ? As I experienced it, llama.cpp is more portable and has more flexiility than vLLM. You have also more contraints running vLLM depending on your infrastructure. So the first items to check are:</p> <ul> <li>your target architecture (framework, GPUs, model...)</li> <li>your target model size</li> <li>your target model usage (# clients, # requests)</li> <li>your target model constraints (throughput, latency etc...)</li> </ul> <p>You can see more requests and details in the LLMOps section.</p> <p>SGLang in some benchmarks (https://lmsys.org/blog/2024-07-25-sglang-llama3/) outperforms vLLM in particular for smallLM. It is a great alternative to test. Again, it will depends on your target deployments.</p>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#feature-comparison","title":"Feature comparison","text":"Feature vLLM llama.cpp TGI Ollama sglang TensorRT Language Python/CUDA C/C++ Rust/Python Go/C/TS Python/Rust/C++ C++/Python Model Support HF, Llama, MoE, Multi-modal GGUF, Llama, HF HF, Llama, Falcon, StarCoder, BLOOM Llama, Gemma, Mistral, Qwen, etc. Llama, Qwen, DeepSeek, Gemma, Mistral, GLM, GPT, etc. ONNX, HF, Llama, custom Quantization GPTQ, AWQ, INT4/8, FP8, TensorRT 1.5-8bit, GGUF bitsandbytes, GPTQ, AWQ, Marlin, fp8 GGUF, Safetensors FP4/FP8/INT4/AWQ/GPTQ, Multi-LoRA INT8/FP16/FP8 Hardware Support NVIDIA, AMD, Intel, ARM, TPU, TensorRT CPU, GPU (NVIDIA, AMD, Apple, Vulkan, etc.) NVIDIA, AMD, Intel, Gaudi, TPU CPU, GPU, Docker NVIDIA (GB200/B300/H100/A100), AMD (MI355/MI300), Intel Xeon, TPU, Ascend NPU NVIDIA GPU, Triton API Compatibility OpenAI, HF, REST, Open WebUI OpenAI, REST OpenAI, REST OpenAI, REST OpenAI, REST, HF, custom Triton, REST, custom Streaming Yes Yes Yes Yes Yes Yes Distributed/Parallel Serving Yes Partial Yes Partial Yes (tensor/pipeline/expert/data parallelism) Yes (via Triton) Prefix/Attention Caching Yes (Paged) Yes Yes Yes Yes (RadixAttention, paged) Yes Multi-LoRA Yes Yes Yes Yes Yes No Observability/Tracing OpenTelemetry Basic OpenTelemetry 3rd party OpenTelemetry, metrics OpenTelemetry Web UI Open WebUI, 3rd party Basic Swagger Many SGLang Web UI, 3rd party Triton UI Docker Support Yes Yes Yes Yes Yes Yes License Apache-2.0 MIT Apache-2.0 MIT Apache-2.0 Apache-2.0"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#benchmarks","title":"Benchmarks","text":"<p>Note that this benchmark table is a very high level summary and should be used with caution depending on your target architecture/model/usage.</p>"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#2-performance-benchmarks-2024","title":"2. Performance Benchmarks (2024)","text":"Framework Model Hardware Throughput (tokens/s) Latency (ms) Notes vLLM Llama-2-7B A100 80GB ~10,000+ &lt;100 Continuous batching, TensorRT support llama.cpp Llama-2-7B M2 Pro (Apple) ~5,000 &lt;200 Quantized, Metal TGI Llama-2-7B A100 80GB ~8,000 &lt;120 Tensor parallelism Ollama Llama-2-7B M2 Pro (Apple) ~4,000 &lt;250 GGUF, local sglang Llama-3-8B H100/GB200 ~12,000+ (prefill), ~15,000+ (decode) &lt;80 Large-scale expert parallelism TensorRT Llama-2-7B A100/H100 ~10,000+ &lt;100 INT8/FP16/FP8 quantization"},{"location":"GenAI%20Engineering/Chapter%207%20-%20LLMOps/Serving%20Frameworks/#sources","title":"Sources","text":"<ul> <li>vLLM Paper</li> <li>llama.cpp Docs</li> <li>TGI Docs</li> <li>Ollama Docs</li> <li>HuggingFace Model Benchmarks</li> <li>https://www.linkedin.com/posts/arazvant_why-do-i-use-ollama-for-most-of-my-local-llm-activity-7389632890915135489-lo9u?utm_source=share&amp;utm_medium=member_android&amp;rcm=ACoAAAVV2dEBAuuJCv1jGmfAXdBgR9YAUI0StlM</li> </ul>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Async/","title":"Async Python Client Usage","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Async/#async-use-cases","title":"Async use cases","text":"<p>E-commerce search platform:</p> <ul> <li>Consider an online store where hundreds of customers are simultaneously searching for products. Each search query must look through tens of millions of products and return results quickly to maintain a good user experience. This must hold true, even during busy periods like sales events.</li> </ul> <p>Personal productivity app:</p> <ul> <li>Imagine a note-taking application where users can save documents, to-do lists, and research materials. Users frequently perform multiple operations simultaneously - saving a new document while searching through their existing notes in multiple tabs. The app should be able to handle all of these operations happening at the same time.</li> </ul> <p>Customer support chatbot:</p> <ul> <li>Consider a RAG-powered support system. When a customer asks a question, the system must: (1) search the knowledge base for information, (2) gather context, and (3) send the retrieved context to an LLM for response generation. The complexity and length of these queries make it even more important to efficiently coordinate these multi-step workflows, to ensure fast response times even under high query volumes.</li> </ul> <p>In all these use cases, the common thread is I/O-bound operations, from distributed sources such as from multiple users. The async client shines when you need to coordinate multiple network calls (to main API, embedding models, or LLMs) efficiently. This is a basic explanation of how concurrency helps to speed up operations.</p> <p></p> <p>But, even when compared to parallelized sync client calls (e.g. through threads of multiple processes), the async client is more efficient, as you will see.</p> <p>So let\u2019s dive in to the world of async Python client.</p>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Async/#concurrency-in-python","title":"Concurrency in Python","text":"<p>We assume familiarity with concurrency. If this is new to you, we recommend skimming through this Python Documentation section, or online tutorials such as this or this.</p> <p>You can perform concurrent operations in Python using multi-processing (<code>multiprocessing</code>), multi-threading (<code>multithreading</code>) and coroutines (<code>asyncio</code>).</p> <p></p> <p>Multi-processing is the most isolated and resource-heavy, as each \u201cprocess\u201d can be thought as separate programs. At the other end of the scale are coroutines, where multiple tasks are coordinated in one thread to minimize unproductive time in any one operation.</p> <p>Concurrency in Weaviate are typically required where end users are involved; for data insertions, for search queries or for RAG queries. They come at somewhat unpredictable rates - as a distributed set of varying sizes and rates - and therefore apps must be able to handle a variety of loads.</p> <p>They are also typically I/O-bound, meaning that the application host, is doing a lot of waiting. This is caused by network latency and model inference from the integrated AI models.</p> <p>To illustrate, take a look at the below system diagram of Weaviate that illustrates the steps in a retrieval-augmented-generation (RAG) query.</p> <p></p> <p>This diagram shows that in one query, there may be 6(!) steps that involve data transport through a network. That\u2019s even before we discuss the time taken for model inference and time taken for search inside Weaviate.</p> <p>All this is to say that these are perfect for the use of <code>async/await</code> pattern coroutines that can be spun up at scale with very little overhead.</p>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Async/#when-to-choose-async-vs-sync-client","title":"When to Choose Async vs Sync Client","text":"<p>Given the above, is there a use case at all for the sync client? Well, the async client may be the best choice for concurrency unless there are specific reasons to not use it. For example - due to:</p> <ul> <li>Framework incompatibility (e.g. Django/Flask)</li> <li>Overhead of existing codebase</li> <li>Library incompatibility</li> </ul> <p>However, in many cases the async client will be preferable, such as with new projects, or for where the highest throughput is desired. The async client offers:</p> <ul> <li>More efficient concurrency (lighter than threads)</li> <li>Compatibility with modern frameworks (e.g. FastAPI)</li> </ul>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/","title":"Docker Cheatsheet","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#container-management","title":"Container Management","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#startstoprestart","title":"Start/Stop/Restart","text":"<pre><code>docker start &lt;container&gt;          # Start stopped container\ndocker stop &lt;container&gt;           # Graceful stop (SIGTERM)\ndocker kill &lt;container&gt;           # Force stop (SIGKILL)\ndocker restart &lt;container&gt;        # Restart container\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#run-containers","title":"Run Containers","text":"<pre><code>docker run -d &lt;image&gt;             # Run in background (detached)\ndocker run -it &lt;image&gt;            # Interactive with TTY\ndocker run -p 8080:80 &lt;image&gt;     # Port mapping\ndocker run -v /host:/container &lt;image&gt;  # Volume mount\ndocker run --rm &lt;image&gt;           # Auto-remove when stopped\ndocker run --name myapp &lt;image&gt;   # Custom name\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#list-inspect","title":"List &amp; Inspect","text":"<pre><code>docker ps                         # Running containers\ndocker ps -a                      # All containers (including stopped)\ndocker inspect &lt;container&gt;        # Detailed info\ndocker logs &lt;container&gt;           # View logs\ndocker logs -f &lt;container&gt;        # Follow logs\ndocker stats                      # Resource usage\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#execute-commands","title":"Execute Commands","text":"<pre><code>docker exec -it &lt;container&gt; bash  # Interactive shell\ndocker exec &lt;container&gt; &lt;command&gt; # Run command\ndocker cp &lt;container&gt;:/path /host # Copy files from container\ndocker cp /host &lt;container&gt;:/path # Copy files to container\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#image-management","title":"Image Management","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#build-tag","title":"Build &amp; Tag","text":"<pre><code>docker build -t &lt;name&gt;:&lt;tag&gt; .    # Build from Dockerfile\ndocker build -f &lt;dockerfile&gt; .    # Custom Dockerfile\ndocker tag &lt;image&gt; &lt;new-name&gt;     # Tag image\ndocker push &lt;image&gt;               # Push to registry\ndocker pull &lt;image&gt;               # Pull from registry\ndocker buildx build -f &lt;dockerfile&gt; -t &lt;name&gt;:&lt;tag&gt; --platform linux/amd64 . # build with specific platform\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#list-remove","title":"List &amp; Remove","text":"<pre><code>docker images                     # List images\ndocker rmi &lt;image&gt;                # Remove image\ndocker rmi $(docker images -q)   # Remove all images\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#cleanup-pruning","title":"Cleanup &amp; Pruning","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#remove-containers","title":"Remove Containers","text":"<pre><code>docker rm &lt;container&gt;             # Remove stopped container\ndocker rm -f &lt;container&gt;          # Force remove running container\ndocker rm $(docker ps -aq)       # Remove all containers\ndocker container prune           # Remove all stopped containers\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#remove-images","title":"Remove Images","text":"<pre><code>docker rmi &lt;image&gt;                # Remove image\ndocker image prune               # Remove dangling images\ndocker image prune -a            # Remove all unused images\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#system-cleanup","title":"System Cleanup","text":"<pre><code>docker system prune              # Remove stopped containers, dangling images, unused networks\ndocker system prune -a           # Remove all unused containers, images, networks, volumes (high pruning)\ndocker system prune --volumes    # Include volumes in cleanup\ndocker system df                 # Show disk usage\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#volume-management","title":"Volume Management","text":"<pre><code>docker volume ls                  # List volumes\ndocker volume rm &lt;volume&gt;        # Remove volume\ndocker volume prune              # Remove unused volumes\ndocker volume create &lt;name&gt;      # Create volume\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#network-management","title":"Network Management","text":"<pre><code>docker network ls                # List networks\ndocker network rm &lt;network&gt;      # Remove network\ndocker network prune            # Remove unused networks\ndocker network create &lt;name&gt;     # Create network\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#bulk-operations","title":"Bulk Operations","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#stop-all-containers","title":"Stop All Containers","text":"<pre><code>docker stop $(docker ps -q)      # Stop all running containers\ndocker kill $(docker ps -q)      # Force stop all containers\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#remove-all-containers","title":"Remove All Containers","text":"<pre><code>docker rm $(docker ps -aq)       # Remove all containers\ndocker rm -f $(docker ps -aq)    # Force remove all containers\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#remove-all-images","title":"Remove All Images","text":"<pre><code>docker rmi $(docker images -q)   # Remove all images\ndocker rmi -f $(docker images -q) # Force remove all images\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#docker-compose","title":"Docker Compose","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#basic-commands","title":"Basic Commands","text":"<pre><code>docker-compose up                 # Start services\ndocker-compose up -d              # Start in background\ndocker-compose down               # Stop and remove containers\ndocker-compose stop               # Stop services\ndocker-compose restart            # Restart services\ndocker-compose build              # Build images\ndocker-compose pull               # Pull images\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#logs-status","title":"Logs &amp; Status","text":"<pre><code>docker-compose logs               # View logs\ndocker-compose logs -f            # Follow logs\ndocker-compose ps                 # List services\ndocker-compose exec &lt;service&gt; bash # Execute command\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#registry-operations","title":"Registry Operations","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#login-push","title":"Login &amp; Push","text":"<pre><code>docker login                      # Login to Docker Hub\ndocker login &lt;registry&gt;           # Login to custom registry\ndocker push &lt;image&gt;               # Push image\ndocker pull &lt;image&gt;               # Pull image\ndocker search &lt;term&gt;              # Search Docker Hub\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#useful-flags-options","title":"Useful Flags &amp; Options","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#common-flags","title":"Common Flags","text":"<pre><code>-d, --detach                      # Run in background\n-it                               # Interactive + TTY\n-p, --publish                     # Port mapping\n-v, --volume                      # Volume mount\n--rm                              # Auto-remove container\n--name                            # Container name\n-e, --env                         # Environment variable\n--network                         # Network to connect\n--restart                         # Restart policy\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#resource-limits","title":"Resource Limits","text":"<pre><code>--memory 512m                     # Memory limit\n--cpus 1.5                        # CPU limit\n--memory-swap 1g                  # Swap limit\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#debug-commands","title":"Debug Commands","text":"<pre><code>docker inspect &lt;container&gt;        # Detailed container info\ndocker logs --details &lt;container&gt; # Detailed logs\ndocker events                     # Real-time events\ndocker version                    # Docker version\ndocker info                       # System info\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#common-issues","title":"Common Issues","text":"<pre><code># Permission denied\nsudo docker &lt;command&gt;\n\n# Port already in use\ndocker ps | grep &lt;port&gt;\n\n# Out of disk space\ndocker system prune -a --volumes\n\n# Container won't stop\ndocker kill &lt;container&gt;\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/Docker/#quick-cleanup-script","title":"Quick Cleanup Script","text":"<pre><code>#!/bin/bash\n# Nuclear cleanup - removes everything\ndocker kill $(docker ps -q) 2&gt;/dev/null\ndocker rm $(docker ps -aq) 2&gt;/dev/null\ndocker rmi $(docker images -q) 2&gt;/dev/null\ndocker volume prune -f\ndocker network prune -f\ndocker system prune -af --volumes\n</code></pre>"},{"location":"ML%20Engineering/Chapter%201%20-%20Engineering/resources/","title":"ML Engineering - WIP","text":"<p>From classical Machine Learning to Generative learning.</p> <p>Overview of latency in docs/img/latency_processing_pipelines.jpeg</p>"}]}