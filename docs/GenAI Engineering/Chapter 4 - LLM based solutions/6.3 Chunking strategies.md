# Chunking strategies

Having the relevant chunking imports a lot in the outcome of your solution.
If you cut too early, you're missing some context but if you take too much information then you might lose some specific semantic or increase the latnecy of your solution.

But do you even need chunking ? With LLMs having more and more context, it really depends on the size of your document(s).
If yes, it's desirable to start with a deterministic or simple strategy to stabilize other elements of the solution first.
In the reality, some solution require hierarchical or multi-steps chunking/processing because of the complexity of the document.

If you process research papers, they have indeed the same structure even though they might have very specific components.
But in a global enterprise, you can have in your database a wide variety of documents to handle.

- ğ——ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ ğ—¦ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²: Are you working with highly structured content (code, JSON, Markdown) or unstructured narrative text? Structure-aware chunking preserves logical organization.
- ğ——ğ—²ğ˜ğ—®ğ—¶ğ—¹ ğ—Ÿğ—²ğ˜ƒğ—²ğ—¹: Do you need to retrieve specific, granular facts or broader conceptual summaries? This determines whether you need smaller, focused chunks or larger, context-rich ones.
- ğ—¤ğ˜‚ğ—²ğ—¿ğ˜† ğ—–ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜…ğ—¶ğ˜ğ˜†: Simple questions benefit from targeted chunks. Complex queries often need more surrounding context to generate accurate responses.
- ğ—£ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ˜ƒğ˜€. ğ—¤ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜†: Simple methods like fixed-size chunking are fast and easy to implement. Advanced techniques like semantic or LLM-based chunking deliver better quality but require more compute.

**Pro tips**: Learn how to feed an LLM the right information at the right time, connecting it to external data, live tools, and memory.

![Chunking Strategy (source Weaviate)](../../img/chunking_strategies.jpeg)
