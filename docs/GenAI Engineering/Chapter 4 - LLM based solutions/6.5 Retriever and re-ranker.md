# Retriever strategies in Vector Database Retrieval

## Overview

When building LLM applications that query vector databases, understanding the difference between **Bi-Encoders** and **Cross-Encoders** is crucial for optimizing both performance and accuracy.

## Bi-Encoders

### What are Bi-Encoders?

Bi-Encoders process sentences independently to produce fixed-size vector embeddings. Each sentence is encoded separately, allowing for efficient similarity comparisons using cosine similarity or dot product.
![Bi-encoder (source Weaviate)](../../img/bi-encoder-weaviate.png)
**Architecture:**

- Query and documents are encoded independently
- Produces sentence embeddings (vectors) that can be stored and indexed
- Similarity calculated using cosine similarity between embeddings

### How Bi-Encoders Work in Vector Databases

```python
from sentence_transformers import SentenceTransformer

# Load bi-encoder model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encode documents (done once during indexing)
documents = ["The cat sat on the mat", "Dogs are loyal animals", "Python is a programming language"]
doc_embeddings = model.encode(documents)

# Encode user query
query = "What animals are mentioned?"
query_embedding = model.encode([query])

# Find most similar documents using cosine similarity
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(query_embedding, doc_embeddings)
```

**Advantages:**

- Fast retrieval (pre-computed embeddings)
- Scalable to millions of documents
- Efficient vector database operations

**Disadvantages:**

- Limited context interaction between query and document
- Lower accuracy compared to cross-encoders

## Cross-Encoders

### What are Cross-Encoders?

Cross-Encoders process query-document pairs simultaneously, allowing for rich interaction between the two texts. They produce a single relevance score (0-1) rather than separate embeddings.

**Architecture:**

- Query and document are concatenated and processed together
- No individual sentence embeddings produced
- Direct similarity score output

### Cross-Encoder Implementation

```python
from sentence_transformers.cross_encoder import CrossEncoder

# Load cross-encoder model
model = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")

# Score query against multiple documents
query = "What animals are mentioned?"
documents = ["The cat sat on the mat", "Dogs are loyal animals", "Python is a programming language"]

# Create query-document pairs
pairs = [[query, doc] for doc in documents]

# Get relevance scores
scores = model.predict(pairs)
print(scores)  # [0.8, 0.9, 0.1] - higher scores = more relevant
```

**Advantages:**

- Higher accuracy due to query-document interaction
- Better understanding of semantic relationships
- Superior performance on ranking tasks

**Disadvantages:**

- Computationally expensive (O(QÃ—D) comparisons)
- Cannot pre-compute embeddings
- Not suitable for large-scale initial retrieval

## Combining Bi-Encoders and Cross-Encoders: The Re-ranking Pipeline

The optimal approach combines both architectures in a two-stage pipeline:

- Use bi-encoder for fast, scalable retrieval of top-k candidates from vector database.
- Use cross-encoder to re-rank the top candidates for maximum accuracy.

![Cross-encoder as re-ranker (source Weaviate)](../../img/cross-encoder_weaviate.png)

```python
from sentence_transformers import SentenceTransformer
from sentence_transformers.cross_encoder import CrossEncoder
import numpy as np

# Initialize models
bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")

def retrieve_and_rerank(query, documents, top_k=100, rerank_top_k=10):
    """
    Two-stage retrieval and re-ranking pipeline
    """
    
    # Stage 1: Bi-encoder retrieval
    query_embedding = bi_encoder.encode([query])
    doc_embeddings = bi_encoder.encode(documents)
    
    # Calculate similarities and get top-k
    similarities = np.dot(query_embedding, doc_embeddings.T)[0]
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    # Stage 2: Cross-encoder re-ranking
    candidate_docs = [documents[i] for i in top_indices]
    pairs = [[query, doc] for doc in candidate_docs]
    
    # Get cross-encoder scores
    cross_scores = cross_encoder.predict(pairs)
    
    # Re-rank based on cross-encoder scores
    reranked_indices = np.argsort(cross_scores)[::-1][:rerank_top_k]
    
    # Return final results
    final_results = []
    for idx in reranked_indices:
        original_idx = top_indices[idx]
        final_results.append({
            'document': documents[original_idx],
            'bi_encoder_score': similarities[original_idx],
            'cross_encoder_score': cross_scores[idx],
            'original_index': original_idx
        })
    
    return final_results

# Example usage
query = "How do machine learning models work?"
documents = [
    "Machine learning models learn patterns from data",
    "Neural networks are a type of ML model",
    "Cats are popular pets",
    "Deep learning uses multiple layers",
    "Python is used for ML development"
]

results = retrieve_and_rerank(query, documents, top_k=3, rerank_top_k=2)
```

## Re-ranker in the Context of LLM Applications

### What is a Re-ranker?

A **re-ranker** is typically a cross-encoder model specifically trained for improving the relevance ranking of retrieved documents. In LLM applications, re-rankers serve as the bridge between initial retrieval and final answer generation.

### Re-ranking Pipeline in RAG Systems

```python
def rag_with_reranking(user_query, vector_db, llm_model):
    """
    Complete RAG pipeline with re-ranking
    """
    
    # 1. Initial retrieval from vector database (bi-encoder)
    initial_results = vector_db.similarity_search(user_query, k=50)
    
    # 2. Re-rank with cross-encoder
    reranked_results = rerank_documents(user_query, initial_results, top_k=5)
    
    # 3. Prepare context for LLM
    context = "\n".join([doc.content for doc in reranked_results])
    
    # 4. Generate answer with LLM
    prompt = f"Context: {context}\n\nQuestion: {user_query}\n\nAnswer:"
    answer = llm_model.generate(prompt)
    
    return answer, reranked_results
```

## Performance Comparison

| Aspect | Bi-Encoder | Cross-Encoder | Combined Pipeline |
|--------|------------|---------------|-------------------|
| **Speed** | Very Fast | Slow | Fast (optimized) |
| **Accuracy** | Good | Excellent | Excellent |
| **Scalability** | High | Low | High |
| **Memory Usage** | Low | High | Moderate |
| **Use Case** | Initial retrieval | Final ranking | Production RAG |

## Best Practices

### When to Use Each Approach

**Bi-Encoder Only:**

- Large-scale similarity search
- Real-time applications requiring low latency
- Limited computational resources

**Cross-Encoder Only:**

- Small document collections (< 1000 docs)
- Offline processing where accuracy is paramount
- Fine-grained relevance scoring

**Combined Pipeline (Recommended):**

- Production RAG systems
- Large document collections with accuracy requirements
- Applications with moderate latency tolerance

### Implementation Tips

1. **Optimize Top-K Values:** Balance between recall (bi-encoder top-k) and computational cost (cross-encoder re-ranking)

2. **Model Selection:** Choose domain-specific models when available (e.g., `ms-marco` for passage retrieval)

3. **Caching:** Cache bi-encoder embeddings but compute cross-encoder scores dynamically

4. **Batch Processing:** Process cross-encoder pairs in batches for efficiency

## Late interaction retrieval mode

<https://weaviate.io/blog/late-interaction-overview>
