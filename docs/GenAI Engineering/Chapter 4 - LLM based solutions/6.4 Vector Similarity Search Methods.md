## Major Categories of Vector Similarity Search methods

### 1. **Graph-Based Methods** (Current State-of-the-Art)

**HNSW (Hierarchical Navigable Small World)**

- **Strengths**: Super fast search speeds and fantastic recall, excellent for high-dimensional data
- **Best for**: In-memory applications with moderate dataset sizes (<50M vectors), real-time search requirements
- **Weaknesses**: High memory consumption, expensive index construction
- **Performance**: Typically achieves >95% recall with sub-millisecond query times

**DiskANN/Vamana**

- **Strengths**: Designed to handle vast datasets efficiently by operating primarily on disk
- **Best for**: Billion-scale datasets, memory-constrained environments
- **Weaknesses**: Construction is much slower than HNSW, limited update support
- **Performance**: Can handle billion-point datasets with good recall-QPS trade-offs

**NSW (Navigable Small World)**

- **Strengths**: Simpler than HNSW, good theoretical foundations
- **Best for**: Medium-scale applications where simplicity matters
- **Weaknesses**: Generally outperformed by HNSW in practice

### 2. **Quantization-Based Methods**

**Product Quantization (PQ)**

- **Strengths**: Excellent memory efficiency, good for high-dimensional vectors
- **Best for**: Large datasets with memory constraints, batch processing
- **Weaknesses**: Lower accuracy than graph methods, requires careful parameter tuning

**ScaNN (Scalable Nearest Neighbors)**

- **Strengths**: Google's optimized quantization with learned quantization
- **Best for**: Large-scale production systems, batch queries
- **Weaknesses**: Complex implementation, less flexible than graph methods

**Optimized Product Quantization (OPQ)**

- **Strengths**: Better accuracy than vanilla PQ through rotation optimization
- **Best for**: Similar to PQ but when higher accuracy is needed

### 3. **Tree-Based Methods**

**Annoy (Approximate Nearest Neighbors Oh Yeah)**

- **Strengths**: Simple, deterministic builds, good for static datasets
- **Best for**: Recommendation systems, small to medium datasets
- **Weaknesses**: Don't seem to have a sweet spot just yet, generally outperformed by modern methods

**Random Projection Trees**

- **Strengths**: Good theoretical guarantees, works well in moderate dimensions
- **Best for**: Medium-dimensional data, when interpretability matters
- **Weaknesses**: Performance degrades in very high dimensions

### 4. **Hash-Based Methods**

**Locality Sensitive Hashing (LSH)**

- **Strengths**: Theoretical guarantees, simple implementation
- **Best for**: Streaming data, distributed systems, sparse vectors
- **Weaknesses**: Parameter tuning complexity, often outperformed by modern methods

**Learning to Hash**

- **Strengths**: Data-adaptive, can learn optimal hash functions
- **Best for**: Specific domains with training data available
- **Weaknesses**: Requires training phase, domain-specific

### 5. **Hybrid and Emerging Methods**

**HNSW-IF (Inverted File + HNSW)**

- **Strengths**: Takes advantage of the speed/recall of HNSW in combination with the disk scalability of inverted indices
- **Best for**: Large-scale applications needing both speed and memory efficiency
- **Implementation**: Available in systems like Vespa

**GPU-Accelerated Methods**

- **Examples**: BANG, GPU-DiskANN, RAPIDS cuVS
- **Strengths**: Leveraging massive parallelism for billion-scale search
- **Best for**: High-throughput applications with GPU resources available

## Choosing the Right Algorithm

**For Real-time Applications (<1ms latency)**:

- HNSW for datasets up to 50M vectors
- GPU-accelerated methods for larger datasets with available hardware

**For Large-Scale Applications (>100M vectors)**:

- DiskANN for disk-based storage with high recall requirements
- Quantization methods (ScaNN, PQ) for memory-efficient solutions

**For Memory-Constrained Environments**:

- Product Quantization variants
- DiskANN for disk-based approach
- LSH for distributed scenarios

**For High-Dimensional Data (>1000 dimensions)**:

- HNSW generally performs best
- Learned quantization methods like ScaNN
- Avoid tree-based methods

**For Dynamic Datasets (frequent updates)**:

- HNSW supports incremental updates well
- LSH-based methods for streaming scenarios
- Avoid DiskANN due to poor update support

## Current State-of-the-Art (2024-2025)

The field is converging on **graph-based methods** as the dominant approach, with HNSW being the most widely adopted. HNSW following Milvus' Knowhere and Zilliz Cloud's performance is very competitive compared to other ANN methods. However, for specific use cases:

- **Billion-scale**: DiskANN and GPU-accelerated variants
- **Production systems**: Hybrid approaches like HNSW-IF
- **Edge devices**: Lightweight quantization methods
- **Streaming data**: Advanced LSH variants

The trend is toward hybrid methods that combine the strengths of different approaches, such as using quantization for memory efficiency while maintaining graph connectivity for search performance.

## Resources

<https://www.pinecone.io/learn/series/faiss/hnsw/>
