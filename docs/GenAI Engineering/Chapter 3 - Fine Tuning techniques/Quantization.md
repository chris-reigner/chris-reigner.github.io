# Understanding quantization (WIP)

*** Add cleaned notebooks from deeplearning AI ****

Top explanation: <https://docs.weaviate.io/weaviate/concepts/vector-quantization>

<https://blog.gopenai.com/exploring-bits-and-bytes-awq-gptq-exl2-and-gguf-quantization-techniques-with-practical-examples-74d590063d34>

<https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96>

<https://huggingface.co/blog/embedding-quantization#try-it-yourself>

Quantization considerations:

- 4-bit quantization reduces memory to ~25% of original - a 7B model drops from ~14GB to ~3.5GB
- 8-bit quantization halves memory requirements with minimal quality loss

<https://bentoml.com/llm/getting-started/llm-quantization>
<https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/>
<https://yousefhosni.medium.com/overview-of-llm-quantization-techniques-where-to-learn-each-of-them-0d8599acfec8>
