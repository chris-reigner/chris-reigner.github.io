
# Resources

Over the year, we all accumulated a great amount of resources.
This handbook is built upon a great deal of experiences but also based on the resources I'm sharing below.

Feel free to explore, follow and recommend new ones.
I'm doing my best to keep the list up to date.

- Blogs:
  - [Sebastian Raschka](https://magazine.sebastianraschka.com/) blog
  - [Hugging Face](https://huggingface.co/blog) blog posts
  - [Chip Huyen](https://huyenchip.com/blog/)’s blog
  - [Lilian Weng](https://lilianweng.github.io/)’s blog
    <!--- - [Tim Dettmers](https://timdettmers.com/)’s blog'-->
  - [Andrej Karpathy](https://karpathy.github.io/)'s blog
  - The [Deep Learning](https://www.deeplearningbook.org/) textbook
  - The [GenAI Handbook](https://genai-handbook.github.io/#) from Brown, William
  
- GitHub courses:
  - [ML Engineering repository](https://github.com/stas00/ml-engineering?tab=readme-ov-file)
  - [LLM Engineering Handbook from MLabonne](https://github.com/mlabonne/llm-course?tab=readme-ov-file)
  - [LLM Engineering weeks](https://github.com/ed-donner/llm_engineering?tab=readme-ov-file)
  - [The Hundred-Page Language Models Book from A Burkov](https://github.com/aburkov/theLMbook) NOT TESTED
  - [LLM Engineering guide](https://github.com/SylphAI-Inc/LLM-engineer-handbook?tab=readme-ov-file#fine-tuning)
  - [LLM Course](https://github.com/mlabonne/llm-course) from Maxime Labonne
  - DAIR repositories: <https://github.com/orgs/dair-ai/repositories>

- Courses:
  - The [d2l.ai](http://d2l.ai) interactive textbook with notebooks and github examples
  - The [Weaviate Academy](https://docs.weaviate.io/academy/py/compression) is very interesting about indexinv, vectorization, quantization, tokenization, compression strategies
  - The [Standford CME 295](https://cme295.stanford.edu/syllabus/) is quite dense but definitely the best in class for those who wants to dive technical in the topic

- (Free) Certifications
  - [Lighting AI certification](https://lightning.ai/ai-education/deep-learning-fundamentals/certification/)

- YouTube:
  - Andrej Karpathy’s [“Zero to Hero”](https://karpathy.ai/zero-to-hero.html) videos
  - [3Blue1Brown](https://www.youtube.com/c/3blue1brown) videos

Interview sandboxes:

- <https://github.com/Devinterview-io?tab=repositories>
- <https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/tree/main>

<https://docs.zenml.io/user-guides/llmops-guide>
<https://weaviate.io/blog/cross-encoders-as-reranker>
unsloth: <https://github.com/unslothai/notebooks/>
<https://github.com/philschmid/gemini-2.5-ai-engineering-workshop?tab=readme-ov-file>
<https://github.com/ritchieng/the-incredible-pytorch>
<https://github.com/youssefHosni/Data-Science-Interview-Questions-Answers/blob/main/Deep%20Learning%20Questions%20&%20Answers%20for%20Data%20Scientists.md>
<https://github.com/khangich/machine-learning-interview>
<https://tactlabs.gitbook.io/featurepreneur/pytorch-interview-questions>
<https://paperswithcode.com/paper/deep-learning-interviews-hundreds-of-fully>
<https://multimodalai.substack.com/p/understanding-llm-optimization-techniques>

<https://www.linkedin.com/posts/aishwarya-srinivasan_if-youre-an-ai-engineer-trying-to-optimize-activity-7334975771561680900-BXr-?utm_source=share&utm_medium=member_android&rcm=ACoAAAVV2dEBAuuJCv1jGmfAXdBgR9YAUI0StlMhttps://encord.com/blog/embeddings-machine-learning/?utm_source=linkedin>
Postraining labonne MIT: <https://www.youtube.com/watch?v=_HfdncCbMOE&list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&index=10>
Merge models: only merge same architecture and same size models ; merge FP/quantized not recommended ; test-time compute scaling (process reward models)
Fine tuning data preparation: <https://docs.unsloth.ai/basics/datasets-guidehttps://www.anthropic.com/news/contextual-retrievalhttps://github.com/KalyanKS-NLP/rag-zero-to-hero-guide?tab=readme-ov-file>
Opti: <https://neuralbits.substack.com/p/understanding-llm-optimization-techniqueshttps://github.com/stas00/ml-engineering?tab=readme-ov-filehttps://github.com/huggingface/evaluation-guidebook?tab=readme-ov-file> (useful to have beginner and advanced (tips and tricks)
<https://github.com/youssefHosni/Awesome-AI-Data-Guided-Projects?tab=readme-ov-filehttps://levelup.gitconnected.com/14-free-large-language-models-fine-tuning-notebooks-532055717cb7https://pub.towardsai.net/mastering-large-language-model-llm-fine-tuning-top-learning-resources-dcef012256fdhttps://levelup.gitconnected.com/best-resources-on-building-datasets-to-trian-llms-f6c6e02fc375https://levelup.gitconnected.com/14-free-large-language-models-fine-tuning-notebooks-532055717cb7https://pub.towardsai.net/best-resources-to-learn-understand-evaluating-llms-4610ee5dc5c1https://inkd.in/dW8wBJjXhttps://inkd.in/dSU922JQhttps://inkd.in/dz2rJ2PC>
Explication of quantiazation and lora: <https://mccormickml.com/2024/09/14/qlora-and-4bit-quantization/>
Search and sampling techniques next K: <https://huggingface.co/blog/mlabonne/decoding-strategieshttps://poloclub.github.io/transformer-explainer/https://huggingface.co/docs/safetensors/en/index>
MoE: <https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts>
<https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/README.md>
<https://gist.github.com/philschmid/d188034c759811a7183e7949e1fa0aa4>
<https://cameronrwolfe.substack.com/p/nano-moe>
<https://machinelearningmastery.com/mixture-of-experts-architecture-in-transformer-models/>

<https://genai-handbook.github.io/#>
Free clouds and GPU:
<https://www.ori.co/alphasignal-1000?utm_source=referral&utm_medium=alphasignal&utm_campaign=alphasignal_newsletter_1>
bentoML: <https://github.com/bentoml/OpenLLM>
Nvidia:
<https://github.com/NVIDIA/GenerativeAIExamples>
<https://www.linkedin.com/pulse/build-rag-app-nvidia-nim-milvus-running-locally-janakiram-msv-lch4c/?utm_source=share&utm_medium=member_android&utm_campaign=share_via>

Cookbook/Courses:
<https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x>
<https://github.com/PrunaAI/ai-efficiency-courses/blob/main/slides/01-language_model_architectures.pdf>
Mistral Cookbook: <https://github.com/mistralai/cookbook>
HF cookbook: <https://huggingface.co/learn/cookbook/index>
Anthropic cookbook: <https://github.com/anthropics/anthropic-cookbook>

Gui thomas: <https://github.com/guillaume-thomas/kto-mlflowhttps://benjaminwarner.dev/2023/07/01/attention-mechanism>
Mlops course: <https://github.com/iusztinpaul/energy-forecastinghttps://archive.docs.dagger.io/0.9/sdk/python/628797/get-started/https://mlflow.org/docs/latest/recipes.html>
MIT courses: <https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB>

Tutorial for RAG: <https://www.sakunaharinda.xyz/ragatouille-book/intro.htmlhttps://github.com/decodingml/llm-twin-course?tab=readme-ov-file>
Microsoft engineering playbook: <https://microsoft.github.io/code-with-engineering-playbook/https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ>
For beginner: <https://www.youtube.com/playlist?list=PLlrxD0HtieHj2nfK54c62lcs3-YSTx3Je>
DL LeCun: <https://atcold.github.io/NYU-DLSP21/>
Git flush: <https://www.linkedin.com/posts/rajatgajbhiye_i-will-never-understand-git-it-is-so-confusing-activity-7231890613313396736-XJZA/?utm_source=share&utm_medium=member_android>
example: <https://weaviate.io/developers/academy/py/multitenancy/overview>
End to end llm twin: <https://github.com/decodingml/llm-twin-course> —> advanced course for optimization engineering
Productionize LLM: <https://mrmaheshrajput.medium.com/how-to-productionize-large-language-models-llms-060a4cb1a169>
FINOS certifs: [https://teams.microsoft.com/l/message/19:da592d9ec47a4b75b5c5be73d06a82c6@thread.skype/1722937676415?tenantId=396b38cc-aa65-492b-bb0e-3d94ed25a97b&groupId=380efd03-e182-4c93-987f-90a86cd096d7&parentMessageId=1722937676415&teamName=AXA Software Engineering (SE4A)&channelName=Open Source and InnerSource&createdTime=1722937676415](https://teams.microsoft.com/l/message/19:da592d9ec47a4b75b5c5be73d06a82c6@thread.skype/1722937676415?tenantId=396b38cc-aa65-492b-bb0e-3d94ed25a97b&groupId=380efd03-e182-4c93-987f-90a86cd096d7&parentMessageId=1722937676415&teamName=AXA%20Software%20Engineering%20(SE4A)&channelName=Open%20Source%20and%20InnerSource&createdTime=1722937676415)
Llm prod like: <https://github.com/decodingml/llm-twin-course?tab=readme-ov-file>
<https://apps.courses.comet.com/learning/course/course-v1:Comet+101+2024/home>
<https://www.zenml.io/blog/automating-lightning-studio-ml-pipelines-for-fine-tuning-llm>
<https://parlance-labs.com/education/#fine-tuning>
<https://learning.oreilly.com/videos/what-is-llmops/0642572035242/0642572035242-video361334/>
Books list pdf: <https://github.com/anishLearnsToCode/books/tree/master>
Anriy burkov: <https://github.com/aburkov/theLMbook>
<https://github.com/marvelousmlops/marvel-characters>
pruning and prunai:
<https://github.com/PrunaAI/awesome-ai-efficiency?tab=readme-ov-file>
<https://github.com/PrunaAI/ai-efficiency-courses>

Dependencies:
Comparison: <https://www.playfulpython.com/environment-tools-pdm-poetry-rye/>
rye: <https://github.com/astral-sh/ryehttps://github.com/pdm-project/pdm>
<https://github.com/pdm-project/pdm>

Generic:
Repo of repo: <https://github.com/eugeneyan/applied-ml>
<https://ml-ops.org/content/mlops-principles>
Yellowbrick: <https://www.scikit-yb.org/en/latest/>
ludwig: <https://ludwig.ai/latest/>
pyreft: <https://github.com/stanfordnlp/pyrefthttps://github.com/stanfordnlp/pyreft?tab=readme-ov-filehttps://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing#scrollTo=NL0yGhbe3EFk>
Git: <https://dev.to/glasskube/the-guide-to-git-i-never-had-1450>
Link list: <https://towardsai.net/book>
Nice LLM overview: [https://areganti.notion.site/Week-7-Building-Your-Own-LLM-Application-fbac298e688a42148c18a1ddc7594362](https://www.notion.so/fbac298e688a42148c18a1ddc7594362?pvs=21)
Uihub: <https://uithub.com/openapi.html#/operations/getRepositoryContents>

Other

- wavelet: <https://github.com/ShawhinT/YouTube-Blog/tree/main>
