
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    
      <meta name="description" content="Handbook covering ML, LLM, GenAI and Agentic AI">
    
    
    
    
      <link rel="prev" href="../Debugging%20Pytorch/">
    
    
      <link rel="next" href="../Deep%20Learning%20Overview/">
    
    
    <link rel="icon" href="../../../assets/logo-light.png" sizes="any">
    <link rel="icon" href="../../../assets/logo-auto.svg" type="image/svg+xml">
    <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">

    
      
        <title>GPU Optimization - AI Engineering Handbook</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        
        <link rel="stylesheet" href="../../../assets/external/fonts.googleapis.com/css.49ea35f2.css">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#description-useful-tips-for-optimizing-pytorch-runs" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="AI Engineering Handbook" class="md-header__button md-logo" aria-label="AI Engineering Handbook" data-md-component="logo">
      <!-- Logo -->

  <img src="../../../assets/logo-light.svg" alt="logo" class="logo-light" />
  <img src="../../../assets/logo-dark.svg" alt="logo" class="logo-dark" />

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Engineering Handbook
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              GPU Optimization
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/chris-reigner/chris-reigner.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    chris-reigner.github.io
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Introduction/" class="md-tabs__link">
          
  
  
  Welcome

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/Async/" class="md-tabs__link">
          
  
  
  ML Engineering

        </a>
      </li>
    
  

    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../CPU%20vs%20GPU%20vs%20TPU/" class="md-tabs__link">
          
  
  
  GenAI Engineering

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../AI%20Agents%20Engineering/Resources/" class="md-tabs__link">
          
  
  
  AI Agents Engineering

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../About%20the%20Author/" class="md-tabs__link">
        
  
  
    
  
  About me

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://github.com/chris-reigner/chris-reigner.github.io" class="md-tabs__link">
        
  
  
    
  
  Contribute ❤️

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="AI Engineering Handbook" class="md-nav__button md-logo" aria-label="AI Engineering Handbook" data-md-component="logo">
      <!-- Logo -->

  <img src="../../../assets/logo-light.svg" alt="logo" class="logo-light" />
  <img src="../../../assets/logo-dark.svg" alt="logo" class="logo-dark" />

    </a>
    AI Engineering Handbook
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/chris-reigner/chris-reigner.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    chris-reigner.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Welcome
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ML Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            ML Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Concepts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1_1" id="__nav_2_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 1 - Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_1">
            <span class="md-nav__icon md-icon"></span>
            Chapter 1 - Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/Async/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Async
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/Docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/uv/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UV
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    GenAI Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            GenAI Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 1 - Deep Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Chapter 1 - Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../CPU%20vs%20GPU%20vs%20TPU/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CPU vs GPU vs TPU
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Pytorch%20GPU%20Setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pytorch GPU Setup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Debugging%20Pytorch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Debugging Pytorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    GPU Optimization
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    GPU Optimization
    
  </span>
  

      </a>
      
        

  

<nav class="md-nav md-nav--secondary" aria-label="On this page">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      On this page
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#description-useful-tips-for-optimizing-pytorch-runs" class="md-nav__link">
    <span class="md-ellipsis">
      description: Useful tips for optimizing Pytorch runs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      GPU Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPU Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-gpu-workflow-in-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Basic GPU Workflow in PyTorch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic GPU Workflow in PyTorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-determining-device-availability-and-setting-device" class="md-nav__link">
    <span class="md-ellipsis">
      1. Determining Device Availability and Setting Device
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-moving-models-to-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      2. Moving Models to GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-moving-tensors-to-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      3. Moving Tensors to GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-bringing-tensors-back-to-cpu" class="md-nav__link">
    <span class="md-ellipsis">
      4. Bringing Tensors back to CPU
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-optimization-techniques-in-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Key Optimization Techniques in PyTorch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Optimization Techniques in PyTorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adjusting-batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Adjusting Batch Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Leveraging Mixed Precision Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficient-data-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Efficient Data Loading
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cudnn-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      CuDNN Optimizations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profiling-and-debugging-gpu-code" class="md-nav__link">
    <span class="md-ellipsis">
      Profiling and Debugging GPU Code
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Profiling and Debugging GPU Code">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-profiling-is-essential" class="md-nav__link">
    <span class="md-ellipsis">
      Why Profiling is Essential
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accurately-timing-gpu-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Accurately Timing GPU Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monitoring-gpu-memory-usage" class="md-nav__link">
    <span class="md-ellipsis">
      Monitoring GPU Memory Usage
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-gpu-training-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-GPU Training Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi-GPU Training Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchnndataparallel" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn.DataParallel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnparalleldistributeddataparallel-ddp" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn.parallel.DistributedDataParallel (DDP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-choose" class="md-nav__link">
    <span class="md-ellipsis">
      When to Choose
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#further-considerations-for-gpu-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Further Considerations for GPU Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Further Considerations for GPU Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#impact-of-model-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Impact of Model Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#minimizing-cpu-gpu-synchronization" class="md-nav__link">
    <span class="md-ellipsis">
      Minimizing CPU-GPU Synchronization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-optimized-operations-and-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Leveraging Optimized Operations and Libraries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input-data-properties" class="md-nav__link">
    <span class="md-ellipsis">
      Input Data Properties
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Deep%20Learning%20Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deep Learning Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Deep%20Learning%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deep Learning Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Numerical%20representations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Numerical representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Entropy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Entropy
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 2 - LLM Pipeline
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Chapter 2 - LLM Pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Tokenizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tokenizers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview of embedding model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Encoding and Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 3 - Fine Tuning techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Chapter 3 - Fine Tuning techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine tuning overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Optimization techniques
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Model_choice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Choice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Frameworks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference Optimizations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 4 - LLM based solutions
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Chapter 4 - LLM based solutions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Optimization techniques
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.2%20Parser%20methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Parser methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.3%20Chunking%20strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chunking strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector Similarity Search Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Retriever and re-ranker
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 5 - Evaluation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Chapter 5 - Evaluation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation for NLP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation in LLM training and fine-tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Red%20Teaming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Red Teaming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Responsible%20AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Responsible AI
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 6 - Other type of models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Chapter 6 - Other type of models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%206%20-%20Other%20type%20of%20models/Generative%20modelling%20beyond%20sequences/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative modelling beyond sequences
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%206%20-%20Other%20type%20of%20models/multimodal_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multimodal models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 7 - LLMOps
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Chapter 7 - LLMOps
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/AI%20system%20architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI system architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Data%20for%20LLM/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data for LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Guardrails/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Guardrails
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/LLMSECOps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLMSECOps
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Observability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Observability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Prompt%20Engineering%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Engineering Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Serving%20Frameworks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Serving Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI Agents Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            AI Agents Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../AI%20Agents%20Engineering/Resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Concepts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../AI%20Agents%20Engineering/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../About%20the%20Author/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About me
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/chris-reigner/chris-reigner.github.io" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contribute ❤️
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  

<nav class="md-nav md-nav--secondary" aria-label="On this page">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      On this page
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#description-useful-tips-for-optimizing-pytorch-runs" class="md-nav__link">
    <span class="md-ellipsis">
      description: Useful tips for optimizing Pytorch runs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gpu-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      GPU Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GPU Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-gpu-workflow-in-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Basic GPU Workflow in PyTorch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic GPU Workflow in PyTorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-determining-device-availability-and-setting-device" class="md-nav__link">
    <span class="md-ellipsis">
      1. Determining Device Availability and Setting Device
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-moving-models-to-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      2. Moving Models to GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-moving-tensors-to-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      3. Moving Tensors to GPU
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-bringing-tensors-back-to-cpu" class="md-nav__link">
    <span class="md-ellipsis">
      4. Bringing Tensors back to CPU
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-optimization-techniques-in-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      Key Optimization Techniques in PyTorch
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Optimization Techniques in PyTorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adjusting-batch-size" class="md-nav__link">
    <span class="md-ellipsis">
      Adjusting Batch Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Leveraging Mixed Precision Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficient-data-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Efficient Data Loading
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-accumulation" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Accumulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cudnn-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      CuDNN Optimizations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#profiling-and-debugging-gpu-code" class="md-nav__link">
    <span class="md-ellipsis">
      Profiling and Debugging GPU Code
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Profiling and Debugging GPU Code">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-profiling-is-essential" class="md-nav__link">
    <span class="md-ellipsis">
      Why Profiling is Essential
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accurately-timing-gpu-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Accurately Timing GPU Operations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monitoring-gpu-memory-usage" class="md-nav__link">
    <span class="md-ellipsis">
      Monitoring GPU Memory Usage
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-gpu-training-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-GPU Training Strategies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi-GPU Training Strategies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#torchnndataparallel" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn.DataParallel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#torchnnparalleldistributeddataparallel-ddp" class="md-nav__link">
    <span class="md-ellipsis">
      torch.nn.parallel.DistributedDataParallel (DDP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-choose" class="md-nav__link">
    <span class="md-ellipsis">
      When to Choose
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#further-considerations-for-gpu-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Further Considerations for GPU Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Further Considerations for GPU Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#impact-of-model-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Impact of Model Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#minimizing-cpu-gpu-synchronization" class="md-nav__link">
    <span class="md-ellipsis">
      Minimizing CPU-GPU Synchronization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#leveraging-optimized-operations-and-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Leveraging Optimized Operations and Libraries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input-data-properties" class="md-nav__link">
    <span class="md-ellipsis">
      Input Data Properties
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<hr />
<h2 id="description-useful-tips-for-optimizing-pytorch-runs">description: Useful tips for optimizing Pytorch runs<a class="headerlink" href="#description-useful-tips-for-optimizing-pytorch-runs" title="Permanent link">&para;</a></h2>
<h1 id="gpu-optimization">GPU Optimization<a class="headerlink" href="#gpu-optimization" title="Permanent link">&para;</a></h1>
<p>Optimizing GPU utilization is paramount in deep learning due to the sheer volume of computations involved. Deep learning models, especially large ones, rely heavily on operations like matrix multiplications and convolutions, which can be computationally intensive. GPUs, with their massively parallel architecture consisting of thousands of cores, are specifically designed to execute these types of operations far more efficiently than Central Processing Units (CPUs). Leveraging GPUs effectively translates to substantially faster training times, which in turn allows for more rapid experimentation, iteration, and the feasibility of developing more complex and powerful models.</p>
<p>The strength of GPUs lies in their nature as parallel processors. They excel at Single Instruction, Multiple Data (SIMD) tasks, where the same operation is performed simultaneously on many data elements. This paradigm is a perfect match for the tensor and matrix operations that form the backbone of deep learning computations. In contrast, CPUs are typically optimized for sequential task execution or handling a smaller number of parallel threads, making them less suitable for the large-scale parallel computations inherent in training deep neural networks.</p>
<h2 id="basic-gpu-workflow-in-pytorch">Basic GPU Workflow in PyTorch<a class="headerlink" href="#basic-gpu-workflow-in-pytorch" title="Permanent link">&para;</a></h2>
<p>Understanding how to manage computations between the CPU and GPU is fundamental for leveraging PyTorch effectively. Here's a breakdown of the typical workflow:</p>
<h3 id="1-determining-device-availability-and-setting-device">1. Determining Device Availability and Setting Device<a class="headerlink" href="#1-determining-device-availability-and-setting-device" title="Permanent link">&para;</a></h3>
<p>Before performing any GPU operations, you need to check for GPU availability and define the device you intend to use.</p>
<ul>
<li><strong>Check Availability:</strong> Use <code>torch.cuda.is_available()</code> to determine if a CUDA-enabled GPU is present and usable by PyTorch.</li>
<li><strong>Set Device:</strong> Create a <code>torch.device</code> object. A common practice is to set it to <code>"cuda"</code> if a GPU is available, otherwise fallback to <code>"cpu"</code>.</li>
<li><strong>GPU Count:</strong> <code>torch.cuda.device_count()</code> returns the number of available GPUs.</li>
<li><strong>Specific GPU Selection:</strong></li>
<li>If you have multiple GPUs, you can select a specific one using its index (e.g., <code>torch.device('cuda:0')</code> for the first GPU, <code>torch.device('cuda:1')</code> for the second).</li>
<li><code>torch.cuda.set_device(index)</code> can set the default GPU globally for CUDA operations (less recommended for library code as it's a global state).</li>
<li>Alternatively, the <code>CUDA_VISIBLE_DEVICES</code> environment variable can control which GPUs are visible to PyTorch.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Check for GPU availability</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;CUDA is available. Number of GPUs: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Set the device to CUDA. If multiple GPUs are available, PyTorch will default to &#39;cuda:0&#39;.</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Primary GPU being used: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span><span class="si">}</span><span class="s2"> (Indices are 0-based, this shows the current default CUDA device)&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CUDA is not available. Using CPU.&quot;</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="2-moving-models-to-gpu">2. Moving Models to GPU<a class="headerlink" href="#2-moving-models-to-gpu" title="Permanent link">&para;</a></h3>
<p>PyTorch models (<code>torch.nn.Module</code> subclasses) need to be explicitly moved to the desired device to perform computations on that device.</p>
<ul>
<li>The recommended method is <code>.to(device)</code>, which is flexible and works for any <code>torch.device</code> object (CPU or GPU).</li>
<li>An older method, <code>.cuda()</code>, specifically moves the model to the default GPU. While it works, <code>.to(device)</code> is preferred for consistency and better device management.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Assuming &#39;device&#39; is defined from the previous example</span>
<span class="c1"># # (e.g., device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;))</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SimpleModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># A simple linear layer</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Instantiate the model (it&#39;s on CPU by default)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model device before moving: </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Move the model to the selected device (GPU or CPU)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model device after moving: </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="3-moving-tensors-to-gpu">3. Moving Tensors to GPU<a class="headerlink" href="#3-moving-tensors-to-gpu" title="Permanent link">&para;</a></h3>
<p>Similarly to models, tensors must be on the same device as the model for computations to occur between them.</p>
<ul>
<li>Use the <code>.to(device)</code> method to move a tensor to the target device. The older <code>.cuda()</code> method also works for moving to the default GPU.</li>
<li><strong>Crucially, all input tensors to a model's <code>forward()</code> method must reside on the same device as the model itself.</strong> PyTorch will raise an error if devices mismatch.</li>
<li>Tensors can also be created directly on a specific device (e.g., <code>torch.randn(size, device=device)</code>), which avoids an explicit move.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Example input tensor (initially on CPU by default)</span>
<span class="n">input_tensor_cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor device before moving: </span><span class="si">{</span><span class="n">input_tensor_cpu</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Move the input tensor to the same device as the model</span>
<span class="n">input_tensor_gpu</span> <span class="o">=</span> <span class="n">input_tensor_cpu</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input tensor device after moving: </span><span class="si">{</span><span class="n">input_tensor_gpu</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># If the model and input_tensor_gpu are on the same device, computation can proceed</span>
<span class="c1"># For demonstration, if &#39;model&#39; is on &#39;device&#39; and &#39;input_tensor_gpu&#39; is on &#39;device&#39;:</span>
<span class="c1"># output = model(input_tensor_gpu) </span>
<span class="c1"># print(f&quot;Output tensor device: {output.device}&quot;) # Will be same as &#39;device&#39;</span>

<span class="c1"># Creating a tensor directly on the device (avoids explicit move)</span>
<span class="n">tensor_direct_on_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tensor created directly on device: </span><span class="si">{</span><span class="n">tensor_direct_on_device</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="4-bringing-tensors-back-to-cpu">4. Bringing Tensors back to CPU<a class="headerlink" href="#4-bringing-tensors-back-to-cpu" title="Permanent link">&para;</a></h3>
<p>After performing computations on the GPU, you might need to move tensors back to the CPU for various reasons:</p>
<ul>
<li>Interacting with libraries that expect CPU tensors (e.g., NumPy for numerical operations, Matplotlib for plotting).</li>
<li>Saving tensor data to disk in a format primarily handled by CPU-based I/O.</li>
<li>Performing operations that are only implemented for CPU tensors.</li>
</ul>
<p>Use the <code>.cpu()</code> method to transfer a tensor to the CPU. If the tensor has gradients and you want to convert it to a NumPy array, you must first <code>.detach()</code> it to remove it from the computation graph.</p>
<h2 id="key-optimization-techniques-in-pytorch">Key Optimization Techniques in PyTorch<a class="headerlink" href="#key-optimization-techniques-in-pytorch" title="Permanent link">&para;</a></h2>
<p>Beyond the basic workflow, several techniques can further optimize your PyTorch code for GPU performance, leading to faster training and better resource utilization.</p>
<h3 id="adjusting-batch-size">Adjusting Batch Size<a class="headerlink" href="#adjusting-batch-size" title="Permanent link">&para;</a></h3>
<p>Batch size plays a crucial role in GPU utilization and model training dynamics.</p>
<ul>
<li><strong>GPU Utilization:</strong> Larger batch sizes can lead to better parallelism by providing more data for the GPU to process simultaneously. This helps saturate the GPU cores and can improve throughput (the amount of data processed per unit of time).</li>
<li><strong>Memory Trade-off:</strong> The primary constraint for batch size is GPU memory. Larger batches require more memory to store activations, gradients, and model parameters.</li>
<li><strong>Convergence Impact:</strong> The relationship between batch size and model convergence is complex. Larger batches might lead to quicker convergence per epoch but can sometimes result in finding sharper minima, which may generalize less well. Smaller batches can introduce noise that acts as a regularizer but might take longer to converge.</li>
<li><strong>Recommendation:</strong> Experimentation is key. The optimal batch size depends heavily on the specific model architecture, the available GPU memory, and the dataset. Start with a moderate size and increase it until you approach memory limits or observe diminishing returns in speed or undesirable convergence behavior.</li>
</ul>
<h3 id="leveraging-mixed-precision-training">Leveraging Mixed Precision Training<a class="headerlink" href="#leveraging-mixed-precision-training" title="Permanent link">&para;</a></h3>
<p>Mixed precision training combines lower-precision formats (like FP16 or BF16) with higher-precision FP32 to accelerate training and reduce memory usage.</p>
<ul>
<li><strong>Benefits:</strong> This technique offers significant speedups, especially on NVIDIA GPUs equipped with Tensor Cores, and can halve the memory footprint for parts of the model. For a detailed understanding of floating-point formats like FP16 and BF16, refer to the 'Numerical representations.md' document.</li>
<li><strong>PyTorch Implementation (<code>torch.cuda.amp</code>):</strong> PyTorch's Automatic Mixed Precision (AMP) module, <code>torch.cuda.amp</code>, simplifies this process.</li>
<li><code>autocast</code>: This context manager automatically casts operations within its scope to lower-precision types (FP16 or BF16 where appropriate and safe) to leverage hardware acceleration.</li>
<li><code>GradScaler</code>: Helps prevent underflow of gradients (where small gradient values become zero in FP16 due to its limited dynamic range). It scales the loss up before backpropagation, and then unscales the gradients before the optimizer step.</li>
</ul>
<p><strong>Code Example (PyTorch <code>torch.cuda.amp</code>):</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Enable AMP if CUDA is available</span>
<span class="n">amp_enabled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>

<span class="c1"># Create a gradient scaler for mixed precision</span>
<span class="c1"># The &#39;enabled&#39; flag allows conditional operation based on CUDA availability</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="n">amp_enabled</span><span class="p">)</span>

<span class="c1"># Example training loop iteration</span>
<span class="c1"># for data, target in data_loader:</span>
<span class="c1">#     data, target = data.to(device), target.to(device) # Move data to the target device</span>
<span class="c1">#     optimizer.zero_grad() # Clear previous gradients</span>
<span class="c1">#</span>
<span class="c1">#     # Cast operations to mixed precision (FP16/BF16 where appropriate)</span>
<span class="c1">#     # &#39;autocast&#39; is a context manager that enables mixed precision for the enclosed operations</span>
<span class="c1">#     with torch.cuda.amp.autocast(enabled=amp_enabled):</span>
<span class="c1">#         output = model(data) # Model forward pass in mixed precision</span>
<span class="c1">#         loss = criterion(output, target) # Calculate loss</span>
<span class="c1">#</span>
<span class="c1">#     # Scale loss and call backward() to create scaled gradients</span>
<span class="c1">#     # scaler.scale multiplies the loss by the current scale factor</span>
<span class="c1">#     scaler.scale(loss).backward()</span>
<span class="c1">#</span>
<span class="c1">#     # Unscale gradients (if any were scaled) and call optimizer.step()</span>
<span class="c1">#     # scaler.step also checks for inf/NaN gradients and skips optimizer.step if found</span>
<span class="c1">#     scaler.step(optimizer)</span>
<span class="c1">#</span>
<span class="c1">#     # Update the scale for next iteration</span>
<span class="c1">#     # scaler.update adjusts the scale factor for the next iteration based on gradient statistics</span>
<span class="c1">#     scaler.update()</span>
</code></pre></div>
<h3 id="efficient-data-loading">Efficient Data Loading<a class="headerlink" href="#efficient-data-loading" title="Permanent link">&para;</a></h3>
<p>Data loading can become a bottleneck if the GPU is idle while waiting for data from the CPU. <code>torch.utils.data.DataLoader</code> provides key parameters to optimize this:</p>
<ul>
<li><strong><code>num_workers</code>:</strong> Setting <code>num_workers &gt; 0</code> enables multi-process data loading. This means multiple worker processes load data in parallel, pre-fetching batches so they are ready when the GPU needs them. The optimal value depends on CPU cores and the nature of the data loading task, but a common starting point is the number of CPU cores.</li>
<li><strong><code>pin_memory=True</code>:</strong> When using GPUs, setting <code>pin_memory=True</code> in the <code>DataLoader</code> tells PyTorch to allocate the loaded data in "pinned" (page-locked) CPU memory. This allows for faster asynchronous data transfer from CPU memory to GPU memory, as pinned memory can be accessed directly by the GPU without intermediate copying to a staging area.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span> <span class="c1"># Assuming device is defined, e.g. from previous sections</span>

<span class="c1"># Example Dataset (replace with your actual dataset)</span>
<span class="c1"># class MyDataset(Dataset):</span>
<span class="c1">#     def __init__(self):</span>
<span class="c1">#         self.data = torch.randn(1000, 10) # Example data</span>
<span class="c1">#         self.labels = torch.randn(1000, 1) # Example labels</span>
<span class="c1">#     def __len__(self):</span>
<span class="c1">#         return len(self.data)</span>
<span class="c1">#     def __getitem__(self, idx):</span>
<span class="c1">#         return self.data[idx], self.labels[idx]</span>

<span class="c1"># dataset = MyDataset() # Instantiate your dataset</span>
<span class="c1"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Define device</span>

<span class="c1"># Optimized DataLoader</span>
<span class="c1"># num_workers_val = 4 if device.type == &#39;cuda&#39; else 0 # Use workers only for GPU</span>
<span class="c1"># pin_memory_val = True if device.type == &#39;cuda&#39; else False # Pin memory only for GPU</span>

<span class="c1"># train_loader = DataLoader(</span>
<span class="c1">#     dataset,</span>
<span class="c1">#     batch_size=64,</span>
<span class="c1">#     shuffle=True,</span>
<span class="c1">#     num_workers=num_workers_val,  # Adjust based on your CPU cores and task</span>
<span class="c1">#     pin_memory=pin_memory_val   # Speeds up CPU to GPU data transfer</span>
<span class="c1"># )</span>
<span class="c1"># print(f&quot;Using DataLoader with num_workers={train_loader.num_workers}, pin_memory={train_loader.pin_memory if device.type == &#39;cuda&#39; else &#39;N/A (CPU)&#39;}&quot;)</span>
</code></pre></div>
<h3 id="gradient-accumulation">Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="Permanent link">&para;</a></h3>
<p>Gradient accumulation is a technique to simulate a larger effective batch size when GPU memory limits the actual batch size that can be processed at once.</p>
<ul>
<li><strong>Process:</strong> Instead of updating model weights after each mini-batch, gradients are accumulated over several mini-batches. The optimizer step (<code>optimizer.step()</code>) is called only after a specified number of accumulation steps.</li>
<li><strong>Utility:</strong> This is useful when you want the benefits of a larger batch size (e.g., more stable gradients) but cannot fit that large batch into GPU memory.</li>
<li><strong>Optimizer Calls:</strong> <code>optimizer.zero_grad()</code> should be called at the beginning of each accumulation cycle (i.e., before processing the first mini-batch of an effective larger batch) and after <code>optimizer.step()</code>. The loss computed for each mini-batch should typically be normalized by the number of accumulation steps.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="c1"># # (Assuming model, optimizer, data_loader, device, criterion, and num_epochs are defined)</span>
<span class="c1"># model = YourModel().to(device)</span>
<span class="c1"># optimizer = torch.optim.Adam(model.parameters())</span>
<span class="c1"># data_loader = YourDataLoader(...) </span>
<span class="c1"># criterion = nn.MSELoss()</span>
<span class="c1"># num_epochs = 10</span>

<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Accumulate gradients over 4 mini-batches to simulate 4x batch size</span>

<span class="c1"># for epoch in range(num_epochs):</span>
<span class="c1">#     optimizer.zero_grad() # Zero gradients at the start of each new effective batch/epoch</span>
<span class="c1">#     for i, (inputs, labels) in enumerate(data_loader):</span>
<span class="c1">#         inputs, labels = inputs.to(device), labels.to(device)</span>
<span class="c1">#</span>
<span class="c1">#         # Forward pass</span>
<span class="c1">#         outputs = model(inputs)</span>
<span class="c1">#         loss = criterion(outputs, labels)</span>
<span class="c1">#</span>
<span class="c1">#         # Normalize loss to account for accumulation</span>
<span class="c1">#         # This ensures the effective loss magnitude is as if it were a single larger batch</span>
<span class="c1">#         loss = loss / accumulation_steps </span>
<span class="c1">#</span>
<span class="c1">#         # Backward pass (accumulates gradients)</span>
<span class="c1">#         loss.backward()</span>
<span class="c1">#</span>
<span class="c1">#         # Perform optimizer step after &#39;accumulation_steps&#39; mini-batches</span>
<span class="c1">#         if (i + 1) % accumulation_steps == 0:</span>
<span class="c1">#             optimizer.step()  # Update weights based on accumulated gradients</span>
<span class="c1">#             optimizer.zero_grad()  # Reset gradients for the next accumulation cycle</span>
<span class="c1">#</span>
<span class="c1">#     # Handle the case where the total number of batches isn&#39;t a multiple of accumulation_steps</span>
<span class="c1">#     # This ensures any remaining gradients are used for an update.</span>
<span class="c1">#     if len(data_loader) % accumulation_steps != 0:</span>
<span class="c1">#         optimizer.step() # Perform the final optimizer step for the epoch</span>
<span class="c1">#         optimizer.zero_grad() # Clear gradients before the next epoch</span>
</code></pre></div>
<h3 id="cudnn-optimizations">CuDNN Optimizations<a class="headerlink" href="#cudnn-optimizations" title="Permanent link">&para;</a></h3>
<p>CuDNN is NVIDIA's library of highly optimized primitives for deep learning operations (like convolutions). PyTorch leverages CuDNN for GPU computations.</p>
<ul>
<li><strong><code>torch.backends.cudnn.benchmark = True</code>:</strong> Setting this to <code>True</code> enables CuDNN's auto-tuner. Before the first execution of a new convolutional layer (or other supported operations) with a specific input size, CuDNN will benchmark several algorithms and select the fastest one for that particular configuration.</li>
<li><strong>Use Case:</strong> Ideal when input sizes to your model (especially for convolutional layers) remain constant throughout training.</li>
<li><strong>Trade-off:</strong> There's an upfront cost for benchmarking at the beginning or when input sizes change. If input sizes vary frequently, this might hurt performance.</li>
<li><strong><code>torch.backends.cudnn.deterministic = True</code>:</strong> For reproducibility, you might want to ensure that CuDNN uses deterministic algorithms. Setting this to <code>True</code> can achieve that.</li>
<li><strong>Trade-off:</strong> Deterministic algorithms may be less performant than non-deterministic ones chosen by the auto-tuner. This ensures bitwise reproducibility across runs on the same hardware, but potentially at the cost of speed.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="c1"># Enable CuDNN auto-tuner to find the best algorithm for the hardware</span>
    <span class="c1"># This can speed up training if input sizes to layers are consistent.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;torch.backends.cudnn.benchmark set to </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># For deterministic results (can impact performance and might not always be achievable)</span>
    <span class="c1"># If you need strict reproducibility, you might also need to set other random seeds (Python, NumPy, PyTorch).</span>
    <span class="c1"># torch.backends.cudnn.deterministic = True</span>
    <span class="c1"># print(f&quot;torch.backends.cudnn.deterministic set to {torch.backends.cudnn.deterministic}&quot;)</span>
</code></pre></div>
<h2 id="profiling-and-debugging-gpu-code">Profiling and Debugging GPU Code<a class="headerlink" href="#profiling-and-debugging-gpu-code" title="Permanent link">&para;</a></h2>
<p>Effective optimization requires understanding where your code spends its time and how it utilizes resources. Profiling and careful debugging are essential steps in this process.</p>
<h3 id="why-profiling-is-essential">Why Profiling is Essential<a class="headerlink" href="#why-profiling-is-essential" title="Permanent link">&para;</a></h3>
<p>Optimization efforts should always be guided by data, not just intuition. Human intuition about performance bottlenecks in complex software, especially involving hardware interactions like GPUs, is often misleading.</p>
<ul>
<li><strong>Identify Actual Bottlenecks:</strong> Profilers help pinpoint the true bottlenecks in your deep learning pipeline. These could be in data loading (<code>DataLoader</code> inefficiencies), specific model operations (e.g., large matrix multiplies, custom layers), CPU-GPU data transfers, or inefficient CUDA kernel implementations.</li>
<li><strong>Focus Efforts:</strong> By identifying the most time-consuming parts, you can focus your optimization efforts where they will have the most impact.</li>
<li><strong>Save Time:</strong> Time invested in profiling can save significant development time in the long run by preventing wasted effort on optimizing non-critical code sections.</li>
</ul>
<h3 id="accurately-timing-gpu-operations">Accurately Timing GPU Operations<a class="headerlink" href="#accurately-timing-gpu-operations" title="Permanent link">&para;</a></h3>
<p>CUDA operations are often asynchronous. When PyTorch code on the CPU calls a GPU operation, the CPU queues the operation and returns control to the Python script almost immediately, before the GPU has necessarily completed the task.</p>
<ul>
<li><strong><code>torch.cuda.synchronize(device=None)</code>:</strong> To get accurate timing for GPU code sections, you must use <code>torch.cuda.synchronize()</code>. This function blocks CPU execution until all previously queued kernels on the specified GPU (or the current GPU if <code>device</code> is <code>None</code>) have finished.</li>
</ul>
<p><strong>Code Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="c1"># Assuming &#39;device&#39; is a CUDA device # e.g., device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span>
<span class="c1"># And &#39;model&#39; is a PyTorch model # e.g., model = YourModel().to(device)</span>
<span class="c1"># And &#39;input_tensor&#39; is on the same device # e.g., input_tensor = torch.randn(128, 3, 224, 224, device=device)</span>

<span class="c1"># if device.type == &#39;cuda&#39;:</span>
<span class="c1">#     # Incorrect timing (measures CPU dispatch time, not actual GPU execution)</span>
<span class="c1">#     start_time_naive = time.time()</span>
<span class="c1">#     # output = model(input_tensor) # Example operation</span>
<span class="c1">#     end_time_naive = time.time()</span>
<span class="c1">#     # print(f&quot;Naive timing: {end_time_naive - start_time_naive:.6f} seconds (may be inaccurate for GPU)&quot;)</span>

<span class="c1">#     # Correct timing for GPU operations</span>
<span class="c1">#     torch.cuda.synchronize() # Wait for all preceding GPU work to finish</span>
<span class="c1">#     start_time_sync = time.time()</span>
<span class="c1">#</span>
<span class="c1">#     # output_sync = model(input_tensor) # The operation to time</span>
<span class="c1">#</span>
<span class="c1">#     torch.cuda.synchronize() # Wait for &#39;model(input_tensor)&#39; (the operation being timed) to finish</span>
<span class="c1">#     end_time_sync = time.time()</span>
<span class="c1">#     # print(f&quot;Accurate timing with synchronize(): {end_time_sync - start_time_sync:.6f} seconds&quot;)</span>
<span class="c1"># else:</span>
<span class="c1">#     # print(&quot;CUDA not available, skipping GPU timing example.&quot;)</span>
</code></pre></div>
<h3 id="monitoring-gpu-memory-usage">Monitoring GPU Memory Usage<a class="headerlink" href="#monitoring-gpu-memory-usage" title="Permanent link">&para;</a></h3>
<p>Understanding and monitoring GPU memory usage is critical for preventing out-of-memory (OOM) errors and for optimizing batch sizes to maximize GPU utilization without exceeding memory capacity.</p>
<ul>
<li><strong>PyTorch Functions:</strong> PyTorch provides several functions to inspect memory usage on CUDA devices:</li>
<li><code>torch.cuda.memory_allocated(device=None)</code>: Returns the current GPU memory occupied by tensors in bytes for the given (or current) device. This reflects memory used by your active tensors.</li>
<li><code>torch.cuda.max_memory_allocated(device=None)</code>: Returns the peak GPU memory occupied by tensors since the beginning of the program or the last call to <code>reset_peak_memory_stats</code>.</li>
<li><code>torch.cuda.memory_reserved(device=None)</code>: Returns the total GPU memory currently managed by PyTorch's caching memory allocator. This includes memory allocated for tensors plus any reserved but currently unused cached blocks.</li>
<li><code>torch.cuda.max_memory_reserved(device=None)</code>: Returns the peak GPU memory managed by the caching allocator since the program start or last reset.</li>
<li><code>torch.cuda.empty_cache()</code>: Releases all unused cached memory blocks from PyTorch's caching allocator back to the OS. This does <em>not</em> free memory occupied by active tensors. It can be useful if memory fragmentation is suspected, but frequent use can slow down subsequent allocations as PyTorch might have to re-request memory from the OS.</li>
<li><strong><code>nvidia-smi</code> Command-Line Tool:</strong> The NVIDIA System Management Interface (<code>nvidia-smi</code>) is an external command-line utility that provides real-time monitoring of NVIDIA GPU devices. It displays GPU utilization, memory usage, temperature, power consumption, and currently running processes on each GPU. It's excellent for a quick overview of GPU health and identifying which processes are consuming GPU resources.</li>
</ul>
<p><strong>Code Example (PyTorch functions):</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Assuming &#39;device&#39; is a CUDA device # e.g., device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span>

<span class="c1"># if device.type == &#39;cuda&#39;:</span>
<span class="c1">#     # Initial memory snapshot</span>
<span class="c1">#     initial_allocated = torch.cuda.memory_allocated(device)</span>
<span class="c1">#     initial_reserved = torch.cuda.memory_reserved(device)</span>
<span class="c1">#     print(f&quot;Initial memory allocated: {initial_allocated / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#     print(f&quot;Initial memory reserved by cache: {initial_reserved / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#</span>
<span class="c1">#     # Example: Create a large tensor</span>
<span class="c1">#     # x = torch.randn(10000, 10000, device=device) # Approx 381 MB for FP32</span>
<span class="c1">#     # print(f&quot;Memory allocated after creating tensor x: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#     # print(f&quot;Memory reserved by cache after x: {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#     # print(f&quot;Max memory allocated so far: {torch.cuda.max_memory_allocated(device) / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#     # print(f&quot;Max memory reserved by cache so far: {torch.cuda.max_memory_reserved(device) / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#</span>
<span class="c1">#     # del x # Delete the tensor</span>
<span class="c1">#     # print(f&quot;Memory allocated after &#39;del x&#39;: {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB (tensor gone, but cache might hold it)&quot;)</span>
<span class="c1">#</span>
<span class="c1">#     # torch.cuda.empty_cache() # Release unused cached memory</span>
<span class="c1">#     # print(f&quot;Memory allocated after empty_cache(): {torch.cuda.memory_allocated(device) / 1024**2:.2f} MB&quot;)</span>
<span class="c1">#     # print(f&quot;Memory reserved by cache after empty_cache(): {torch.cuda.memory_reserved(device) / 1024**2:.2f} MB&quot;)</span>
<span class="c1"># else:</span>
<span class="c1">#     # print(&quot;CUDA not available, skipping GPU memory usage example.&quot;)</span>
</code></pre></div>
<p><strong><code>nvidia-smi</code> Command-Line Example (run in your terminal):</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># To view current GPU status including memory usage, utilization, etc.</span>
nvidia-smi

<span class="c1"># To continuously monitor GPU status (updates every 1 second)</span>
watch<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="w"> </span>nvidia-smi
</code></pre></div>
<h2 id="multi-gpu-training-strategies">Multi-GPU Training Strategies<a class="headerlink" href="#multi-gpu-training-strategies" title="Permanent link">&para;</a></h2>
<p>When a single GPU is insufficient for training speed or model/batch size, PyTorch offers built-in ways to distribute training across multiple GPUs. This can significantly accelerate the training process or enable the use of larger models and batches that wouldn't fit on a single device.</p>
<h3 id="torchnndataparallel"><code>torch.nn.DataParallel</code><a class="headerlink" href="#torchnndataparallel" title="Permanent link">&para;</a></h3>
<p><code>torch.nn.DataParallel</code> (DP) is a simpler way to achieve multi-GPU training within a single process using Python threads.</p>
<ul>
<li><strong>How it Works:</strong><ol>
<li>The input batch is split along the batch dimension and distributed to the available GPUs.</li>
<li>The model is replicated on each GPU.</li>
<li>A forward pass is performed on each GPU with its slice of data.</li>
<li>Outputs are gathered on a primary GPU (usually <code>cuda:0</code>), where the loss is computed.</li>
<li>Gradients are then computed on the primary GPU and scattered back to each model replica for the backward pass and parameter updates.</li>
</ol>
</li>
<li><strong>Pros:</strong></li>
<li>Easy to implement, often requiring just wrapping the model.</li>
<li><strong>Cons:</strong></li>
<li><strong>Imbalanced GPU Utilization:</strong> The primary GPU (where outputs are gathered and loss is computed) often bears a higher load.</li>
<li><strong>GIL Issues:</strong> Python's Global Interpreter Lock (GIL) can be a bottleneck due to its reliance on threading.</li>
<li><strong>Model Replication Overhead:</strong> The model is copied to each GPU in each forward pass, which can be inefficient.</li>
<li><strong>Generally Not Recommended:</strong> Generally, <code>DistributedDataParallel</code> is preferred for better performance.</li>
</ul>
<p><strong>Code Example (Conceptual - how to wrap a model):</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># class MyModel(nn.Module):</span>
<span class="c1">#     def __init__(self):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.fc = nn.Linear(10,1)</span>
<span class="c1">#     def forward(self, x):</span>
<span class="c1">#         return self.fc(x)</span>

<span class="c1"># model = MyModel() # Your model definition</span>
<span class="c1"># # Check if multiple GPUs are available</span>
<span class="c1"># if torch.cuda.is_available() and torch.cuda.device_count() &gt; 1:</span>
<span class="c1">#     print(f&quot;Using {torch.cuda.device_count()} GPUs!&quot;)</span>
<span class="c1">#     model = nn.DataParallel(model) # Wrap the model</span>

<span class="c1"># device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # Ensure the model wrapper is on the primary device</span>
<span class="c1"># model.to(device) # Move the DataParallel wrapper to the primary GPU (e.g., cuda:0)</span>
<span class="c1"># # The DataParallel model itself resides on &#39;device&#39; (e.g. cuda:0), which acts as the primary device for output gathering.</span>
</code></pre></div>
<h3 id="torchnnparalleldistributeddataparallel-ddp"><code>torch.nn.parallel.DistributedDataParallel</code> (DDP)<a class="headerlink" href="#torchnnparalleldistributeddataparallel-ddp" title="Permanent link">&para;</a></h3>
<p><code>torch.nn.parallel.DistributedDataParallel</code> (DDP) is the generally recommended approach for multi-GPU training, offering better performance and scalability, including multi-node (multiple machines) training.</p>
<ul>
<li><strong>How it Works:</strong> DDP uses multi-processing, where one independent process is typically created for each GPU.<ol>
<li>The model is replicated once on each GPU at the beginning of training.</li>
<li>Each process handles a portion of the input data.</li>
<li>During the backward pass, gradients are computed locally and then efficiently averaged across all processes using optimized collective communication operations (often via NCCL).</li>
<li>Each process then updates its local copy of the model weights independently.</li>
</ol>
</li>
<li><strong>Pros:</strong></li>
<li><strong>Faster Performance:</strong> Typically provides significant speedups over <code>DataParallel</code> due to more efficient gradient communication and no GIL bottleneck.</li>
<li><strong>Efficient GPU Utilization:</strong> Workload is generally more balanced across GPUs.</li>
<li><strong>Overcomes GIL:</strong> By using separate processes, it avoids GIL limitations.</li>
<li><strong>Standard for Distributed Training:</strong> The preferred method for most serious multi-GPU and distributed training scenarios.</li>
<li><strong>Cons:</strong></li>
<li>
<p><strong>More Setup:</strong> Requires more boilerplate code to initialize process groups, set up communication backends, and launch multiple processes.</p>
</li>
<li>
<p><strong>Note on Usage:</strong>
    Setting up DDP is more involved than <code>DataParallel</code>. It typically requires:</p>
<ol>
<li>Initializing a process group (e.g., using <code>torch.distributed.init_process_group</code>).</li>
<li>Ensuring each process works on its designated GPU.</li>
<li>Wrapping the model with <code>DistributedDataParallel</code>.</li>
<li>Using a distributed sampler for the <code>DataLoader</code> to ensure each process gets a unique part of the data.</li>
<li>Launching the script using a utility like <code>torchrun</code> (recommended) or <code>torch.multiprocessing.spawn</code>.
Due to its complexity and context-dependent setup (especially for different environments like single-node multi-GPU vs. multi-node), a full code example is not provided here. Please refer to the <a href="https://pytorch.org/tutorials/beginner/dist_overview.html">official PyTorch documentation on Distributed Training</a> and <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">DDP examples</a> for detailed implementation guides.</li>
</ol>
</li>
</ul>
<h3 id="when-to-choose">When to Choose<a class="headerlink" href="#when-to-choose" title="Permanent link">&para;</a></h3>
<ul>
<li><strong><code>torch.nn.DataParallel</code>:</strong> Consider for quick experiments or simple scenarios where ease of implementation is paramount and peak performance or scalability is not critical.</li>
<li><strong><code>torch.nn.parallel.DistributedDataParallel</code> (DDP):</strong> Recommended for most other cases, especially for serious training, achieving better performance, and scaling to multiple GPUs or even multiple machines. The initial setup effort is often outweighed by the performance gains.</li>
</ul>
<h2 id="further-considerations-for-gpu-optimization">Further Considerations for GPU Optimization<a class="headerlink" href="#further-considerations-for-gpu-optimization" title="Permanent link">&para;</a></h2>
<p>Beyond the specific techniques discussed, several other factors and practices can influence GPU performance.</p>
<h3 id="impact-of-model-architecture">Impact of Model Architecture<a class="headerlink" href="#impact-of-model-architecture" title="Permanent link">&para;</a></h3>
<p>The design of the neural network itself can significantly impact GPU performance.</p>
<ul>
<li>Some operations are inherently more GPU-friendly than others. Large matrix multiplications and standard convolutions, which form the backbone of many deep learning models, are well-suited for GPU parallel processing.</li>
<li>Operations that are highly sequential, involve custom CUDA kernels that are not fully optimized, or require frequent CPU-GPU interaction (e.g., control flow decisions based on tensor values that need to be brought to CPU) can become bottlenecks.</li>
<li>When designing or choosing models, especially for resource-constrained environments or performance-critical applications, consider GPU efficiency. For example, architectures using depthwise separable convolutions (common in mobile-efficient models) might offer a better trade-off between performance and accuracy compared to standard convolutions in some scenarios. Being aware of operations that are computationally expensive or less parallelizable can guide model selection.</li>
</ul>
<h3 id="minimizing-cpu-gpu-synchronization">Minimizing CPU-GPU Synchronization<a class="headerlink" href="#minimizing-cpu-gpu-synchronization" title="Permanent link">&para;</a></h3>
<p>As mentioned earlier, CUDA operations are asynchronous. The CPU queues operations on the GPU and then continues its own work.</p>
<ul>
<li>Explicit synchronization points, such as <code>torch.cuda.synchronize()</code>, force the CPU to wait until all previously queued GPU tasks on a specific device are complete. While essential for accurate timing (as discussed in profiling) or when data is immediately needed on the CPU (e.g., <code>.item()</code> or <code>.cpu()</code> on a tensor needed for a control flow decision), overuse in performance-critical loops can stall the GPU pipeline and negate the benefits of asynchronous execution.</li>
<li>Minimize explicit synchronizations. Let the GPU work asynchronously as much as possible.</li>
<li>Be aware that some PyTorch operations might implicitly synchronize. Profiling tools like NVIDIA Nsight Systems can help identify such synchronization points and their impact.</li>
</ul>
<h3 id="leveraging-optimized-operations-and-libraries">Leveraging Optimized Operations and Libraries<a class="headerlink" href="#leveraging-optimized-operations-and-libraries" title="Permanent link">&para;</a></h3>
<p>PyTorch's strength lies in its extensive library of built-in operations that are highly optimized for both CPU and GPU execution.</p>
<ul>
<li>Prefer PyTorch's built-in layers and functions (e.g., <code>torch.nn.Conv2d</code>, <code>torch.matmul</code>, optimized activation functions) whenever possible, as these often have underlying implementations that call highly optimized libraries like CuDNN for NVIDIA GPUs.</li>
<li>Avoid re-implementing complex operations manually in Python if optimized versions are available. Custom Python loops over tensor elements, for example, will be significantly slower than equivalent vectorized PyTorch operations.</li>
<li>For specific tasks or experimental features, external libraries might offer further specialized, performance-tuned implementations. For instance, libraries like NVIDIA's Apex have historically provided cutting-edge features, and higher-level frameworks like <code>fastai</code> often incorporate performance best practices by default. The core idea is to leverage existing, well-tested, and optimized code rather than writing from scratch where performance is critical.</li>
</ul>
<h3 id="input-data-properties">Input Data Properties<a class="headerlink" href="#input-data-properties" title="Permanent link">&para;</a></h3>
<p>The characteristics of your input data can also affect performance and memory usage.</p>
<ul>
<li><strong>Input Size:</strong> Larger input dimensions (e.g., high-resolution images, long sequences) directly translate to increased memory consumption for activations and intermediate tensors, and more computational work. This might necessitate smaller batch sizes to fit within GPU memory.</li>
<li><strong>Variable Input Sizes:</strong> If inputs within a batch have varying sizes (e.g., sentences of different lengths in NLP), they often need to be padded to a common size to be processed in a batch. Excessive padding can lead to wasted computation on padding tokens. While techniques like bucketing (grouping inputs of similar sizes into batches) or using packed sequences (for RNNs) can mitigate this, they add complexity. Where feasible, batching inputs of similar sizes or using models/techniques robust to variable sizes can be beneficial.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "content.code.select", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.top", "search.highlight", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../../assets/external/unpkg.com/mermaid@11/dist/mermaid.min.js"></script>
      
    
  </body>
</html>