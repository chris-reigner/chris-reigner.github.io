
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    
      <meta name="description" content="Handbook covering ML, LLM, GenAI and Agentic AI">
    
    
    
    
      <link rel="prev" href="../Generative%20modelling%20beyond%20sequences/">
    
    
      <link rel="next" href="../../Chapter%207%20-%20LLMOps/AI%20system%20architecture/">
    
    
    <link rel="icon" href="../../../assets/logo-light.png" sizes="any">
    <link rel="icon" href="../../../assets/logo-auto.svg" type="image/svg+xml">
    <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.14">

    
      
        <title>Multimodal models - AI Engineering Handbook</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        
        <link rel="stylesheet" href="../../../assets/external/fonts.googleapis.com/css.49ea35f2.css">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#understanding-multimodal-large-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="AI Engineering Handbook" class="md-header__button md-logo" aria-label="AI Engineering Handbook" data-md-component="logo">
      <!-- Logo -->

  <img src="../../../assets/logo-light.svg" alt="logo" class="logo-light" />
  <img src="../../../assets/logo-dark.svg" alt="logo" class="logo-dark" />

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AI Engineering Handbook
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multimodal models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/chris-reigner/chris-reigner.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    chris-reigner.github.io
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Introduction/" class="md-tabs__link">
          
  
  
  Welcome

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/Async/" class="md-tabs__link">
          
  
  
  ML Engineering

        </a>
      </li>
    
  

    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/" class="md-tabs__link">
          
  
  
  GenAI Engineering

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../AI%20Agents%20Engineering/Resources/" class="md-tabs__link">
          
  
  
  AI Agents Engineering

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../About%20the%20Author/" class="md-tabs__link">
        
  
  
    
  
  About me

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://github.com/chris-reigner/chris-reigner.github.io" class="md-tabs__link">
        
  
  
    
  
  Contribute ❤️

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="AI Engineering Handbook" class="md-nav__button md-logo" aria-label="AI Engineering Handbook" data-md-component="logo">
      <!-- Logo -->

  <img src="../../../assets/logo-light.svg" alt="logo" class="logo-light" />
  <img src="../../../assets/logo-dark.svg" alt="logo" class="logo-dark" />

    </a>
    AI Engineering Handbook
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/chris-reigner/chris-reigner.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    chris-reigner.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Welcome
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    ML Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            ML Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Concepts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1_1" id="__nav_2_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Chapter 1 - Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1_1">
            <span class="md-nav__icon md-icon"></span>
            Chapter 1 - Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/Async/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Async
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/Docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/uv/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    UV
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ML%20Engineering/Chapter%201%20-%20Engineering/resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    GenAI Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            GenAI Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 1 - Deep Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Chapter 1 - Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/CPU%20vs%20GPU%20vs%20TPU/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    CPU vs GPU vs TPU
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/Pytorch%20GPU%20Setup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pytorch GPU Setup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/Debugging%20Pytorch/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Debugging Pytorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/GPU%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPU Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deep Learning Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/Deep%20Learning%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deep Learning Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/Numerical%20representations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Numerical representations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%201%20-%20Deep%20Learning/Entropy/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Entropy
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 2 - LLM Pipeline
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Chapter 2 - LLM Pipeline
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Tokenizers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tokenizers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Overview%20of%20embedding%20model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Overview of embedding model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Encoding%20and%20Decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Encoding and Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/Transformers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%202%20-%20LLM%20Pipeline/LLM%20Architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLM Architecture
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 3 - Fine Tuning techniques
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Chapter 3 - Fine Tuning techniques
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Fine%20tuning%20overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine tuning overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Training/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Training%20Optimization%20techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Training Optimization techniques
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Quantization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Quantization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Supervised_fine_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Supervised Fine Tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Model_choice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Choice
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Frameworks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%203%20-%20Fine%20Tuning%20techniques/Inference%20Optimizations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Inference Optimizations
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 4 - LLM based solutions
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Chapter 4 - LLM based solutions
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.0%20RAG%20Overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.1%20RAG%20Optimization%20techniques/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG Optimization techniques
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.2%20Parser%20methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Parser methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.3%20Chunking%20strategies/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Chunking strategies
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.4%20Vector%20Similarity%20Search%20Methods/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vector Similarity Search Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%204%20-%20LLM%20based%20solutions/6.5%20Retriever%20and%20re-ranker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Retriever and re-ranker
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 5 - Evaluation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Chapter 5 - Evaluation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Evaluation%20for%20NLP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation for NLP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Evaluation%20in%20LLM%20training%20and%20fine-tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation in LLM training and fine-tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Red%20Teaming/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Red Teaming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%205%20-%20Evaluation/Responsible%20AI/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Responsible AI
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" checked>
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 6 - Other type of models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Chapter 6 - Other type of models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Generative%20modelling%20beyond%20sequences/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Generative modelling beyond sequences
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Multimodal models
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Multimodal models
    
  </span>
  

      </a>
      
        

  

<nav class="md-nav md-nav--secondary" aria-label="On this page">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      On this page
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#core-concepts-in-multimodal-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Core Concepts in Multimodal Architectures
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Concepts in Multimodal Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unified-embedding-single-decoder-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Unified Embedding / Single Decoder Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modality-attention-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Modality Attention Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploring-key-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      Exploring Key Modalities
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exploring Key Modalities">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-models-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      Text Models (Baseline)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-modality-vision-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      Image Modality (Vision Understanding)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speech-modality-speech-to-text-and-text-to-speech" class="md-nav__link">
    <span class="md-ellipsis">
      Speech Modality (Speech-to-Text and Text-to-Speech)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-models-vlms-deeper-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Vision-Language Models (VLMs - Deeper Integration)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#segment-anything-advanced-vision-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      'Segment Anything' / Advanced Vision Segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-modality-vs-learned-capabilities" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding Modality vs. Learned Capabilities
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Understanding Modality vs. Learned Capabilities">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-a-modality" class="md-nav__link">
    <span class="md-ellipsis">
      What is a Modality?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-are-learned-capabilities-skillspatterns" class="md-nav__link">
    <span class="md-ellipsis">
      What are Learned Capabilities (Skills/Patterns)?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-specialized-skills-are-achieved-within-a-modality" class="md-nav__link">
    <span class="md-ellipsis">
      How Specialized Skills are Achieved within a Modality
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-text-modality-llm-for-coding" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Text Modality LLM for Coding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Chapter 7 - LLMOps
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Chapter 7 - LLMOps
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/AI%20system%20architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI system architecture
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Data%20for%20LLM/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data for LLM
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Guardrails/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Guardrails
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/LLMSECOps/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLMSECOps
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Observability/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Observability
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Prompt%20Engineering%20Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prompt Engineering Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Chapter%207%20-%20LLMOps/Serving%20Frameworks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Serving Frameworks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    AI Agents Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            AI Agents Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../AI%20Agents%20Engineering/Resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Resources
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Concepts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../AI%20Agents%20Engineering/introduction/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../About%20the%20Author/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    About me
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://github.com/chris-reigner/chris-reigner.github.io" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contribute ❤️
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  

<nav class="md-nav md-nav--secondary" aria-label="On this page">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      On this page
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#core-concepts-in-multimodal-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Core Concepts in Multimodal Architectures
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Core Concepts in Multimodal Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unified-embedding-single-decoder-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Unified Embedding / Single Decoder Approach
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modality-attention-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Modality Attention Approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploring-key-modalities" class="md-nav__link">
    <span class="md-ellipsis">
      Exploring Key Modalities
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exploring Key Modalities">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-models-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      Text Models (Baseline)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-modality-vision-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      Image Modality (Vision Understanding)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speech-modality-speech-to-text-and-text-to-speech" class="md-nav__link">
    <span class="md-ellipsis">
      Speech Modality (Speech-to-Text and Text-to-Speech)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-language-models-vlms-deeper-integration" class="md-nav__link">
    <span class="md-ellipsis">
      Vision-Language Models (VLMs - Deeper Integration)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#segment-anything-advanced-vision-segmentation" class="md-nav__link">
    <span class="md-ellipsis">
      'Segment Anything' / Advanced Vision Segmentation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#understanding-modality-vs-learned-capabilities" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding Modality vs. Learned Capabilities
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Understanding Modality vs. Learned Capabilities">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-a-modality" class="md-nav__link">
    <span class="md-ellipsis">
      What is a Modality?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-are-learned-capabilities-skillspatterns" class="md-nav__link">
    <span class="md-ellipsis">
      What are Learned Capabilities (Skills/Patterns)?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-specialized-skills-are-achieved-within-a-modality" class="md-nav__link">
    <span class="md-ellipsis">
      How Specialized Skills are Achieved within a Modality
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-text-modality-llm-for-coding" class="md-nav__link">
    <span class="md-ellipsis">
      Example: Text Modality LLM for Coding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="understanding-multimodal-large-language-models">Understanding Multimodal Large Language Models<a class="headerlink" href="#understanding-multimodal-large-language-models" title="Permanent link">&para;</a></h1>
<p>Multimodal Large Language Models (LLMs) represent a significant advancement in artificial intelligence, extending the capabilities of traditional LLMs beyond text processing. These models can understand, interpret, and generate information from a variety of data types, or "modalities," such as text, images, speech, and more. Their development is crucial for creating AI systems that can interact with the world in a more human-like manner and tackle a broader spectrum of complex tasks. This document will explore several key modalities, including text as a baseline, image understanding, speech processing (both speech-to-text and text-to-speech), and advanced vision capabilities like image segmentation.</p>
<p>For a comprehensive technical dive into how multimodal LLMs are built and a review of recent models, Sebastian Raschka's article "<a href="https://magazine.sebastianraschka.com/p/understanding-multimodal-llms">Understanding Multimodal LLMs</a>" is an excellent resource.</p>
<h2 id="core-concepts-in-multimodal-architectures">Core Concepts in Multimodal Architectures<a class="headerlink" href="#core-concepts-in-multimodal-architectures" title="Permanent link">&para;</a></h2>
<p>The fundamental challenge in building multimodal LLMs lies in the fact that LLMs are inherently text-based. To integrate other modalities like images or speech, specialized methods are required to convert or fuse these different data types into a format that the LLM can process and reason about alongside text.</p>
<p>There are several ways to design these multimodal LLMs, but two common high-level approaches are prevalent, as detailed in Raschka's article:</p>
<h3 id="unified-embedding-single-decoder-approach">Unified Embedding / Single Decoder Approach<a class="headerlink" href="#unified-embedding-single-decoder-approach" title="Permanent link">&para;</a></h3>
<p>This method focuses on converting non-textual data into embedding vectors that are compatible with the LLM's existing text embeddings. These multimodal embeddings are then typically concatenated or otherwise combined with text embeddings and processed together by a single, often pre-trained, LLM decoder.
Key components involved in this approach usually include:</p>
<ul>
<li><strong>Modality Encoders:</strong> Specialized encoders for each non-text modality. For example, image encoders like Vision Transformers (ViT) or CNN-based architectures (often leveraging pre-trained models like those from CLIP) are used to extract features from images.</li>
<li><strong>Projector Layers:</strong> These are typically linear layers or small Multi-Layer Perceptrons (MLPs) that map the output embeddings from the modality encoders to the same dimensionality as the LLM's text embeddings. This alignment allows the different types of embeddings to be seamlessly combined and fed into the LLM.</li>
</ul>
<h3 id="cross-modality-attention-approach">Cross-Modality Attention Approach<a class="headerlink" href="#cross-modality-attention-approach" title="Permanent link">&para;</a></h3>
<p>In this approach, information from different modalities is fused more deeply within the transformer architecture itself, primarily using cross-attention mechanisms. Instead of simply concatenating embeddings at the input stage, the LLM's attention layers can directly attend to features or representations from other modalities (e.g., image patch features) at various stages of processing.
This is analogous to the original Transformer architecture used for machine translation, where the decoder part attended to the encoder's output (representing the source language) via cross-attention. In a multimodal context, the "encoder" can be thought of as the modality-specific encoder (e.g., an image encoder), and the LLM decoder can then cross-attend to these features, allowing for a more integrated fusion of information.</p>
<p>The choice of architecture impacts how modalities are integrated, the overall complexity of the model, the amount of new parameters that need to be trained, and potentially its performance characteristics on different types of multimodal tasks.</p>
<h2 id="exploring-key-modalities">Exploring Key Modalities<a class="headerlink" href="#exploring-key-modalities" title="Permanent link">&para;</a></h2>
<h3 id="text-models-baseline">Text Models (Baseline)<a class="headerlink" href="#text-models-baseline" title="Permanent link">&para;</a></h3>
<p>Traditional Large Language Models (LLMs) like OpenAI's GPT series (e.g., GPT-3, GPT-4), Meta's Llama family (e.g., Llama 2, Llama 3), Google's Gemini (in its text-only capacity or as a base for multimodal versions), Anthropic's Claude models, and Mistral AI's models (e.g., Mistral 7B, Mixtral models) are primarily unimodal when focusing on their text processing capabilities. They serve as the foundation upon which many multimodal systems are built.</p>
<ul>
<li><strong>Input Modality:</strong> Text (sequences of characters, words, or tokens).</li>
<li><strong>Core Functionality:</strong></li>
<li><strong>Text Generation:</strong> Creating coherent and contextually relevant text (e.g., stories, articles, dialogues).</li>
<li><strong>Reading Comprehension &amp; Question Answering:</strong> Understanding text passages and answering questions based on them.</li>
<li><strong>Summarization:</strong> Condensing long texts into shorter summaries while preserving key information.</li>
<li><strong>Translation:</strong> Converting text from one language to another.</li>
<li><strong>Code Generation:</strong> Generating programming code based on natural language descriptions.</li>
<li><strong>How They Work (Briefly):</strong> These models use transformer architectures to process sequences of text tokens. They learn statistical patterns and relationships in language data during pretraining on vast text corpora, enabling them to predict subsequent tokens in a sequence or understand the semantic meaning of the input.</li>
<li><strong>Real-Life Usage:</strong></li>
<li><strong>Chatbots and Conversational AI:</strong> Powering interactive agents like OpenAI's ChatGPT (based on GPT models), Google's Gemini experiences, Anthropic's Claude chatbot, and other customer service bots.</li>
<li><strong>Writing Assistance:</strong> Tools for grammar correction, style improvement, content generation (e.g., marketing copy, emails).</li>
<li><strong>Search Engines:</strong> Enhancing query understanding and providing more relevant results.</li>
<li><strong>Software Development:</strong> Assisting with code completion, documentation, and even simple program generation.</li>
</ul>
<p>Understanding these text-only models is crucial as they often form the core reasoning engine in more complex multimodal systems, with other modalities being "translated" or aligned to integrate with this linguistic foundation.</p>
<h3 id="image-modality-vision-understanding">Image Modality (Vision Understanding)<a class="headerlink" href="#image-modality-vision-understanding" title="Permanent link">&para;</a></h3>
<p>Integrating images as an input modality allows LLMs to "see" and interpret visual information, bridging the gap between linguistic and visual understanding.</p>
<ul>
<li><strong>Input Modality:</strong> Images (pixels, typically processed as patches).</li>
<li><strong>How It's Integrated (Common Approaches):</strong></li>
<li><strong>Image Encoders:</strong> Raw images are processed by specialized vision models, often Vision Transformers (ViTs) or Convolutional Neural Networks (CNNs). Pretrained encoders like those from CLIP (Contrastive Language-Image Pre-Training) by OpenAI are commonly used to generate meaningful image embeddings. These encoders convert images into a sequence of vectors representing different parts or aspects of the image.</li>
<li><strong>Projector Layers:</strong> As discussed in the "Core Concepts" section, a projector layer (usually a small neural network, e.g., a Multi-Layer Perceptron or MLP) is often used to map the image embeddings from the vision encoder's output space to the LLM's text embedding space. This alignment is crucial for the LLM to process these visual tokens alongside text tokens.</li>
<li><strong>Concatenation with Text:</strong> In unified decoder architectures, these projected image embeddings are treated as a sequence of special "image tokens" and are often concatenated with text token embeddings before being fed into the LLM.</li>
<li><strong>Core Functionality:</strong></li>
<li><strong>Image Captioning:</strong> Generating descriptive text that explains the content of an image.</li>
<li><strong>Visual Question Answering (VQA):</strong> Answering questions about an image (e.g., "What color is the car?", "Are there any dogs in the picture?").</li>
<li><strong>Image Classification/Tagging:</strong> Assigning one or more labels or tags to an image based on its content.</li>
<li><strong>Object Recognition (within context):</strong> Identifying objects in an image and understanding their relationships, often guided by textual prompts or questions.</li>
<li><strong>Real-Life Usage:</strong></li>
<li><strong>Accessibility:</strong> Generating image descriptions for visually impaired users.</li>
<li><strong>Content Moderation:</strong> Identifying inappropriate or harmful visual content.</li>
<li><strong>Visual Search &amp; E-commerce:</strong> Allowing users to search for products using images or find visually similar items.</li>
<li><strong>Robotics &amp; Autonomous Systems:</strong> Enabling robots to understand their environment visually to navigate and interact.</li>
<li><strong>Education:</strong> Explaining diagrams, charts, and historical images.</li>
<li><strong>Key Models &amp; Providers:</strong></li>
<li><strong>CLIP (OpenAI):</strong> While not an end-to-end VLM itself, its image and text encoders are foundational for many models that understand images.</li>
<li><strong>ViT (Google Brain team, now Google DeepMind):</strong> Vision Transformer, another foundational architecture for image encoding.</li>
<li><strong>Flamingo (Google DeepMind):</strong> An earlier influential model demonstrating VQA and image captioning with a cross-attention approach to fuse visual features.</li>
<li><strong>BLIP / BLIP-2 (Salesforce Research):</strong> Models focused on bootstrapping vision-language pretraining, effective for VQA and captioning.</li>
<li><strong>Fuyu-8B (Adept AI):</strong> Noted for its simpler architecture directly processing image patches.</li>
<li><strong>IDEFICS (Hugging Face):</strong> An open-source reproduction of Flamingo, useful for image-text tasks.</li>
<li>Many models mentioned by Raschka, such as <strong>Molmo (Allen AI)</strong> and <strong>NVLM (NVIDIA)</strong>, also fit here when discussing their image understanding capabilities.</li>
</ul>
<h3 id="speech-modality-speech-to-text-and-text-to-speech">Speech Modality (Speech-to-Text and Text-to-Speech)<a class="headerlink" href="#speech-modality-speech-to-text-and-text-to-speech" title="Permanent link">&para;</a></h3>
<p>Integrating speech allows for natural voice-based interaction with LLMs, encompassing both understanding spoken language
and generating audible responses.</p>
<ul>
<li><strong>Input/Output Modalities:</strong></li>
<li><strong>Speech-to-Text (STT):</strong> Audio waveforms as input.</li>
<li><strong>Text-to-Speech (TTS):</strong> Text sequences as input, audio waveforms as output.</li>
<li><strong>How It's Integrated:</strong></li>
<li><strong>Speech Encoders (for STT):</strong> These models (e.g., OpenAI's Whisper, Meta AI's Wav2Vec 2.0, Google's Conformer-based architectures used in their ASR models) process raw audio. They typically convert audio into a sequence of feature vectors (embeddings) that represent phonetic or acoustic information. These embeddings can then be:<ul>
<li>Directly used by a specialized decoder to produce text.</li>
<li>Fed into an LLM (possibly via a projector layer, similar to image modality) to allow the LLM to "understand" the spoken content and generate a text response or perform a task.</li>
</ul>
</li>
<li><strong>Speech Decoders/Vocoders (for TTS):</strong> To generate speech from text:<ul>
<li>An LLM might generate the textual response.</li>
<li>This text is then fed into a Text-to-Speech (TTS) model. TTS models often consist of two parts:
        1. A <strong>spectrogram generator</strong> (e.g., Google's Tacotron 2, Microsoft/FastSpeech team's FastSpeech) that converts text into a mel-spectrogram (a visual representation of sound).
        2. A <strong>vocoder</strong> (e.g., Google DeepMind's WaveNet, HiFi-GAN (various researchers), MelGAN (various researchers)) that converts the mel-spectrogram into an audible waveform.</li>
<li>Some newer end-to-end models can generate speech more directly.</li>
</ul>
</li>
<li><strong>Core Functionality:</strong></li>
<li><strong>Speech-to-Text (STT) / Automatic Speech Recognition (ASR):</strong> Transcribing spoken language into written text.</li>
<li><strong>Text-to-Speech (TTS):</strong> Converting written text into natural-sounding spoken language.</li>
<li><strong>Spoken Language Understanding (SLU):</strong> Understanding the intent and content of spoken queries, often involving STT followed by NLU (Natural Language Understanding by an LLM).</li>
<li><strong>Voice-Controlled Systems:</strong> Enabling interaction with devices and applications using voice commands.</li>
<li><strong>Key Differences:</strong></li>
<li><strong>STT</strong> is about <em>understanding</em> audio and converting it to a symbolic representation (text).</li>
<li><strong>TTS</strong> is about <em>generating</em> audio from a symbolic representation (text).</li>
<li>Multimodal LLMs can use STT as an input mechanism and TTS as an output mechanism to create conversational voice agents.</li>
<li><strong>Real-Life Usage:</strong></li>
<li><strong>Voice Assistants:</strong> Siri, Google Assistant, Amazon Alexa rely heavily on STT and TTS.</li>
<li><strong>Transcription Services:</strong> Converting lectures, meetings, and dictations into text.</li>
<li><strong>Accessibility Tools:</strong> Screen readers for visually impaired users (TTS), voice input for users with motor impairments (STT).</li>
<li><strong>Customer Service:</strong> Automated voice responses and call routing.</li>
<li><strong>Content Creation:</strong> Generating voiceovers for videos, podcasts, and audiobooks.</li>
<li><strong>In-car Systems:</strong> Voice control for navigation and entertainment.</li>
<li><strong>Key Models &amp; Systems/Providers:</strong></li>
<li><strong>Whisper (OpenAI):</strong> A highly effective open-source model for Automatic Speech Recognition (ASR).</li>
<li><strong>Wav2Vec 2.0 (Meta AI):</strong> A framework for self-supervised learning of speech representations, forming the basis for many ASR systems.</li>
<li><strong>Conformer (Google):</strong> An architecture combining CNNs and Transformers, widely used in Google's speech recognition services.</li>
<li><strong>Tacotron 2 (Google) &amp; WaveNet (Google DeepMind):</strong> Foundational models for high-quality Text-to-Speech synthesis; many newer TTS systems build upon these concepts.</li>
<li><strong>FastSpeech / FastSpeech 2 (Microsoft Research Asia / various):</strong> TTS models known for faster speech generation.</li>
<li><strong>HiFi-GAN / MelGAN (various research groups):</strong> Popular GAN-based vocoders for generating high-fidelity audio from mel-spectrograms.</li>
<li><strong>SeamlessM4T (Meta AI):</strong> A comprehensive multilingual and multitask model covering speech-to-text, text-to-speech, speech-to-speech translation, and text-to-text translation.</li>
<li><strong>Google Voice Assistant, Amazon Alexa, Apple's Siri:</strong> These commercial voice assistants integrate sophisticated STT and TTS pipelines, often using proprietary models based on architectures like those mentioned above.</li>
<li><strong>ElevenLabs, Coqui.ai:</strong> Examples of companies providing advanced TTS and voice cloning technologies.</li>
<li>The Llama 3.2 multimodal architecture (as per Raschka's article) also includes provisions for speech modality, indicating future integration in such LLMs.</li>
</ul>
<h3 id="vision-language-models-vlms-deeper-integration">Vision-Language Models (VLMs - Deeper Integration)<a class="headerlink" href="#vision-language-models-vlms-deeper-integration" title="Permanent link">&para;</a></h3>
<p>While basic image understanding capabilities like captioning are foundational, Vision-Language Models (VLMs) represent a more advanced class of multimodal systems. These models aim for a deeper, more contextual understanding and reasoning ability that spans both visual and textual information. They often build upon the same core architectural concepts (unified embeddings or cross-attention) but are trained on more diverse datasets and tasks that require sophisticated interplay between vision and language.</p>
<ul>
<li><strong>Input Modalities:</strong> Primarily Images/Video and Text.</li>
<li><strong>How They Differ from Basic Image Modality Integration:</strong></li>
<li><strong>Complexity of Tasks:</strong> VLMs go beyond simple object recognition or captioning. They are designed for tasks requiring multi-step reasoning, understanding nuanced instructions related to visual content, and generating more detailed, context-aware textual outputs grounded in visual data.</li>
<li><strong>Instruction Following:</strong> Many advanced VLMs are instruction-tuned, meaning they can follow complex natural language instructions that refer to elements or concepts within an image or video.</li>
<li><strong>Knowledge Integration:</strong> They often leverage the LLM's vast world knowledge and reasoning capabilities to interpret visual scenes more effectively. For example, identifying not just objects, but also their relationships, potential affordances, or implied actions.</li>
<li><strong>Output Modalities:</strong> While primarily text output, some VLMs might also generate other outputs like bounding boxes or segmentation masks in response to queries (though dedicated segmentation models are more specialized for the latter).</li>
<li><strong>Core Functionality:</strong></li>
<li><strong>Advanced Visual Question Answering (VQA):</strong> Answering complex questions that require deeper reasoning about image content, relationships between objects, and implied information (e.g., "Why might the person in the image be feeling happy?" or "What is likely to happen next?").</li>
<li><strong>Visual Dialogue:</strong> Engaging in multi-turn conversations about an image or video.</li>
<li><strong>Instruction Following with Visual Grounding:</strong> Performing tasks based on textual instructions that refer to specific parts or aspects of an image (e.g., "Describe the object to the left of the red car," or "If I move the blue block on top of the green one, what happens?").</li>
<li><strong>Image/Video-based Text Generation:</strong> Writing stories, reports, or detailed descriptions based on visual input.</li>
<li><strong>Optical Character Recognition (OCR) in Context:</strong> Reading and understanding text embedded in images (e.g., street signs, labels on products) and using that information for broader reasoning.</li>
<li><strong>Real-Life Usage:</strong></li>
<li><strong>Enhanced AI Assistants:</strong> More capable virtual assistants that can understand and discuss images or what a user is seeing (e.g., through a smartphone camera).</li>
<li><strong>Education &amp; Training:</strong> Interactive learning tools that can explain diagrams, scientific figures, or historical images in detail and answer student questions.</li>
<li><strong>Medical Image Analysis:</strong> Assisting radiologists by describing medical scans, answering questions about anomalies, or summarizing findings (though expert oversight is crucial).</li>
<li><strong>Content Creation &amp; Augmentation:</strong> Generating rich descriptions for products in e-commerce, creating alternative text for complex images, or even drafting articles based on visual information.</li>
<li><strong>Robotics and Embodied AI:</strong> Enabling robots to better understand and interact with their environment based on visual input and natural language commands.</li>
<li><strong>Key Models &amp; Providers:</strong></li>
<li><strong>GPT-4V (OpenAI):</strong> A highly capable VLM known for its strong performance on various vision-language tasks, including complex reasoning and instruction following.</li>
<li><strong>Gemini (Google DeepMind):</strong> Google's flagship multimodal model series, designed to understand and reason across text, code, images, audio, and video.</li>
<li><strong>LLaVA / LLaVA-NeXT (various researchers, e.g., from UW Madison, Microsoft Research):</strong> Popular open-source approaches for building VLMs by connecting vision encoders (like CLIP's ViT) with LLMs (like Llama) using a simple projector, known for good performance with efficient training.</li>
<li><strong>Llama 3.2 Multimodal (Meta AI):</strong> As mentioned in Raschka's article, these models (11B and 90B parameters) use a cross-attention approach and are designed for image-text tasks.</li>
<li><strong>Qwen2-VL (Alibaba Cloud):</strong> Also from Raschka's article, notable for its "Naive Dynamic Resolution" mechanism to handle images of varying resolutions.</li>
<li><strong>NVLM (NVIDIA):</strong> Explores decoder-only, cross-attention, and hybrid approaches for VLMs, as detailed in Raschka's article.</li>
<li><strong>Pixtral 12B (Mistral AI):</strong> Mistral's first multimodal model, using a unified embedding decoder approach.</li>
<li><strong>CogVLM (THUDM - Tsinghua University):</strong> An open-source VLM known for its strong performance on visual grounding and dialogue tasks.</li>
<li><strong>IDEFICS (Hugging Face):</strong> While also useful for basic image-text tasks, its capabilities extend into VLM territory.</li>
<li><strong>Video-LLaMA, Video-ChatGPT (various researchers):</strong> Examples of models extending VLM concepts to video understanding, enabling tasks like video summarization and Q&amp;A about video content.</li>
</ul>
<h3 id="segment-anything-advanced-vision-segmentation">'Segment Anything' / Advanced Vision Segmentation<a class="headerlink" href="#segment-anything-advanced-vision-segmentation" title="Permanent link">&para;</a></h3>
<p>Beyond recognizing objects or describing scenes, a critical aspect of visual understanding is segmentation: identifying the precise pixel-level boundaries of objects and regions within an image. Models like Meta AI's Segment Anything Model (SAM) have revolutionized this space by enabling highly generalized, promptable segmentation.</p>
<ul>
<li><strong>Input Modality:</strong> Images, along with various types of prompts (text descriptions, points, bounding boxes, or even masks from previous segmentation steps).</li>
<li><strong>How It's Integrated &amp; Key Architectural Components:</strong></li>
<li><strong>Vision Backbone/Image Encoder:</strong> A powerful image encoder (often a Vision Transformer, e.g., ViT-H for SAM) processes the input image to extract robust visual features. This encoder is typically pretrained on a large dataset.</li>
<li><strong>Prompt Encoder:</strong> Encodes various user prompts (points, boxes, text) into embedding vectors. For text prompts, a text encoder like CLIP's might be used.</li>
<li><strong>Mask Decoder:</strong> This lightweight decoder takes the image embeddings and prompt embeddings as input and efficiently predicts segmentation masks for the objects or regions indicated by the prompts. SAM's decoder, for example, can output multiple valid masks for ambiguous prompts, along with confidence scores.</li>
<li><strong>Core Functionality:</strong></li>
<li><strong>Zero-Shot Segmentation:</strong> The ability to segment objects or regions without having been explicitly trained on those specific object categories. The model generalizes from its broad pretraining.</li>
<li><strong>Promptable Segmentation:</strong> Users can guide the segmentation process by providing:<ul>
<li><strong>Points:</strong> Clicking on an object to segment it.</li>
<li><strong>Bounding Boxes:</strong> Drawing a box around an object.</li>
<li><strong>Text Prompts:</strong> Describing what to segment (e.g., "segment all the cats").</li>
<li><strong>Coarse Masks:</strong> Providing a rough mask to refine.</li>
</ul>
</li>
<li><strong>Ambiguity Handling:</strong> For ambiguous prompts (e.g., a point that could belong to multiple nested objects), models like SAM can generate multiple valid masks, allowing the user to select the desired one.</li>
<li><strong>Automatic Mask Generation:</strong> Some models can also generate masks for all detected objects in an image automatically.</li>
<li><strong>Integration with VLMs:</strong> The detailed segmentation masks can serve as precise inputs or grounding for Vision-Language Models, enabling them to reason about and interact with specific, user-defined image regions.</li>
<li><strong>Differences from Traditional Segmentation &amp; Other Vision Tasks:</strong></li>
<li><strong>Generalization:</strong> Unlike traditional segmentation models that are often trained for a fixed set of object categories, "Segment Anything" models aim for universal segmentation capabilities.</li>
<li><strong>Promptability:</strong> The interactive, prompt-based nature is a key differentiator, making them highly versatile.</li>
<li><strong>Output Detail:</strong> Provides pixel-level masks, which are more detailed than bounding boxes (object detection) or image-level labels (classification).</li>
<li><strong>Real-Life Usage:</strong></li>
<li><strong>Data Annotation:</strong> Rapidly annotating images for training other computer vision models by generating accurate masks with minimal human effort.</li>
<li><strong>Image Editing Software:</strong> Advanced selection tools (e.g., "magic wand" on steroids), background removal, object manipulation.</li>
<li><strong>Scientific Research:</strong> Analyzing microscopy images, satellite imagery (e.g., segmenting land cover, water bodies), or medical scans (e.g., identifying cells, tumors, anatomical structures).</li>
<li><strong>Creative Content Creation:</strong> Compositing images, creating visual effects.</li>
<li><strong>Robotics &amp; Autonomous Systems:</strong> Enhancing scene understanding by allowing robots to precisely identify and delineate objects for interaction or navigation.</li>
<li><strong>Augmented Reality (AR):</strong> More realistic placement and interaction of virtual objects with the real world by understanding precise object boundaries.</li>
<li><strong>Key Model &amp; Provider:</strong></li>
<li><strong>Segment Anything Model (SAM) (Meta AI):</strong> The foundational model in this category, developed by Meta AI. SAM is renowned for its remarkable zero-shot generalization and promptable segmentation capabilities, largely due to its training on the extensive SA-1B dataset containing over a billion masks. Its architecture (ViT image encoder, prompt encoder, and mask decoder) has set a benchmark for generalist image segmentation tools.</li>
</ul>
<h2 id="understanding-modality-vs-learned-capabilities">Understanding Modality vs. Learned Capabilities<a class="headerlink" href="#understanding-modality-vs-learned-capabilities" title="Permanent link">&para;</a></h2>
<p>It's important to distinguish between a model's inherent <strong>modalities</strong> (the types of data it can process) and the specific <strong>skills or patterns</strong> it learns through training.</p>
<h3 id="what-is-a-modality">What is a Modality?<a class="headerlink" href="#what-is-a-modality" title="Permanent link">&para;</a></h3>
<p>A <strong>modality</strong> refers to the fundamental type(s) of data a model is architected to accept as input and/or produce as output. Examples include text-only models, image-text models (which can take images and text as input and might output text), audio-text models, and so on. Modality is largely determined by the model's architecture, including its specific encoders (for input) and decoders (for output). For instance, an image-text model needs an image encoder to process visual information and a way to integrate these image features with its language processing components, which might involve a shared or separate decoder.</p>
<h3 id="what-are-learned-capabilities-skillspatterns">What are Learned Capabilities (Skills/Patterns)?<a class="headerlink" href="#what-are-learned-capabilities-skillspatterns" title="Permanent link">&para;</a></h3>
<p>A <strong>learned capability</strong> or <strong>skill</strong> (e.g., coding proficiency, summarization, question answering, specific knowledge domains, particular writing styles) is a behavior or expertise the model acquires through its training data and process. These skills are developed <em>within</em> the model's given modalities. For example, code generation is a skill learned by a text-modal LLM; it doesn't mean the model has a separate "code modality" but rather that it has learned the patterns of code within the text modality.</p>
<h3 id="how-specialized-skills-are-achieved-within-a-modality">How Specialized Skills are Achieved within a Modality<a class="headerlink" href="#how-specialized-skills-are-achieved-within-a-modality" title="Permanent link">&para;</a></h3>
<p>Models develop specialized skills through rigorous training processes:</p>
<ul>
<li><strong>Pretraining:</strong> Models learn general patterns, world knowledge, and foundational capabilities (like language structure for text LLMs) from exposure to vast and diverse datasets relevant to their modalities. For example, text LLMs are pretrained on massive text corpora, while VLMs are pretrained on large datasets of image-text pairs.</li>
<li><strong>Fine-tuning:</strong> To develop more specialized skills or adapt to specific tasks/domains, a pretrained base model is further trained (fine-tuned) on smaller, targeted datasets.</li>
<li><strong>Example (Coding):</strong> A text-modal LLM can be fine-tuned on a large corpus of source code, programming tutorials, and technical documentation. This doesn't change its modality (it still processes text), but it becomes highly proficient at understanding and generating code, effectively learning the "pattern" of coding.</li>
<li><strong>Other Examples:</strong> Fine-tuning for medical knowledge, legal document analysis, specific conversational styles, etc.</li>
<li><strong>Instruction Tuning:</strong> This is a powerful fine-tuning technique where models are trained on datasets of (instruction, desired_output) pairs. This teaches the model to become highly responsive to user instructions and perform a wide array of tasks described in natural language, making them more versatile and useful general-purpose assistants. Instruction tuning is key to how a base LLM learns to "do" many different things like translation, summarization, Q&amp;A, creative writing, etc., all typically within its original modality.</li>
<li><strong>Reinforcement Learning from Human Feedback (RLHF):</strong> Often used after instruction tuning, RLHF further refines model behavior to align better with human preferences regarding helpfulness, honesty, and harmlessness. It helps in fine-tuning the <em>quality</em> and <em>style</em> of the learned skills.</li>
</ul>
<h3 id="example-text-modality-llm-for-coding">Example: Text Modality LLM for Coding<a class="headerlink" href="#example-text-modality-llm-for-coding" title="Permanent link">&para;</a></h3>
<p>Let's explicitly address the example of an LLM demonstrating coding abilities:</p>
<ul>
<li>A Large Language Model like OpenAI's GPT-4 or Meta's Llama is fundamentally a <strong>text-modal</strong> system. Its architecture
is designed to process and generate sequences of text.</li>
<li>When such a model demonstrates strong coding abilities, it's not because it has a separate 'code modality.' Instead,
it has <strong>learned the patterns and structures of programming languages</strong> through its training.</li>
<li>This is typically achieved by including a vast amount of source code from public repositories (like GitHub),
programming textbooks, and coding discussions in its pretraining data. Further specialized versions might be
additionally fine-tuned specifically on code-related tasks or through instruction tuning with coding prompts.</li>
<li>So, the LLM uses its text-processing capabilities to treat code as a
specialized form of text. Its skill in coding is
a highly developed pattern learned within its native text modality.</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "content.code.select", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.top", "search.highlight", "content.code.copy"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="../../../assets/external/unpkg.com/mermaid@11/dist/mermaid.min.js"></script>
      
    
  </body>
</html>